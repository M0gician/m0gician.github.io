<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 强化学习 — 价值函数 | Tianyi Yang </title> <meta name="author" content="Tianyi Yang"> <meta name="description" content="深入探讨强化学习中的价值函数与贝尔曼方程。"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Nunito+Sans:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.gif?7e955dea0276164422304bc0ee30bce6"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://m0gician.github.io/blog/2025/value-functions-zh/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tianyi</span> Yang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">强化学习 — 价值函数</h1> <p class="post-meta"> Created in August 02, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/rl"> <i class="fa-solid fa-hashtag fa-sm"></i> RL</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>   ·   <a href="/blog/category/reinforcement-learning"> <i class="fa-solid fa-tag fa-sm"></i> reinforcement-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>在上一节中，我们介绍了马尔可夫决策过程（MDP）如何融入强化学习。本章将基于 MDP 的定义，展示如何从数学角度评估 RL 智能体。</p> <h2 id="状态价值函数">状态价值函数</h2> <p><em>状态价值函数</em> \(v^\pi(s)\) 表示当智能体从状态 \(s\) 出发并按照策略 \(\pi\) 行动时，其期望折扣回报。通俗地说，它衡量在采用策略 \(\pi\) 时身处状态 \(s\) “有多好”。我们称 \(v^\pi(s)\) 为状态 \(s\) 的价值。</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\ \Bigg[\underbrace{\sum_{k=0}^{\infty}\gamma^k R_{t+k}}_{G_t} \bigg| S_t=s, \pi\ \Bigg] \\ &amp;:= \mathbf{E}[G_t|S_t=s, \pi] \\ &amp;:= \mathbf{E}\ \Bigg[\sum_{t=0}^{\infty}\gamma^k R_{t} \bigg| S_0=s, \pi\ \Bigg] \end{aligned}\] <p>回顾我们在上一节中使用的 \(G_t\)（<em>从时间步 \(t\) 开始的折扣回报</em>）记号，可以发现这正是其等价形式。</p> <h3 id="一个简单的mdp示例">一个简单的 MDP 示例</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mdp-simple-480.webp 480w,/assets/img/rl/mdp-simple-800.webp 800w,/assets/img/rl/mdp-simple-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rl/mdp-simple.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Simple MDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></center> </div> <p>在上图所示的 MDP 中，智能体每次可选择两个动作中的一个：<code class="language-plaintext highlighter-rouge">Left</code> 或 <code class="language-plaintext highlighter-rouge">Right</code>。在状态 \(s_1\) 与 \(s_6\) 中，无论采取何种动作都会直接转移至终止状态 \(s_\infty\)。只有在发生 \(s_2 \to s_1\) 或 \(s_5 \to s_6\) 的转移时，智能体才能获得奖励。为简化计算，设折扣因子 \(\gamma = 0.5\)。</p> <p>我们为该 MDP 尝试两种策略：策略 \(\pi_1\) 始终选择 <code class="language-plaintext highlighter-rouge">Left</code>；策略 \(\pi_2\) 始终选择 <code class="language-plaintext highlighter-rouge">Right</code>。</p> <p><strong>策略 1（\(\pi_1\)）：始终选择 <code class="language-plaintext highlighter-rouge">Left</code></strong></p> <ul> <li>\(v^{\pi_1}(s_1) = 0\) （始终直接进入终止状态）</li> <li>\(v^{\pi_1}(s_2) = 12\gamma^0 = 12\).</li> <li>\(v^{\pi_1}(s_3) = 0\gamma^0 + 12\gamma^1 = 6\).</li> <li>\(v^{\pi_1}(s_4) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(v^{\pi_1}(s_5) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(v^{\pi_1}(s_6) = 0\).</li> </ul> <p><strong>策略 2（\(\pi_2\)）：始终选择 <code class="language-plaintext highlighter-rouge">Right</code></strong></p> <ul> <li>\(v^{\pi_2}(s_1) = 0\).</li> <li>\(v^{\pi_2}(s_2) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(v^{\pi_2}(s_3) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(v^{\pi_2}(s_4) = 0\gamma^0 + 2\gamma^1 = 1\).</li> <li>\(v^{\pi_2}(s_5) = 2\gamma^0 = 2\).</li> <li>\(v^{\pi_2}(s_6) = 0\).</li> </ul> <h2 id="行动价值函数">行动价值函数</h2> <p><em>行动价值函数</em> \(q^\pi(s,a)\)（亦称 <em>Q‑函数</em>）表示当智能体在状态 \(s\) 采取动作 \(a\) 并随后按照策略 \(\pi\) 行动时，其期望折扣回报。</p> \[\begin{aligned} q^\pi(s,a) &amp;:= \mathbf{E}\ \Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, A_t = a, \pi\ \Bigg] \\ &amp;:= \mathbf{E}[G_t|S_t=s, A_t=a, \pi] \\ &amp;:= \mathbf{E}\ \Bigg[\sum_{t=0}^{\infty}\gamma^k R_{t} \bigg| S_0=s, A_0=a, \pi\ \Bigg] \end{aligned}\] <h3 id="再看mdp">👀再看MDP</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mdp-simple-480.webp 480w,/assets/img/rl/mdp-simple-800.webp 800w,/assets/img/rl/mdp-simple-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rl/mdp-simple.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Simple MDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></center> </div> <p><strong>策略 1（\(\pi_1\)）：始终选择 <code class="language-plaintext highlighter-rouge">Left</code></strong></p> <ul> <li>\(q^{\pi_1}(s_1, L) = 0\).</li> <li>\(q^{\pi_1}(s_1, R) = 0\).</li> <li>\(q^{\pi_1}(s_2, L) = 12\gamma^0 =12\).</li> <li>\(q^{\pi_1}(s_2, R) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(q^{\pi_1}(s_3, L) = 0\gamma^0 + 12\gamma^1 = 6\).</li> <li>\(q^{\pi_1}(s_3, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(q^{\pi_1}(s_4, L) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(q^{\pi_1}(s_4, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 0\gamma^3 + 12\gamma^4 = 0.75\).</li> <li>\(q^{\pi_1}(s_5, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(q^{\pi_1}(s_5, R) = 2\gamma^0 = 2\).</li> <li>\(q^{\pi_1}(s_6, L) = 0\).</li> <li>\(q^{\pi_1}(s_6, R) = 0\).</li> </ul> <p><strong>策略 2（\(\pi_2\)）：始终选择 <code class="language-plaintext highlighter-rouge">Right</code></strong></p> <ul> <li>\(q^{\pi_2}(s_1, L) = 0\).</li> <li>\(q^{\pi_2}(s_1, R) = 0\).</li> <li>\(q^{\pi_2}(s_2, L) = 12\gamma^0 =12\).</li> <li>\(q^{\pi_2}(s_2, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(q^{\pi_2}(s_3, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 0\gamma^3 + 2\gamma^4 = 0.125\).</li> <li>\(q^{\pi_2}(s_3, R) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(q^{\pi_2}(s_4, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(q^{\pi_2}(s_4, R) = 0\gamma^0 + 2\gamma^1 = 1\).</li> <li>\(q^{\pi_2}(s_5, L) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(q^{\pi_2}(s_5, R) = 2\gamma^0 = 2\).</li> <li>\(q^{\pi_2}(s_6, L) = 0\).</li> <li>\(q^{\pi_2}(s_6, R) = 0\).</li> </ul> <h2 id="vpi的贝尔曼方程">\(v^\pi\) 的贝尔曼方程</h2> <p><em>状态价值函数的贝尔曼方程</em>是 \(v^\pi\) 的递归表达式。为推导该方程，我们首先将即时奖励从价值函数中分离出来：</p> \[\begin{aligned} v^\pi(s) &amp;:= \textbf{E}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg\vert S_t=s, \pi\right] \\ &amp;= \textbf{E}\left[R_t + \sum_{k=1}^{\infty}\gamma^k R_{t+k} \bigg\vert S_t=s, \pi\right] \\ &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=1}^{\infty}\gamma^{k-1} R_{t+k} \bigg\vert S_t=s, \pi\right] \end{aligned}\] <p>通过将求和索引调整为从 0 开始（即将所有 \(k\) 替换为 \(k+1\)），可得到</p> \[\begin{aligned} \textbf{E}\left[\gamma\sum_{k=1}^{\infty}\gamma^{k-1} R_{t+k} \bigg\vert S_t=s, \pi\right] = \textbf{E}\left[\gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \end{aligned}\] <p>根据全概率公式 \(\textbf{E}[X] = \textbf{E}[\textbf{E}[X \vert Y]]\)，令首次动作 \(A_t=a\) 及下一状态 \(S_{t+1}=s'\) 作为条件变量，可得</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ = &amp;\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s') \, \textbf{E}\left[\gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t=a, S_{t+1}=s', \pi\right] \end{aligned}\] <p>利用马尔可夫性质，可将条件中的 \(S_t\) 和 \(A_t\) 去掉：</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, \pi\right] \\ = &amp;\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s') \, \gamma\textbf{E}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s', \pi\right] \end{aligned}\] <p>根据状态价值函数定义，最后一项正是 \(v^\pi(s')\)：</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ = &amp; \gamma\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) v^\pi(s^\prime) \end{aligned}\] <p>由于对任意给定 \(s\)、\(a\)，转移到下一状态 \(s'\) 的概率之和为 1，可将即时奖励写成</p> \[\begin{aligned} R_t = \sum_{a\in\mathcal{A}}\pi(s,a)R(s,a) = \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)R(s,a) \end{aligned}\] <p>综合可得</p> \[\begin{aligned} v^\pi(s) &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ &amp;= \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)R(s,a) + \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) \gamma v^\pi(s^\prime) \end{aligned}\] <p>最终，可得到简洁形式</p> \[v^\pi(s) = \boxed{\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\big(R(s,a) +\gamma v^\pi(s^\prime)\big)}\] <h3 id="关于贝尔曼方程的优点">关于贝尔曼方程的优点</h3> <div class="callout"> <p>我们可以将贝尔曼方程视为把期望回报拆分为两部分：</p> <ol> <li>下一时间步获得的奖励（<em>即时奖励</em>）</li> <li>下一状态的价值</li> </ol> \[v^\pi(s) = \textbf{E}\left[\underbrace{R(s,A_t)}_{\text{即时奖励}} + \gamma\underbrace{v^\pi(S_{t+1})}_{\text{下一状态价值}}\Bigg\vert S_t=s, \pi\right]\] <p>原始定义需要考虑整条状态序列，而贝尔曼方程<strong>只需向前看一步</strong>。</p> <ul> <li>贝尔曼方程的递归性质使其在计算上更有帮助</li> </ul> </div> <h2 id="qpi的贝尔曼方程">\(q^\pi\) 的贝尔曼方程</h2> <p>就如 \(v^\pi\) 的贝尔曼方程给出了 \(v^\pi\) 的递归关系一样，\(q^\pi\) 的贝尔曼方程则给出了行动价值函数 \(q^\pi\) 的递归关系：</p> \[\begin{aligned} q^\pi(s,a) &amp;:= \mathbf{E}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, A_t = a, \pi\Bigg] \\ &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t = a, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}} \pi(s^\prime,a^\prime)\times\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t = a, S_{t+1}=s^\prime, A_{t+1}=a^\prime, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime) \times\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, A_{t+1}=a^\prime, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)\gamma q^\pi(s^\prime, s^\prime) \\ &amp;= \boxed{R(s,a) + \gamma\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)q^\pi(s^\prime, a^\prime)} \end{aligned}\] <p>或简写为</p> \[q^\pi(s,a) = \boxed{R(s,a) + \gamma\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)q^\pi(s^\prime, a^\prime)}\] <h2 id="最优价值函数">最优价值函数</h2> <div class="callout"> <p><strong>最优策略</strong> \(\pi^*\)<br> 若某策略 \(\pi^*\) 至少与所有其他策略一样好，则称其为最优策略。即</p> \[\forall \pi \in \Pi, \; \pi^* \ge \pi\] </div> <div class="callout"> <p>即便最优策略可能不唯一，最优价值函数 \(v^*\) 与 \(q^*\) 却是唯一的——所有最优策略共享同一状态价值函数与行动价值函数。</p> </div> <div class="callout"> <details> <summary>已知最优状态价值函数，若未知转移概率及奖励函数，能否求得最优策略？</summary> <br><strong>不能</strong>。 $$ \arg\max_{a\in\mathcal{A}}\sum_{s'}p(s,a,s')\big[R(s,a) + \gamma v^\pi(s')\big] $$ 的计算仍依赖于 p 和 R。 </details> </div> <div class="callout"> <details> <summary>已知最优行动价值函数，若未知转移概率及奖励函数，能否求得最优策略？</summary> <br><strong>可以</strong>。 $$ \arg\max_{a\in\mathcal{A}}q^*(s,a) $$ 即为状态 s 下的最优动作。 </details> </div> <h3 id="v的贝尔曼最优方程">\(v^*\) 的贝尔曼最优方程</h3> <p>从贝尔曼方程出发，</p> \[v^*(s) = \sum_{a\in\mathcal{A}}\pi^*(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s')\big[R(s,a) + \gamma v^*(s')\big]\] <p>由于最优策略 \(\pi^*\) 仅选择能最大化 \(q^*(s,a)\) 的动作，可写为</p> \[v^*(s) = \max_{a\in\mathcal{A}}\sum_{s' \in \mathcal{S}}p(s,a,s')\big[R(s,a) + \gamma v^*(s')\big]\] <p>这就是 <em>\(v^*\) 的贝尔曼最优方程</em>。</p> <div class="callout"> <p>若一个策略 \(\pi\) 满足贝尔曼最优方程，则对所有状态 \(s \in \mathcal{S}\) 有</p> \[v^*(s) = \max_{a\in\mathcal{A}}\sum_{s' \in \mathcal{S}}p(s,a,s')\big[R(s,a) + \gamma v^*(s')\big]\] </div> <h3 id="q的贝尔曼最优方程">\(q^*\) 的贝尔曼最优方程</h3> <div class="callout"> <p>若一个策略 \(\pi\) 满足贝尔曼最优方程，则对所有动作 \(a \in \mathcal{A}\) 有</p> \[q^*(s,a) = \sum_{s' \in \mathcal{S}} p(s,a,s')\left[ R(s,a) + \gamma \max_{a'\in\mathcal{A}}q^*(s', a')\right]\] </div> <h3 id="贝尔曼最优方程与最优策略">贝尔曼最优方程与最优策略</h3> <div class="callout"> <p><em>若策略 \(\pi\) 满足贝尔曼最优方程，则 \(\pi\) 为最优策略。</em></p> </div> <p><em>证明：</em></p> <p>假设一个策略 \(\pi\) 满足贝尔曼最优方程，那么对于所有状态 \(s\)：</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^\pi(s^\prime)]\] <p>我们可以将贝尔曼最优方程递归地代入表达式中，并替换 \(v^\pi(s^\prime)\)：</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma v^\pi(s^{\prime\prime})\right)\right]\] <p>我们可以无限地继续这个过程，直到 \(\pi\) 从表达式中完全消失：</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right]\] <p>在每个时间步 \(t\)，选择的动作都是最大化未来期望折扣回报的动作，前提是未来的动作也是为了最大化未来折扣回报。</p> <p>现在，让我们考虑任何一个新的策略 \(\pi^\prime\)。如果我们将 \(\max_{a \in \mathcal{A}}\) 替换为 \(\sum_{a \in \mathcal{A}}\pi^\prime(s,a)\)，关系会怎样？我们认为表达式的值不会变得比之前更大。也就是说，对于任何策略 \(\pi^\prime\)：</p> \[\begin{aligned} v^\pi(s) &amp;= \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \\ &amp;\geq \sum_{a \in \mathcal{A}}\pi^\prime(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\sum_{a^\prime \in \mathcal{A}}\pi^\prime(s^\prime,a^\prime)\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \end{aligned}\] <p>鉴于上述不等式对所有策略 \(\pi^\prime\) 都成立，我们得出对于所有状态 \(s \in \mathcal{S}\) 和所有策略 \(\pi^\prime \in \Pi\)：</p> \[\begin{aligned} v^\pi(s) &amp;= \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \\ &amp;\geq \mathbf{E}[G_t | S_t = s, \pi^\prime] \\ &amp;= v^{\pi^\prime}(s) \end{aligned}\] <p>因此，对于所有状态 \(s \in \mathcal{S}\) 和所有策略 \(\pi^\prime \in \Pi\)，\(v^\pi(s) \geq v^{\pi^\prime}(s)\)。换句话说，对于所有策略 \(\pi^\prime \in \Pi\)，我们有 \(\pi \geq \pi^\prime\)，因此 \(\pi\) 是一个最优策略。</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Tianyi Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"publications",description:"\u201c\u5f88\u60ed\u6127\uff0c\u5c31\u505a\u4e86\u4e00\u70b9\u5fae\u5c0f\u7684\u5de5\u4f5c\uff0c\u8c22\u8c22\u5927\u5bb6\u3002\u201d",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-repositories",title:"repositories",description:"\u201c\u6211\u4eec\u5fc5\u987b\u53bb\u52aa\u529b\u5b66\u4e60\u65b0\u4e1c\u897f\u3002\u5373\u4f7f\u6211\u4eec\u9047\u5230\u4e86\u95ee\u9898\uff0c\u5927\u5bb6\u4e5f\u4f1a\u7406\u89e3\u7684\u3002\u201d",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-reinforcement-learning-value-functions",title:"Reinforcement Learning \u2014 Value Functions",description:"A deep dive into value functions and the Bellman equation in Reinforcement Learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/value-functions/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u4ef7\u503c\u51fd\u6570",title:"\u5f3a\u5316\u5b66\u4e60 \u2014 \u4ef7\u503c\u51fd\u6570",description:"\u6df1\u5165\u63a2\u8ba8\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4ef7\u503c\u51fd\u6570\u4e0e\u8d1d\u5c14\u66fc\u65b9\u7a0b\u3002",section:"Posts",handler:()=>{window.location.href="/blog/2025/value-functions-zh/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e0e\u5f3a\u5316\u5b66\u4e60",title:"\u5f3a\u5316\u5b66\u4e60 - \u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e0e\u5f3a\u5316\u5b66\u4e60",description:"\u5f3a\u5316\u5b66\u4e60\u6982\u5ff5\u5165\u95e8\uff0c\u5305\u62ec\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u3002",section:"Posts",handler:()=>{window.location.href="/blog/2025/mdp-and-rl-zh/"}},{id:"post-reinforcement-learning-mdp-and-rl",title:"Reinforcement Learning - MDP and RL",description:"An introduction to Reinforcement Learning concepts, including Markov Decision Processes (MDPs).",section:"Posts",handler:()=>{window.location.href="/blog/2025/mdp-and-rl/"}},{id:"post-reinforcement-learning-mathematical-foundations",title:"Reinforcement Learning - Mathematical Foundations",description:"A very, very, very large chunk of math",section:"Posts",handler:()=>{window.location.href="/blog/2025/math-foundations-for-RL/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u6570\u5b66\u57fa\u7840",title:"\u5f3a\u5316\u5b66\u4e60 - \u6570\u5b66\u57fa\u7840",description:"\u8d85\u5927\u4e00\u5768\u6570\u5b66\u516c\u5f0f",section:"Posts",handler:()=>{window.location.href="/blog/2025/math-foundations-for-RL-zh/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>