<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 强化学习 - 马尔可夫决策过程与强化学习 | Tianyi Yang </title> <meta name="author" content="Tianyi Yang"> <meta name="description" content="强化学习概念入门，包括马尔可夫决策过程（MDP）。"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Nunito+Sans:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.gif?7e955dea0276164422304bc0ee30bce6"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://m0gician.github.io/blog/2025/mdp-and-rl-cn/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tianyi</span> Yang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">强化学习 - 马尔可夫决策过程与强化学习</h1> <p class="post-meta"> Created in July 15, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/rl"> <i class="fa-solid fa-hashtag fa-sm"></i> RL</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>   ·   <a href="/blog/category/reinforcement-learning"> <i class="fa-solid fa-tag fa-sm"></i> reinforcement-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="什么是强化学习">什么是强化学习？</h2> <div class="blockquote"> <p>强化学习是机器学习的一个领域，其灵感来自行为主义心理学，研究智能体如何从与环境的互动中学习。 <br>—Sutton &amp; Barto (1998), Phil, <cite>维基百科</cite></p> </div> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/rl-system-480.webp 480w,/assets/img/rl/rl-system-800.webp 800w,/assets/img/rl/rl-system-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rl/rl-system.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="强化学习图示" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></center> </div> <p>一个典型的强化学习系统由5个部分组成：<strong>智能体</strong>（agent）在<strong>环境</strong>（environment）中的每个<strong>状态</strong>（state）下执行一个<strong>动作</strong>（action），并在满足某些标准时获得<strong>奖励</strong>（reward）。</p> <div class="callout"> <details><summary><strong>监督学习问题可以转化为强化学习问题吗？</strong></summary> <strong>可以</strong>。我们可以将一个监督学习问题转化为一个强化学习问题（状态作为分类器的输入；动作作为标签；如果标签正确，奖励为1，否则为-1）。</details> </div> <div class="callout"> <details><summary><strong>强化学习是监督学习的替代品吗？</strong></summary> <p><strong>不是</strong>。监督学习使用指导性反馈（智能体应该采取什么行动）。任何偏离所提供反馈的行为都会受到惩罚。</p> <p>另一方面，强化学习问题不是以固定的数据集形式提供的，而是以代码或整个环境的描述形式提供的。强化学习中的奖励应该传达智能体的行为有多“好”，而不是最好的行为应该是什么。智能体的目标是最大化总奖励，这可能需要智能体放弃眼前的奖励以获得以后更大的奖励。</p> <p>如果你有一个序列问题或一个只有评估性反馈可用的问题（或两者兼有！），那么你应该考虑使用强化学习。</p></details> </div> <h3 id="示例网格世界">示例：网格世界</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/gridworld-480.webp 480w,/assets/img/rl/gridworld-800.webp 800w,/assets/img/rl/gridworld-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rl/gridworld.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="网格世界" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></center> </div> <p><strong>状态</strong>: 机器人的位置。机器人没有朝向。</p> <p><strong>动作</strong>: <code class="language-plaintext highlighter-rouge">尝试向上</code> (AU), <code class="language-plaintext highlighter-rouge">尝试向下</code> (AD), <code class="language-plaintext highlighter-rouge">尝试向左</code> (AL), <code class="language-plaintext highlighter-rouge">尝试向右</code> (AR)</p> <p><strong>环境动态</strong>:</p> <p><strong>奖励</strong>:</p> <ul> <li>智能体进入有水的状态会得到-10的奖励，进入目标状态会得到+10的奖励。</li> <li>进入任何其他状态的奖励为零。</li> <li>任何导致智能体停留在状态21的动作都将被视为再次进入水域状态，并导致额外的-10奖励。</li> <li>奖励折扣参数 \(\gamma = 0.9\)。</li> </ul> <p><strong>状态数量</strong>: 24</p> <ul> <li>23个正常状态 + 1个终止吸收状态 (\(s_\infty\)) <ul> <li>一旦进入\(s_\infty\)，智能体就永远无法离开（<strong>回合</strong>结束）。</li> <li>\(s_\infty\) 不应被认为是“目标”状态。</li> </ul> </li> </ul> <hr> <h2 id="用数学方式描述智能体和环境">用数学方式描述智能体和环境</h2> <h3 id="环境的数学定义">环境的数学定义</h3> <p>我们可以使用<strong>马尔可夫决策过程</strong>（MDPs）来形式化强化学习问题的环境。其中的独特术语是\(\mathcal{S}\)（所有可能状态的集合），\(\mathcal{A}\)（所有可能动作的集合），\(p\)（转移函数），\(d_R\)（奖励分布），\(R\)（奖励函数），\(d_0\)（初始状态分布）和\(\gamma\)（奖励折扣参数）。环境的通用定义是</p> \[(\mathcal{S}, \mathcal{A}, p, R, \gamma)\] <h3 id="智能体的数学定义">智能体的数学定义</h3> <p>我们将智能体选择动作的决策规则定义为<strong>策略</strong>。形式上，策略\(\pi\)是一个函数</p> \[\begin{aligned} &amp;\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0,1] \\ &amp;\pi(s,a) := \text{Pr}(A_t=a | S_t=s) \end{aligned}\] <div class="callout"> <p><strong>智能体的目标</strong></p> <p>智能体的目标是找到一个最优策略\(\pi^*\)，以最大化智能体将获得的总奖励的期望值。</p> </div> <h3 id="示例山地车">示例：山地车</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mountain-car-480.webp 480w,/assets/img/rl/mountain-car-800.webp 800w,/assets/img/rl/mountain-car-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rl/mountain-car.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="山地车" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></center> </div> <div class="caption"> 山地车环境 </div> <ul> <li> <strong>状态</strong>: \(s=(x,v)\), 其中 \(x \in \mathbb{R}\) 是小车的位置，\(v \in \mathbb{R}\) 是速度。</li> <li> <strong>动作</strong>: \(a \in \{\texttt{倒车}, \texttt{空挡}, \texttt{前进}\}\). 这些动作被映射为数值 \(a \in \{-1, 0 ,1\}\)。</li> <li> <p><strong>动态</strong>: 动态是确定性的——在状态\(s\)下采取动作\(a\)总是产生相同的状态\(s^\prime\)。因此，\(p(s,a,s^\prime) \in \{0, 1\}\)。动态特性如下：</p> \[\begin{aligned} v_{t+1} &amp;= v_t + 0.001 a_t - 0.0025 \cos(3x_t) \\ x_{t+1} &amp;= x_t + v_{t+1} \end{aligned}\] <p>在计算出下一个状态 \(s^\prime = [x_{t+1}, v_{t+1}]\) 后，</p> <ul> <li>\(x_{t+1}\) 的值被限制在闭区间 \([-1.2, 0.5]\) 内。</li> <li>\(v_{t+1}\) 的值被限制在闭区间 \([-0.7, 0.7]\) 内。</li> <li>如果 \(x_{t+1}\) 到达左边界或右边界（\(x_{t+1} = -1.2\) 或 \(x_{t+1} = 0.5\)），那么小车的速度将重置为零（\(v_{t+1} = 0\)）。</li> </ul> </li> <li> <strong>初始状态</strong>: \(S_0 = (X_0, 0)\), 其中 \(X_0\) 是从区间 \([-0.6, -0.4]\) 中均匀随机抽取的初始位置。</li> <li> <strong>终止状态</strong>: 如果 \(x_t = 0.5\)，则该状态为终止状态（它总是转移到 \(s_\infty\)）。</li> <li> <strong>奖励</strong>: \(R_t\) 总是为 -1，除非转移到 \(s_\infty\)（从 \(s_\infty\) 或从终止状态），此时 \(R_t = 0\)。</li> <li> <strong>折扣</strong>: \(\gamma = 1.0\)。</li> </ul> <hr> <h3 id="附加术语符号和假设">附加术语、符号和假设</h3> <ul> <li> <p><strong>历史</strong>（history），\(H_t\)，是回合中直到时间\(t\)所发生事件的记录：</p> \[H_t := (S_0, A_0, R_0, S_1, A_1, R_1, \ldots, S_t, A_t, R_t)\] </li> <li> <strong>轨迹</strong>（trajectory）是整个回合的历史：\(H_\infty\)</li> <li>轨迹的<strong>回报</strong>（return）或<strong>折扣回报</strong>（discounted return）是奖励的折扣总和 \(G := \sum_{t = 0}^{\infty} \gamma^t R_t\)</li> <li> <strong>期望回报</strong>（expected return）或<strong>期望折扣回报</strong>（expected discounted return）可以写成 \(J(\pi) := \mathbf{E}[G\vert\pi]\)</li> <li>从时间\(t\)开始的<strong>回报</strong>或从时间\(t\)开始的<strong>折扣回报</strong>，\(G_t\)，是从时间\(t\)开始的奖励的折扣总和</li> </ul> \[G_t := \sum_{k=1}^{\infty} \gamma^k R_{t+k}\] <ul> <li> <p>MDP的<strong>范围</strong>（horizon），\(L\)，是满足以下条件的最小整数</p> \[\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\] <ul> <li>如果对于所有策略 \(L &lt; \infty\)，我们称该MDP为<strong>有限范围</strong>（finite horizon）</li> <li>如果 \(L = \infty\)，则该领域可能是<strong>不确定范围</strong>（indefinite horizon）（智能体总是会进入\(s_\infty\)）或<strong>无限范围</strong>（infinite horizon）（智能体可能永远不会进入\(s_\infty\)）</li> </ul> </li> </ul> <hr> <h3 id="马尔可夫性质">马尔可夫性质</h3> <div class="callout"> <p><strong>马尔可夫性质 (</strong>马尔可夫假设<strong>)</strong></p> <p>简而言之：<strong><em>给定现在，未来与过去无关</em></strong>。</p> <p>形式上，给定\(S_t\)，\(S_{t+1}\) 条件独立于 \(H_{t-1}\)。也就是说，对于所有的 \(h, s, a, s^\prime, t\)：</p> <p>\(\text{Pr}(S_{t+1} = s^\prime | H_{t-1} = h, S_{t}=s, A_{t}=a) = \text{Pr}(S_{t+1}=s^\prime | S_{t}=s, A_{t}=a)\)</p> </div> <p>如果一个模型（环境、奖励…）满足马尔可夫假设，我们就说它具有马尔可夫性质，或者说这个模型是 <strong><em>Markovian</em></strong>的。</p> <hr> <h2 id="为什么在强化学习中使用mdp">为什么在强化学习中使用MDP？</h2> <p>MDP不仅功能强大，足以模拟学习智能体与其环境之间的交互，它还带来了一些关键的保证，使我们的“强化学习”能够真正起作用。</p> <p>现在，让我们跳过推导，直接看结论。</p> <div class="callout"> <p><strong>最优策略的存在性</strong></p> <p>对于所有满足 \(|\mathcal{S}| &lt; \infty\), \(|\mathcal{A}| &lt; \infty\), \(R_\text{max} &lt; \infty\) 和 \(\gamma &lt; 1\) 的MDP，至少存在一个最优策略 \(\pi^*\)。</p> </div> <p>稍后当我们介绍<strong>贝尔曼方程</strong>和<strong>贝尔曼最优方程</strong>时，我们将进一步确定：</p> <ol> <li>如果一个策略\(\pi\)在每个步骤中都达到了一个状态，其期望的未来奖励无法通过任何其他行动或决策进一步提高（<strong>贝尔曼最优方程</strong>），那么它就是一个最优策略。</li> <li>如果只有有限数量的可能状态和动作，奖励有界，并且未来奖励被折扣（折扣因子\(\gamma &lt; 1\)），那么存在一个满足贝尔曼最优方程的策略\(\pi\)。</li> </ol> <p>此外，我们可以使用贝尔曼方程和贝尔曼最优方程执行策略/价值迭代（稍后将介绍）。因此，我们不仅可以一次又一次地迭代得到更好的策略，而且在某些约束下，还可以证明最终策略将收敛到最优策略。</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Tianyi Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"publications",description:"\u201c\u5f88\u60ed\u6127\uff0c\u5c31\u505a\u4e86\u4e00\u70b9\u5fae\u5c0f\u7684\u5de5\u4f5c\uff0c\u8c22\u8c22\u5927\u5bb6\u3002\u201d",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-repositories",title:"repositories",description:"\u201c\u6211\u4eec\u5fc5\u987b\u53bb\u52aa\u529b\u5b66\u4e60\u65b0\u4e1c\u897f\u3002\u5373\u4f7f\u6211\u4eec\u9047\u5230\u4e86\u95ee\u9898\uff0c\u5927\u5bb6\u4e5f\u4f1a\u7406\u89e3\u7684\u3002\u201d",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e0e\u5f3a\u5316\u5b66\u4e60",title:"\u5f3a\u5316\u5b66\u4e60 - \u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e0e\u5f3a\u5316\u5b66\u4e60",description:"\u5f3a\u5316\u5b66\u4e60\u6982\u5ff5\u5165\u95e8\uff0c\u5305\u62ec\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u3002",section:"Posts",handler:()=>{window.location.href="/blog/2025/mdp-and-rl-cn/"}},{id:"post-reinforcement-learning-mdp-and-rl",title:"Reinforcement Learning - MDP and RL",description:"An introduction to Reinforcement Learning concepts, including Markov Decision Processes (MDPs).",section:"Posts",handler:()=>{window.location.href="/blog/2025/mdp-and-rl/"}},{id:"post-reinforcement-learning-mathematical-foundations",title:"Reinforcement Learning - Mathematical Foundations",description:"A very, very, very large chunk of math",section:"Posts",handler:()=>{window.location.href="/blog/2025/math-foundations-for-RL/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u6570\u5b66\u57fa\u7840",title:"\u5f3a\u5316\u5b66\u4e60 - \u6570\u5b66\u57fa\u7840",description:"\u8d85\u5927\u4e00\u5768\u6570\u5b66\u516c\u5f0f",section:"Posts",handler:()=>{window.location.href="/blog/2025/math-foundations-for-RL-cn/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>