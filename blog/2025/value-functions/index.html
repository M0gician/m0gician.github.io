<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reinforcement Learning — Value Functions | Tianyi Yang </title> <meta name="author" content="Tianyi Yang"> <meta name="description" content="A deep dive into value functions and the Bellman equation in Reinforcement Learning."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Nunito+Sans:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.gif?7e955dea0276164422304bc0ee30bce6"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://m0gician.github.io/blog/2025/value-functions/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tianyi</span> Yang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning — Value Functions</h1> <p class="post-meta"> Created in August 02, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/rl"> <i class="fa-solid fa-hashtag fa-sm"></i> RL</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>   ·   <a href="/blog/category/reinforcement-learning"> <i class="fa-solid fa-tag fa-sm"></i> reinforcement-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Last time, we covered how MDPs are integrated into Reinforcement Learning. In this chapter, we will see how we can evaluate an RL agent mathematically based on the definitions of MDPs.</p> <h2 id="state-value-function">State-Value Function</h2> <p>The <em>State-Value function</em> \(v^\pi(s)\) calculates the expected discounted return if the agent starts in state \(s\) and follows policy \(\pi\). Informally, it tells us how “good” it is for the agent to be in state \(s\) when using policy \(\pi\). We call \(v^\pi(s)\) the <em>value of state</em> \(s\).</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\ \Bigg[\underbrace{\sum_{k=0}^{\infty}\gamma^k R_{t+k}}_{G_t} \bigg| S_t=s, \pi\ \Bigg] \\ &amp;:= \mathbf{E}[G_t|S_t=s, \pi] \\ &amp;:= \mathbf{E}\ \Bigg[\sum_{t=0}^{\infty}\gamma^k R_{t} \bigg| S_0=s, \pi\ \Bigg] \end{aligned}\] <p>Recalling the \(G_t\) notation (<em>discounted return from time</em> \(t\)) we covered last time, we can see that this is an equivalent definition.</p> <h3 id="a-simple-mdp-example">A Simple MDP Example</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mdp-simple-480.webp 480w,/assets/img/rl/mdp-simple-800.webp 800w,/assets/img/rl/mdp-simple-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rl/mdp-simple.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Simple MDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></center> </div> <p>In the MDP above, the agent can choose between two actions: <code class="language-plaintext highlighter-rouge">Left</code> or <code class="language-plaintext highlighter-rouge">Right</code>. In states \(s_1\) and \(s_6\), any action will cause a transition to the terminal state \(s_\infty\). The agent only gets a reward when transitioning from \(s_2 \to s_1\) or \(s_5 \to s_6\). For simplicity, we will use \(\gamma = 0.5\).</p> <p>Let’s test two policies for this MDP. One policy, \(\pi_1\), will always select <code class="language-plaintext highlighter-rouge">Left</code> action; another policy, \(\pi_2\), will always select the <code class="language-plaintext highlighter-rouge">Right</code> action.</p> <p><strong>Policy 1 (\(\pi_1\)): Select <code class="language-plaintext highlighter-rouge">Left</code> Action Always</strong></p> <ul> <li>\(v^{\pi_1}(s_1) = 0\) (goes to the terminal state always)</li> <li>\(v^{\pi_1}(s_2) = 12\gamma^0 = 12\).</li> <li>\(v^{\pi_1}(s_3) = 0\gamma^0 + 12\gamma^1 = 6\).</li> <li>\(v^{\pi_1}(s_4) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(v^{\pi_1}(s_5) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(v^{\pi_1}(s_6) = 0\).</li> </ul> <p><strong>Policy 2 (\(\pi_2\)): Select <code class="language-plaintext highlighter-rouge">Right</code> Action Always</strong></p> <ul> <li>\(v^{\pi_2}(s_1) = 0\).</li> <li>\(v^{\pi_2}(s_2) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(v^{\pi_2}(s_3) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(v^{\pi_2}(s_4) = 0\gamma^0 + 2\gamma^1 = 1\).</li> <li>\(v^{\pi_2}(s_5) = 2\gamma^0 = 2\).</li> <li>\(v^{\pi_2}(s_6) = 0\).</li> </ul> <h2 id="action-value-function">Action-Value Function</h2> <p>The <em>Action-Value Function</em> \(q^\pi(s,a)\), or <em>Q-function</em>, evaluates the expected discounted return if the agent takes action \(a\) in state \(s\) and follows policy \(\pi\) thereafter.</p> \[\begin{aligned} q^\pi(s,a) &amp;:= \mathbf{E}\ \Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, A_t = a, \pi\ \Bigg] \\ &amp;:= \mathbf{E}[G_t|S_t=s, A_t=a, \pi] \\ &amp;:= \mathbf{E}\ \Bigg[\sum_{t=0}^{\infty}\gamma^k R_{t} \bigg| S_0=s, A_0=a, \pi\ \Bigg] \end{aligned}\] <h3 id="a-simple-mdp-example-again">A Simple MDP Example, Again</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mdp-simple-480.webp 480w,/assets/img/rl/mdp-simple-800.webp 800w,/assets/img/rl/mdp-simple-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rl/mdp-simple.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Simple MDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></center> </div> <p><strong>Policy 1 (\(\pi_1\)): Select <code class="language-plaintext highlighter-rouge">Left</code> Action Always</strong></p> <ul> <li>\(q^{\pi_1}(s_1, L) = 0\).</li> <li>\(q^{\pi_1}(s_1, R) = 0\).</li> <li>\(q^{\pi_1}(s_2, L) = 12\gamma^0 =12\).</li> <li>\(q^{\pi_1}(s_2, R) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(q^{\pi_1}(s_3, L) = 0\gamma^0 + 12\gamma^1 = 6\).</li> <li>\(q^{\pi_1}(s_3, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(q^{\pi_1}(s_4, L) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(q^{\pi_1}(s_4, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 0\gamma^3 + 12\gamma^4 = 0.75\).</li> <li>\(q^{\pi_1}(s_5, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(q^{\pi_1}(s_5, R) = 2\gamma^0 = 2\).</li> <li>\(q^{\pi_1}(s_6, L) = 0\).</li> <li>\(q^{\pi_1}(s_6, R) = 0\).</li> </ul> <p><strong>Policy 2 (\(\pi_2\)): Select <code class="language-plaintext highlighter-rouge">Right</code> Action Always</strong></p> <ul> <li>\(q^{\pi_2}(s_1, L) = 0\).</li> <li>\(q^{\pi_2}(s_1, R) = 0\).</li> <li>\(q^{\pi_2}(s_2, L) = 12\gamma^0 =12\).</li> <li>\(q^{\pi_2}(s_2, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(q^{\pi_2}(s_3, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 0\gamma^3 + 2\gamma^4 = 0.125\).</li> <li>\(q^{\pi_2}(s_3, R) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(q^{\pi_2}(s_4, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(q^{\pi_2}(s_4, R) = 0\gamma^0 + 2\gamma^1 = 1\).</li> <li>\(q^{\pi_2}(s_5, L) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(q^{\pi_2}(s_5, R) = 2\gamma^0 = 2\).</li> <li>\(q^{\pi_2}(s_6, L) = 0\).</li> <li>\(q^{\pi_2}(s_6, R) = 0\).</li> </ul> <h2 id="the-bellman-equation-for-vpi">The Bellman Equation for \(v^\pi\)</h2> <p>The <em>Bellman Equation for</em> \(v^\pi\) is a recursive expression for the state-value function. To derive the Bellman equation, we first isolate the immediate reward from the state-value function:</p> \[\begin{aligned} v^\pi(s) &amp;:= \textbf{E}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg\vert S_t=s, \pi\right] \\ &amp;= \textbf{E}\left[R_t + \sum_{k=1}^{\infty}\gamma^k R_{t+k} \bigg\vert S_t=s, \pi\right] \\ &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=1}^{\infty}\gamma^{k-1} R_{t+k} \bigg\vert S_t=s, \pi\right] \end{aligned}\] <p>We can perform a simple transformation by modifying the indexing of the sum to start at zero instead of one, which changes all uses of \(k\) within the sum to \(k+1\):</p> \[\begin{aligned} \textbf{E}\left[\gamma\sum_{k=1}^{\infty}\gamma^{k-1} R_{t+k} \bigg\vert S_t=s, \pi\right] = \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \end{aligned}\] <p>Recalling the law of total probability, \(\textbf{E}[X] = \textbf{E}[\textbf{E}[X\vert Y]]\), if we use the first action \(A_t=a\) and the first next state \(S_{t+1}=s^\prime\) as intermediate conditioning variables, we can rewrite the expected reward after the first action term as:</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ = &amp;\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) \times \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t=a, S_{t+1}=s^\prime, \pi\right] \end{aligned}\] <p>Furthermore, the Markov property tells us the future is independent of everything before \(t+1\). Hence, for the expected next-step reward, we can safely remove \(S_t\) and \(A_t\) from the conditioning:</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ = &amp;\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) \times \gamma\textbf{E}\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, \pi\right] \end{aligned}\] <p>Recall the definition of the state-value function, the last term is exactly \(v^\pi(s^\prime)\):</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, \pi\right] \\ = &amp; \gamma\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) v^\pi(s^\prime) \end{aligned}\] <p>Similarly, since for any fixed \(s\) and \(a\) the transition probability to the next state \(s^\prime\) must be 1, we can multiply the immediate reward \(R_t\) by “one”:</p> \[\begin{aligned} R_t = \sum_{a\in\mathcal{A}}\pi(s,a)R(s,a) = \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)R(s,a) \end{aligned}\] <p>Now, we put everything together:</p> \[\begin{aligned} v^\pi(s) &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ &amp;= \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)R(s,a) + \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) \gamma v^\pi(s^\prime) \end{aligned}\] <p>Now, we can combine the common terms to get the final simplified form of the state-value function:</p> \[v^\pi(s) = \boxed{\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\big(R(s,a) +\gamma v^\pi(s^\prime)\big)}\] <h3 id="pros-about-bellman-equation">Pros about Bellman Equation</h3> <div class="callout"> <p>We can view the Bellman equation as breaking the expected return that will occur into two parts:</p> <ol> <li>the reward that we will obtain during the next time step (<em>immediate reward</em>)</li> <li>the value of the next state that we end up in</li> </ol> \[v^\pi(s) = \textbf{E}\left[\underbrace{R(s,A_t)}_{\text{immediate reward}} + \gamma\underbrace{v^\pi(S_{t+1})}_{\text{value of next state}}\Bigg\vert S_t=s, \pi\right]\] <p>While the original definition of the value function must consider the entire sequence of states, the Bellman equation on the other hand, <strong>only needs to look forward one time step into the future</strong>.</p> <ul> <li>the recurrent nature of the Bellman equation makes it more computationally helpful</li> </ul> </div> <h2 id="the-bellman-equation-for-qpi">The Bellman Equation for \(q^\pi\)</h2> <p>While the Bellman equation for \(v^\pi\) is a recursive expression for \(v^\pi\), the Bellman equation for \(q^\pi\) is a recursive expression for the action-value function \(q^\pi\).</p> \[\begin{aligned} q^\pi(s,a) &amp;:= \mathbf{E}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, A_t = a, \pi\Bigg] \\ &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t = a, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}} \pi(s^\prime,a^\prime)\times\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t = a, S_{t+1}=s^\prime, A_{t+1}=a^\prime, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime) \times\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, A_{t+1}=a^\prime, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)\gamma q^\pi(s^\prime, s^\prime) \\ &amp;= \boxed{R(s,a) + \gamma\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)q^\pi(s^\prime, a^\prime)} \end{aligned}\] <h2 id="optimal-value-functions">Optimal Value Functions</h2> <div class="callout"> <p><strong>Optimal Policy</strong> \(\pi^*\) An optimal policy, \(\pi^*\) is any policy that is at least as good as all other policies. In other words, \(\pi^*\) is an optimal policy if and only if</p> \[\forall \pi \in \Pi, \pi^* \geq \pi\] </div> <div class="callout"> <p>Notice that even when \(\pi^*\) is not unique, the optimal value functions \(v^*\) and \(q^*\) are unique—all optimal policies share the same state-value function and action-value function.</p> </div> <div class="callout"> <details> <summary> Given the optimal state-value function, can you compute the optimal policy if you do not know the transition probabilities and reward function? </summary> <br><strong>No</strong>. $$ \arg\max_{a\in\mathcal{A}}\sum_{s^\prime}p(s,a,s^\prime)[R(s,a) + \gamma v^\pi(s^\prime)] $$ is an optimal action in state s. Computing these actions requires knowledge of p and R </details> </div> <div class="callout"> <details> <summary> Given the optimal action-value function, can you compute the optimal policy if you do not know the transition probabilities and reward function? </summary> <br><strong>Yes</strong>. $$ \arg\max_{a\in\mathcal{A}}q^*(s,a) $$ is an optimal action in state s </details> </div> <h3 id="bellman-optimality-equation-for-v">Bellman Optimality Equation for \(v^*\)</h3> <p>The <em>Bellman Optimality Equation for</em> \(v^*\) is a recursive expression for \(v^*\). Let’s start with the Bellman equation:</p> \[v^*(s) = \sum_{a\in\mathcal{A}}\pi^*(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^*(s^\prime)]\] <p>Since the optimal policy \(\pi^*\) only picks the action that maximizes \(q^*(s,a)\), we do not need to consider all possible actions \(a\), but only those that cause the \(q^*(s,a)\) term to be maximized:</p> \[v^*(s) = \max_{a\in\mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^*(s^\prime)]\] <p>The equation above is the <em>Bellman optimality equation</em> <em>for</em> \(v^*\).</p> <div class="callout"> <p>A policy \(\pi\), <em>satisfies the Bellman optimality equation</em> if for all states \(s \in \mathcal{S}\):</p> \[v^*(s) = \max_{a\in\mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^*(s^\prime)]\] </div> <h3 id="bellman-optimality-equation-for-q">Bellman Optimality Equation for \(q^*\)</h3> <div class="callout"> <p>A policy \(\pi\), satisfies the Bellman optimality equation if for all actions \(a \in \mathcal{A}\):</p> \[q^*(s,a) = \sum_{s^\prime \in \mathcal{S}} p(s,a,s^\prime)\left[ R(s,a) + \gamma \max_{a^\prime\in\mathcal{A}}q^*(s^\prime, a^\prime)\right]\] </div> <h3 id="bellman-optimality-equation-and-the-optimal-policy">Bellman Optimality Equation and the Optimal Policy</h3> <div class="callout"> <p><em>If a policy <strong>\(\pi\)</strong> satisfies the Bellman optimality equation, then <strong>\(\pi\)</strong> is an optimal policy.</em></p> </div> <p><em>Proof:</em></p> <p>Assuming a policy \(\pi\) satisfies the Bellman optimality equation, we have for all states \(s\):</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^\pi(s^\prime)]\] <p>We can apply the Bellman optimality equation recursively into the expression and replace \(v^\pi(s^\prime)\):</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma v^\pi(s^{\prime\prime})\right)\right]\] <p>We could continue this process indefinitely until \(\pi\) is completely eliminated from the expression:</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right]\] <p>At each time \(t\), the action is chosen that maximizes the expected discounted sum of future rewards, given that future actions are also chosen to maximize the discounted sum of future rewards.</p> <p>Now, let’s consider any new policy \(\pi^\prime\). What will be the relationship if we replace \(\max_{a \in \mathcal{A}}\) with \(\sum_{a \in \mathcal{A}}\pi^\prime(s,a)\)? We argue that the expression could not become bigger than the previous one. That is, for any policy \(\pi^\prime\):</p> \[\begin{aligned} v^\pi(s) &amp;= \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \\ &amp;\geq \sum_{a \in \mathcal{A}}\pi^\prime(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\sum_{a^\prime \in \mathcal{A}}\pi^\prime(s^\prime,a^\prime)\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \end{aligned}\] <p>Given that the above holds for all policies \(\pi^\prime\), we have that for all states \(s \in \mathcal{S}\) and all policies \(\pi^\prime \in \Pi\):</p> \[\begin{aligned} v^\pi(s) &amp;= \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \\ &amp;\geq \mathbf{E}[G_t | S_t = s, \pi^\prime] \\ &amp;= v^{\pi^\prime}(s) \end{aligned}\] <p>Hence, for all states \(s \in \mathcal{S}\), and all policies \(\pi^\prime \in \Pi\), \(v^\pi(s) \geq v^{\pi^\prime}(s)\). In other words, for all policies \(\pi^\prime \in \Pi\), we have that \(\pi \geq \pi^\prime\), and hence \(\pi\) is an optimal policy.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Tianyi Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"publications",description:"\u201c\u5f88\u60ed\u6127\uff0c\u5c31\u505a\u4e86\u4e00\u70b9\u5fae\u5c0f\u7684\u5de5\u4f5c\uff0c\u8c22\u8c22\u5927\u5bb6\u3002\u201d",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-repositories",title:"repositories",description:"\u201c\u6211\u4eec\u5fc5\u987b\u53bb\u52aa\u529b\u5b66\u4e60\u65b0\u4e1c\u897f\u3002\u5373\u4f7f\u6211\u4eec\u9047\u5230\u4e86\u95ee\u9898\uff0c\u5927\u5bb6\u4e5f\u4f1a\u7406\u89e3\u7684\u3002\u201d",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-reinforcement-learning-value-functions",title:"Reinforcement Learning \u2014 Value Functions",description:"A deep dive into value functions and the Bellman equation in Reinforcement Learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/value-functions/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u4ef7\u503c\u51fd\u6570",title:"\u5f3a\u5316\u5b66\u4e60 \u2014 \u4ef7\u503c\u51fd\u6570",description:"\u6df1\u5165\u63a2\u8ba8\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4ef7\u503c\u51fd\u6570\u4e0e\u8d1d\u5c14\u66fc\u65b9\u7a0b\u3002",section:"Posts",handler:()=>{window.location.href="/blog/2025/value-functions-zh/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e0e\u5f3a\u5316\u5b66\u4e60",title:"\u5f3a\u5316\u5b66\u4e60 - \u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e0e\u5f3a\u5316\u5b66\u4e60",description:"\u5f3a\u5316\u5b66\u4e60\u6982\u5ff5\u5165\u95e8\uff0c\u5305\u62ec\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u3002",section:"Posts",handler:()=>{window.location.href="/blog/2025/mdp-and-rl-zh/"}},{id:"post-reinforcement-learning-mdp-and-rl",title:"Reinforcement Learning - MDP and RL",description:"An introduction to Reinforcement Learning concepts, including Markov Decision Processes (MDPs).",section:"Posts",handler:()=>{window.location.href="/blog/2025/mdp-and-rl/"}},{id:"post-reinforcement-learning-mathematical-foundations",title:"Reinforcement Learning - Mathematical Foundations",description:"A very, very, very large chunk of math",section:"Posts",handler:()=>{window.location.href="/blog/2025/math-foundations-for-RL/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u6570\u5b66\u57fa\u7840",title:"\u5f3a\u5316\u5b66\u4e60 - \u6570\u5b66\u57fa\u7840",description:"\u8d85\u5927\u4e00\u5768\u6570\u5b66\u516c\u5f0f",section:"Posts",handler:()=>{window.location.href="/blog/2025/math-foundations-for-RL-zh/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>