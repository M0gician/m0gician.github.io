<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reinforcement Learning—Mathematical Foundations | Tianyi Yang </title> <meta name="author" content="Tianyi Yang"> <meta name="description" content="A very, very, very large chunk of math"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Nunito+Sans:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.gif?7e955dea0276164422304bc0ee30bce6"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://m0gician.github.io/blog/2025/math-foundations-for-RL/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tianyi</span> Yang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning—Mathematical Foundations</h1> <p class="post-meta"> Created in July 04, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/rl"> <i class="fa-solid fa-hashtag fa-sm"></i> RL</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>   ·   <a href="/blog/category/reinforcement-learning"> <i class="fa-solid fa-tag fa-sm"></i> reinforcement-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="mdp-markov-decision-process">MDP (Markov Decision Process)</h2> <p>We usually define an MDP as a tuple \((\mathcal{S}, \mathcal{A}, p, R, \gamma)\).</p> <div class="callout"> <p><strong>State Set (\(\mathcal{S}\)):</strong> The set of all possible states of the environment.</p> <ul> <li>The state at time \(t\), \(S_t\), always takes values in \(\mathcal{S}\).</li> </ul> </div> <div class="callout"> <p><strong>Action Set (\(\mathcal{A}\)):</strong> The set of all possible actions the agent can take.</p> <ul> <li>The action at time \(t\), \(A_t\), always takes values in \(\mathcal{A}\).</li> </ul> </div> <div class="callout"> <p><strong>Transition Function (\(p\)):</strong> Describes how the state of the environment changes.</p> \[p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]\] <p>For all \(s \in \mathcal{S}\), \(a \in \mathcal{A}\), \(s’ \in \mathcal{S}\), and \(t \in \mathbb{N}_{\geq 0}\):</p> \[p(s,a,s') := \text{Pr}(S_{t+1}=s' | S_t=s, A_t=a)\] <p>A transition function is deterministic if \(p(s,a,s’) \in \{0,1\}\) for all s, a, and s’</p> </div> <div class="callout"> <p>\(d_R\) describes how rewards are generated.</p> \[R_t \sim d_r(S_t, A_t, S_{t+1})\] </div> <div class="callout"> <p><strong>Reward Function (\(R\)):</strong> A function implicitly defined by the reward distribution \(d_R\), which describes how rewards are generated.</p> \[R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\] \[R(s,a) := \mathrm{R}[R_t|S_t = s, A_t = a]\] </div> <div class="callout"> <p><strong>Initial State Distribution (\(d_0\)):</strong></p> \[d_0: \mathcal{S} \rightarrow [0,1]\] \[d_0(s) = \text{Pr}(S_0=s)\] </div> <div class="callout"> <p><strong>Discount Factor (\(\gamma\)):</strong> A parameter in \([0,1]\) that discounts future rewards.</p> </div> <hr> <h3 id="objective">Objective</h3> <p>The goal is to find an optimal policy \(\pi^*\) that maximizes the expected total amount of discounted reward.</p> <ul> <li>\(G^i\) denotes the return of the i-th episode.</li> <li>\(R^i_t\) denotes the reward at time \(t\) during episode \(i\).</li> </ul> <div class="callout"> <p><strong>Objective function (\(J\)):</strong></p> \[J : \Pi \rightarrow \mathbb{R}, \text{where for all } \pi \in \Pi\] \[\begin{aligned} &amp;J(\pi) := \mathrm{E}\left[\sum_{t=1}^{\infty} \gamma^tR_t \bigg| \pi\right] \\ &amp;\hat{J}(\pi) := \frac{1}{N}\sum_{i=1}^{N}G^i = \frac{1}{n}\sum_{i=1}^{N}\sum_{t=0}^{\infty}\gamma^t R_t^i \end{aligned}\] </div> <div class="callout"> <p><strong>Optimal Policy (\(\pi^*\)):</strong></p> \[\pi^* \in \underset{\pi \in \Pi}{\text{argmax}}\,J(\pi)\] </div> <details> <summary>Is the optimal policy always unique when it exists?</summary> No. There can exist multiple optimal policies that are equally good. </details> <hr> <h3 id="properties">Properties</h3> <div class="callout"> <p><strong>Horizon (\(L\)):</strong> The smallest integer \(L\) such that for all \(t \geq L\), the probability of being in a terminal state \(s_\infty\) is 1.</p> \[\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\] <ul> <li>The MDP is <strong>finite horizon</strong> (episodic) if \(L &lt; \infty\) for all policies.</li> <li>The MDP is <strong>infinite horizon</strong> (continuous) when \(L = \infty\).</li> </ul> </div> <div class="callout"> <p><strong>Markov Property:</strong> A property of the state representation. It assumes that the future is independent of the past given the present.</p> <ul> <li>\(S_{t+1}\) is conditionally independent of the history \(H_{t-1}\) given the current state \(S_t\).</li> </ul> </div> <hr> <h2 id="policy">Policy</h2> <div class="callout"> <p>A <strong>policy</strong> is a decision rule—a way that the agent can select actions.</p> \[\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]\] \[\pi(s,a) := \text{Pr}(A_t=a | S_t=s)\] </div> <hr> <h2 id="value-functions">Value Functions</h2> <div class="callout"> <p><strong>State-Value Function (\(v^\pi\))</strong></p> <p>The state-value function \(v^\pi : \mathcal{S} \rightarrow \mathbb{R}\) measures the expected return starting from a state \(s\) and following policy \(\pi\).</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\left[\sum_{k=1}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, \pi\right] \\ &amp;:= \mathbf{E}[G_t|S_t=s, \pi] \end{aligned}\] </div> <div class="callout"> <p><strong>Action-Value Function (Q-function, \(q^\pi\))</strong></p> <p>The action-value function \(q^\pi : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) measures the expected return starting from state \(s\), taking action \(a\), and then following policy \(\pi\).</p> \[q^\pi(s,a) := \mathbf{E}[G_t | S_t=s, A_t=a, \pi]\] </div> <hr> <h3 id="bellman-equations">Bellman Equations</h3> <div class="callout"> <p><strong>Bellman Equation for the State-Value Function (\(v^\pi\))</strong></p> \[\begin{aligned} v^\pi(s) &amp;= \mathbf{E}\left[\underbrace{R(s,A_t)}_{\text{immediate reward}} + \gamma \underbrace{v^\pi(S_{t+1}) }_{\text{value of next state}} \bigg| S_t = s, \pi\right] \\[0.3cm] &amp;= \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s')(R(s,a) + \gamma v^\pi(s')) \end{aligned}\] <ul> <li>The Bellman equation only needs to look forward one time step into the future.</li> <li>The optimal state-value function, \(v^*\), is unique—all optimal policies share the same state-value function.</li> </ul> </div> <div class="callout"> <p><strong>Bellman Equation for the Action-Value Function (\(q^\pi\))</strong></p> \[q^\pi(s,a) = R(s,a) + \gamma\sum_{s' \in \mathcal{S}}p(s,a,s')\sum_{a' \in \mathcal{A}}\pi(s',a')q^\pi(s',a')\] <ul> <li>The optimal action-value function, \(q^*\), is also unique among all optimal policies.</li> </ul> </div> <h3 id="bellman-optimality-equations">Bellman Optimality Equations</h3> <ol> <li>If a policy \(\pi\) satisfies the Bellman optimality equation, then \(\pi\) is an optimal policy.</li> <li>If the state and action sets are finite, rewards are bounded, and \(\gamma &lt; 1\), then there exists a policy \(\pi\) that satisfies the Bellman optimality equation.</li> </ol> <div class="callout"> <p><strong>Bellman Optimality Equation for \(v^*\)</strong></p> <p>A policy \(\pi\) satisfies the Bellman optimality equation if for all states \(s\in\mathcal{S}\):</p> \[v^\pi(s) = \max_{a\in\mathcal{A}}\sum_{s'\in\mathcal{S}}p(s,a,s')[R(s,a)+\gamma v^\pi(s')]\] </div> <div class="callout"> <p><strong>Bellman Optimality Equation for \(q^*\)</strong></p> \[q^*(s,a) = \sum_{s'\in\mathcal{S}}p(s,a,s')\left[R(s,a) + \gamma\max_{a'\in\mathcal{A}}q^*(s',a')\right]\] </div> <hr> <h2 id="policy-iteration">Policy Iteration</h2> <p>Policy iteration is an algorithm that finds an optimal policy by alternating between two steps: policy evaluation and policy improvement.</p> <ul> <li>Even though policy evaluation using dynamic programming is guaranteed to converge to \(v^\pi\), it is not guaranteed to reach \(v^\pi\) in a finite amount of computation.</li> </ul> <div class="callout"> <p><strong>Policy Improvement Theorem</strong></p> <p>For any policy \(\pi\), if \(\pi’\) is a deterministic policy such that \(\forall s \in \mathcal{S}\):</p> \[q^\pi(s, \pi'(s)) \geq v^\pi(s)\] <p>then \(\pi’ \geq \pi\).</p> </div> <div class="callout"> <p><strong>Policy Improvement Theorem for Stochastic Policies</strong></p> <p>For any policy \(\pi\), if \(\pi’\) satisfies:</p> \[\sum_{a\in\mathcal{A}}\pi'(s,a) q^\pi(s,a) \geq v^\pi(s),\] <p>for all \(s \in \mathcal{S}\), then \(\pi' \geq \pi\).</p> </div> <hr> <h2 id="value-iteration">Value Iteration</h2> <p>Value iteration is an algorithm that finds the optimal state-value function by iteratively applying the Bellman optimality update.</p> <div class="callout"> <p><strong>Banach Fixed-Point Theorem</strong></p> <p>If \(f\) is a contraction mapping on a non-empty complete normed vector space, then \(f\) has a unique fixed point, \(x^*\), and the sequence defined by \(x_{k+1} = f(x_k)\), with \(x_0\) chosen arbitrarily, converges to \(x^*\).</p> </div> <div class="callout"> <p><strong>Bellman Operator is a Contraction Mapping</strong></p> <p>The Bellman operator is a contraction mapping on \(\mathbb{R}^{\vert\mathcal{S}\vert}\) with distance metric \(d(v,v’) := \max_{s\in\mathcal{S}}\vert v(s)-v’(s) \vert\) if \(\gamma &lt; 1\).</p> </div> <ul> <li>Value iteration <strong>converges</strong> to a unique fixed point \(v^\infty\) for all MDPs with finite state and action sets, bounded rewards, and \(\gamma &lt; 1\).</li> <li>All MDPs with finite state and action sets, bounded rewards, and \(\gamma &lt; 1\) <strong>have at least one optimal policy</strong>.</li> </ul> <hr> <h2 id="law-of-large-numbers">Law of Large Numbers</h2> <div class="callout"> <p><strong>Khintchine’s Strong Law of Large Numbers</strong></p> <p>Let \(\{X_i\}_{i=1}^{\infty}\) be <strong>independent and identically distributed (i.i.d.) random variables</strong>. Then the sequence of sample averages \((\frac{1}{n} \sum_{i=1}^{n} X_i)_{n=1}^\infty\) converges <strong>almost surely</strong> to the expected value \(\mathbf{E}[X_1]\).</p> <p>i.e., \(\displaystyle \frac{1}{n}\sum_{i=1}^{\infty} X_i \overset{a.s.}{\rightarrow}\mathbf{E}[X_1]\)</p> </div> <div class="callout"> <p><strong>Kolmogorov’s Strong Law of Large Numbers</strong></p> <p>Let \(\{X_i\}^\infty_{i=1}\) be <strong>independent (not necessarily identically distributed) random variables</strong>. If all \(X_i\) have the <strong>same mean and bounded variance</strong>, then the sequence of sample averages \((\frac{1}{n}\sum_{i=1}^n X_i)^\infty_{n=1}\) converges almost surely to \(\mathbf{E}[X_1]\).</p> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Tianyi Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"publications",description:"\u201c\u5f88\u60ed\u6127\uff0c\u5c31\u505a\u4e86\u4e00\u70b9\u5fae\u5c0f\u7684\u5de5\u4f5c\uff0c\u8c22\u8c22\u5927\u5bb6\u3002\u201d",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-repositories",title:"repositories",description:"\u201c\u6211\u4eec\u5fc5\u987b\u53bb\u52aa\u529b\u5b66\u4e60\u65b0\u4e1c\u897f\u3002\u5373\u4f7f\u6211\u4eec\u9047\u5230\u4e86\u95ee\u9898\uff0c\u5927\u5bb6\u4e5f\u4f1a\u7406\u89e3\u7684\u3002\u201d",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-reinforcement-learning-mathematical-foundations",title:"Reinforcement Learning\u2014Mathematical Foundations",description:"A very, very, very large chunk of math",section:"Posts",handler:()=>{window.location.href="/blog/2025/math-foundations-for-RL/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u6570\u5b66\u57fa\u7840",title:"\u5f3a\u5316\u5b66\u4e60\u2014\u6570\u5b66\u57fa\u7840",description:"\u8d85\u5927\u4e00\u5768\u6570\u5b66\u516c\u5f0f",section:"Posts",handler:()=>{window.location.href="/blog/2025/math-foundations-for-RL-cn/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>