<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 强化学习—数学基础 | Tianyi Yang </title> <meta name="author" content="Tianyi Yang"> <meta name="description" content="超大一坨数学公式"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Nunito+Sans:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.gif?7e955dea0276164422304bc0ee30bce6"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://m0gician.github.io/blog/2025/math-foundations-for-RL-cn/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tianyi</span> Yang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">强化学习—数学基础</h1> <p class="post-meta"> Created in July 04, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/rl"> <i class="fa-solid fa-hashtag fa-sm"></i> RL</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>   ·   <a href="/blog/category/reinforcement-learning"> <i class="fa-solid fa-tag fa-sm"></i> reinforcement-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <style>.definition{background-color:#e7f3fe;border-left:6px solid #2196f3;padding:10px;margin-bottom:15px}</style> <h2 id="mdp马尔可夫决策过程">MDP（马尔可夫决策过程）</h2> <p>我们通常把一个 MDP 定义为一个元组 \((\mathcal{S}, \mathcal{A}, p, R, \gamma)\)。</p> <div class="definition"> <p><strong>状态集合（\(\mathcal{S}\)）：</strong> 环境所有可能状态的集合。</p> <ul> <li>时刻 \(t\) 的状态 \(S_t\) 总是取值于 \(\mathcal{S}\)。</li> </ul> </div> <div class="definition"> <p><strong>动作集合（\(\mathcal{A}\)）：</strong> 智能体可以采取的所有可能动作的集合。</p> <ul> <li>时刻 \(t\) 的动作 \(A_t\) 总是取值于 \(\mathcal{A}\)。</li> </ul> </div> <div class="definition"> <p><strong>转移函数（\(p\)）：</strong> 描述环境状态如何变化。</p> \[p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]\] <p>对于所有 \(s \in \mathcal{S}\)、\(a \in \mathcal{A}\)、\(s' \in \mathcal{S}\) 以及 \(t \in \mathbb{N}_{\ge 0}\)：</p> \[p(s,a,s') := \Pr(S_{t+1}=s' \mid S_t=s, A_t=a)\] <p>当 \(p(s,a,s') \in \{0,1\}\) 对所有的 \(s,a,s'\) 成立时，转移函数是确定性的。</p> </div> <div class="definition"> <p>\(d_R\) 描述奖励的生成方式。</p> \[R_t \sim d_R(S_t, A_t, S_{t+1})\] </div> <div class="definition"> <p><strong>奖励函数（\(R\)）：</strong> 由奖励分布 \(d_R\) 隐式定义的函数，描述奖励如何生成。</p> \[R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\] \[R(s,a) := \mathrm{E}[R_t \mid S_t = s, A_t = a]\] </div> <div class="definition"> <p><strong>初始状态分布（\(d_0\)）：</strong></p> \[d_0: \mathcal{S} \rightarrow [0,1]\] \[d_0(s) = \Pr(S_0 = s)\] </div> <div class="definition"> <p><strong>折扣因子（\(\gamma\)）：</strong> 取值范围 \([0,1]\)，用于折扣未来奖励。</p> </div> <hr> <h3 id="目标">目标</h3> <p>我们的目标是找到一条最优策略 \(\pi^*\)，使得期望累计折扣奖励最大化。</p> <ul> <li>\(G^i\) 表示第 \(i\) 个回合的回报（return）。</li> <li>\(R_t^i\) 表示第 \(i\) 个回合时刻 \(t\) 的奖励。</li> </ul> <div class="definition"> <p><strong>目标函数（\(J\)）：</strong></p> \[J : \Pi \rightarrow \mathbb{R}, \quad \text{对于所有 }\pi \in \Pi\] \[\begin{aligned} J(\pi) &amp;:= \mathrm{E}\Bigg[\sum_{t=1}^{\infty} \gamma^t R_t \,\Big|\, \pi\Bigg] \\[0.2cm] \hat{J}(\pi) &amp;:= \frac{1}{N}\sum_{i=1}^{N} G^i = \frac{1}{N}\sum_{i=1}^{N}\sum_{t=0}^{\infty}\gamma^t R_t^i \end{aligned}\] </div> <div class="definition"> <p><strong>最优策略（\(\pi^*\)）：</strong></p> \[\pi^* \in \arg\max_{\pi \in \Pi} J(\pi)\] </div> <details> <summary>当最优策略存在时它总是唯一的吗？</summary> 不一定，可能存在多条同样优秀的最优策略。 </details> <hr> <h3 id="性质">性质</h3> <div class="definition"> <p><strong>时限（Horizon，\(L\)）：</strong> 最小的整数 \(L\)，使得对所有 \(t \ge L\)，处于终止状态 \(s_\infty\) 的概率为 1。</p> \[\forall t \ge L,\; \Pr(S_t = s_\infty) = 1\] <ul> <li>若 \(L &lt; \infty\)（对所有策略均成立），则 MDP 为 <strong>有限时限</strong>（回合式）。</li> <li>若 \(L = \infty\)，则 MDP 为 <strong>无限时限</strong>（连续式）。</li> </ul> </div> <div class="definition"> <p><strong>马尔可夫性（Markov Property）：</strong> 一种关于状态表示的性质，假设在给定当前状态的条件下，未来与过去独立。</p> <ul> <li>给定当前状态 \(S_t\)，\(S_{t+1}\) 与历史 \(H_{t-1}\) 条件独立。</li> </ul> </div> <hr> <h2 id="策略policy">策略（Policy）</h2> <div class="definition"> <p><strong>策略</strong> 是一种决策规则——智能体选择动作的方式。</p> \[\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]\] \[\pi(s,a) := \Pr(A_t = a \mid S_t = s)\] </div> <hr> <h2 id="价值函数value-functions">价值函数（Value Functions）</h2> <div class="definition"> <p><strong>状态价值函数（\(v^\pi\)）</strong></p> <p>状态价值函数 \(v^\pi : \mathcal{S} \rightarrow \mathbb{R}\) 衡量从状态 \(s\) 出发并遵循策略 \(\pi\) 时的期望回报。</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\Bigg[\sum_{k=1}^{\infty}\gamma^k R_{t+k} \,\Big|\, S_t = s, \pi\Bigg] \\[0.2cm] &amp;:= \mathbf{E}[G_t \mid S_t = s, \pi] \end{aligned}\] </div> <div class="definition"> <p><strong>动作价值函数（Q 函数，\(q^\pi\)）</strong></p> <p>动作价值函数 \(q^\pi : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) 衡量在状态 \(s\) 采取动作 \(a\) 后再遵循策略 \(\pi\) 所得到的期望回报。</p> \[q^\pi(s,a) := \mathbf{E}[G_t \mid S_t = s, A_t = a, \pi]\] </div> <hr> <h3 id="bellman-方程">Bellman 方程</h3> <div class="definition"> <p><strong>状态价值函数的 Bellman 方程（\(v^\pi\)）</strong></p> \[\begin{aligned} v^\pi(s) &amp;= \mathbf{E}\Big[R(s,A_t) + \gamma v^\pi(S_{t+1}) \,\Big|\, S_t = s, \pi\Big] \\[0.3cm] &amp;= \sum_{a \in \mathcal{A}} \pi(s,a) \sum_{s' \in \mathcal{S}} p(s,a,s')\big(R(s,a) + \gamma v^\pi(s')\big) \end{aligned}\] <ul> <li>Bellman 方程只需向前看一步。</li> <li>最优状态价值函数 \(v^*\) 是唯一的——所有最优策略共享同一 \(v^*\)。</li> </ul> </div> <div class="definition"> <p><strong>动作价值函数的 Bellman 方程（\(q^\pi\)）</strong></p> \[q^\pi(s,a) = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} p(s,a,s') \sum_{a' \in \mathcal{A}} \pi(s',a') q^\pi(s',a')\] <ul> <li>最优动作价值函数 \(q^*\) 对所有最优策略也是唯一的。</li> </ul> </div> <h3 id="bellman-最优方程">Bellman 最优方程</h3> <ol> <li>若一条策略 \(\pi\) 满足 Bellman 最优方程，则 \(\pi\) 是最优策略。</li> <li>若状态、动作集合有限，奖励有界且 \(\gamma &lt; 1\)，那么存在满足 Bellman 最优方程的策略 \(\pi\)。</li> </ol> <div class="definition"> <p><strong>\(v^*\) 的 Bellman 最优方程</strong></p> <p>对所有状态 \(s \in \mathcal{S}\)：</p> \[v^\pi(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} p(s,a,s') \big[R(s,a) + \gamma v^\pi(s')\big]\] </div> <div class="definition"> <p><strong>\(q^*\) 的 Bellman 最优方程</strong></p> \[q^*(s,a) = \sum_{s' \in \mathcal{S}} p(s,a,s') \big[R(s,a) + \gamma \max_{a' \in \mathcal{A}} q^*(s',a')\big]\] </div> <hr> <h2 id="策略迭代policy-iteration">策略迭代（Policy Iteration）</h2> <p>策略迭代通过交替执行两步——策略评估与策略改进——来寻找最优策略。</p> <ul> <li>通过动态规划进行的策略评估虽然保证收敛到 \(v^\pi\)，但并不保证在有限计算内就能到达。</li> </ul> <div class="definition"> <p><strong>策略改进定理</strong></p> <p>对于任意策略 \(\pi\)，若存在确定性策略 \(\pi'\) 使得 \(\forall s \in \mathcal{S}\)：</p> \[q^\pi(s, \pi'(s)) \ge v^\pi(s)\] <p>则有 \(\pi' \ge \pi\)。</p> </div> <div class="definition"> <p><strong>随机策略的策略改进定理</strong></p> <p>对于任意策略 \(\pi\)，若 \(\pi'\) 满足：</p> \[\sum_{a \in \mathcal{A}} \pi'(s,a) q^\pi(s,a) \ge v^\pi(s),\] <p>则 \(\forall s \in \mathcal{S}\)，都有 \(\pi' \ge \pi\)。</p> </div> <hr> <h2 id="价值迭代value-iteration">价值迭代（Value Iteration）</h2> <p>价值迭代通过迭代应用 Bellman 最优更新来寻找最优状态价值函数。</p> <div class="definition"> <p><strong>Banach 不动点定理</strong></p> <p>若映射 \(f\) 在非空完备赋范向量空间上是收缩映射，则存在唯一不动点 \(x^*\)，且以任意 \(x_0\) 为初始点、按照 \(x_{k+1} = f(x_k)\) 生成的序列收敛到 \(x^*\)。</p> </div> <div class="definition"> <p><strong>Bellman 算子是收缩映射</strong></p> <p>当 \(\gamma &lt; 1\) 时，在度量 \(d(v,v') := \max_{s \in \mathcal{S}} |v(s)-v'(s)|\) 下，Bellman 算子在 \(\mathbb{R}^{|\mathcal{S}|}\) 上是收缩映射。</p> </div> <ul> <li>对于有限状态动作集、有界奖励且 \(\gamma &lt; 1\) 的 MDP，价值迭代 <strong>收敛</strong> 到唯一的固定点 \(v^\infty\)。</li> <li>这类 MDP <strong>至少存在</strong> 一条最优策略。</li> </ul> <hr> <h2 id="大数定律law-of-large-numbers">大数定律（Law of Large Numbers）</h2> <div class="definition"> <p><strong>辛钦强大数定律（Khintchine’s Strong Law of Large Numbers）</strong></p> <p>设 \(\{X_i\}_{i=1}^{\infty}\) 为 <strong>独立同分布（i.i.d.）随机变量</strong>。则样本平均序列 \((\frac{1}{n} \sum_{i=1}^{n} X_i)_{n=1}^\infty\) <strong>几乎必然</strong> 收敛到期望 \(\mathbf{E}[X_1]\)。</p> <p>即 \(\displaystyle \frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{a.s.} \mathbf{E}[X_1]\)</p> </div> <div class="definition"> <p><strong>Kolmogorov 强大数定律</strong></p> <p>设 \(\{X_i\}_{i=1}^{\infty}\) 为 <strong>独立（不要求同分布）随机变量</strong>。若所有 \(X_i\) 具有 <strong>相同均值且方差有界</strong>，则样本平均序列 \((\frac{1}{n}\sum_{i=1}^{n} X_i)^\infty_{n=1}\) 亦几乎必然收敛到 \(\mathbf{E}[X_1]\)。</p> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Tianyi Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"publications",description:"\u201c\u5f88\u60ed\u6127\uff0c\u5c31\u505a\u4e86\u4e00\u70b9\u5fae\u5c0f\u7684\u5de5\u4f5c\uff0c\u8c22\u8c22\u5927\u5bb6\u3002\u201d",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-repositories",title:"repositories",description:"\u201c\u6211\u4eec\u5fc5\u987b\u53bb\u52aa\u529b\u5b66\u4e60\u65b0\u4e1c\u897f\u3002\u5373\u4f7f\u6211\u4eec\u9047\u5230\u4e86\u95ee\u9898\uff0c\u5927\u5bb6\u4e5f\u4f1a\u7406\u89e3\u7684\u3002\u201d",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-reinforcement-learning-mathematical-foundations",title:"Reinforcement Learning\u2014Mathematical Foundations",description:"A very, very, very large chunk of math",section:"Posts",handler:()=>{window.location.href="/blog/2025/math-foundations-for-RL/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u6570\u5b66\u57fa\u7840",title:"\u5f3a\u5316\u5b66\u4e60\u2014\u6570\u5b66\u57fa\u7840",description:"\u8d85\u5927\u4e00\u5768\u6570\u5b66\u516c\u5f0f",section:"Posts",handler:()=>{window.location.href="/blog/2025/math-foundations-for-RL-cn/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>