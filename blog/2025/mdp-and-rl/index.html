<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reinforcement Learning - MDP and RL | Tianyi Yang </title> <meta name="author" content="Tianyi Yang"> <meta name="description" content="An introduction to Reinforcement Learning concepts, including Markov Decision Processes (MDPs)."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Nunito+Sans:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.gif?7e955dea0276164422304bc0ee30bce6"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://m0gician.github.io/blog/2025/mdp-and-rl/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tianyi</span> Yang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning - MDP and RL</h1> <p class="post-meta"> Created in July 14, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/rl"> <i class="fa-solid fa-hashtag fa-sm"></i> RL</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>   ·   <a href="/blog/category/reinforcement-learning"> <i class="fa-solid fa-tag fa-sm"></i> reinforcement-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="what-is-reinforcement-learning">What is Reinforcement Learning?</h2> <div class="blockquote"> <p>"Reinforcement Learning is an area of machine learning, inspired by behaviorist psychology, concerned with how an agent can learn from interactions with an environment." <br>Sutton &amp; Barto (1998), Phil, <cite>Wikipedia</cite></p> </div> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/rl-system-480.webp 480w,/assets/img/rl/rl-system-800.webp 800w,/assets/img/rl/rl-system-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rl/rl-system.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="RL System" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></center> </div> <p>A typical Reinforcement Learning system consists of 5 components: an <strong>agent</strong> takes an <strong>action</strong> at each <strong>state</strong> in an <strong>environment</strong> and receives a <strong>reward</strong> if some criteria are met.</p> <div class="callout"> <details><summary><strong>Can a Supervised Learning problem be converted into a RL problem?</strong></summary> <strong>Yes</strong>. One might take a supervised learning problem and convert it into an RL problem (the state as the input to a classifier; the action as a label; and the reward as 1 if the label is correct and -1 otherwise).</details> </div> <div class="callout"> <details><summary><strong>Is RL an alternative to Supervised Learning?</strong></summary> <p><strong>No</strong>. Supervised learning uses instructive feedback (what action the agent should have taken). Anything deviates the provided feedback will be penalized.</p> <p>RL problems on the other hand aren’t provided as fixed data sets but as code or descriptions of the entire environment. Rewards in RL should convey how “good” an agent’s actions are, not what the best actions would have been. The goal of the agent is to maximize the total reward and this might require the agent forgoing immediate reward to obtain larger reward later.</p> <p>If you have a sequential problem or a problem where only evaluative feedback is available (or both!), then you should consider use RL.</p></details> </div> <h3 id="example-gridworld">Example: Gridworld</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/gridworld-480.webp 480w,/assets/img/rl/gridworld-800.webp 800w,/assets/img/rl/gridworld-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rl/gridworld.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Gridworld" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></center> </div> <p><strong>State</strong>: Position of robot. The robot does not have a direction that it is facing.</p> <p><strong>Actions</strong>: <code class="language-plaintext highlighter-rouge">Attemp_Up</code> (AU), <code class="language-plaintext highlighter-rouge">Attemp_Down</code> (AD), <code class="language-plaintext highlighter-rouge">Attemp_Left</code> (AL), <code class="language-plaintext highlighter-rouge">Attemp_Right</code> (AR)</p> <p><strong>Environment Dynamics</strong>:</p> <p><strong>Rewards</strong>:</p> <ul> <li>The agent receives a reward of -10 for entering the state with the water and a record of +10 for entering the goal state.</li> <li>Entering any other state results in a reward of zero.</li> <li>Any actions that cause the agent stays in state 21 will count as “entering” the water state again and result in an additional reward of -10.</li> <li>Reward discount parameter \(\gamma = 0.9\).</li> </ul> <p><strong>Number of States</strong>: 24</p> <ul> <li>23 normal states + 1 terminal absorbing state (\(s_\infty\)) <ul> <li>Once in \(s_\infty\), the agent can never leave (<em>episode</em> ends).</li> <li>\(s_\infty\) should not be thought as “goal” state.</li> </ul> </li> </ul> <hr> <h2 id="describe-the-agent-and-environment-mathematically">Describe the Agent and Environment Mathematically</h2> <h3 id="math-definition-for-environment">Math Definition for Environment</h3> <p>We can use <em>Markov Decision Processes</em> (MDPs) to formalize the environment of an RL problem. The unique terms are \(\mathcal{S}\) (the set of all possible states), \(\mathcal{A}\) (the set of all possible actions), \(p\) (transition function), \(d_R\) (reward distribution), \(R\) (reward function), \(d_0\) (initial state distribution), and \(\gamma\) (reward discount parameter). The common definition of the environment is</p> \[(\mathcal{S}, \mathcal{A}, p, R, \gamma)\] <h3 id="math-definition-for-agent">Math Definition for Agent</h3> <p>We define the decision rule that the agent selects actions as a <strong>policy</strong>. Formally, a policy \(\pi\) is a function</p> \[\begin{aligned} &amp;\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0,1] \\ &amp;\pi(s,a) := \text{Pr}(A_t=a | S_t=s) \end{aligned}\] <div class="callout"> <p><strong>Agent’s Goal</strong></p> <p>The agent’s goal is to find an optimal policy \(\pi^*\) that maximizes the expected total amount of reward that the agent will obtain.</p> </div> <h3 id="example-mountain-car">Example: Mountain Car</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mountain-car-480.webp 480w,/assets/img/rl/mountain-car-800.webp 800w,/assets/img/rl/mountain-car-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rl/mountain-car.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Mountain Car" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div></center> </div> <ul> <li> <strong>State</strong>: \(s=(x,v)\), where \(x \in \mathbb{R}\) is the position of the car and \(v \in \mathbb{R}\) is the velocity.</li> <li> <strong>Actions</strong>: \(a \in \{\texttt{reverse}, \texttt{neutral}, \texttt{forward}\}\). These actions are mapped to numerical values as \(a \in \{-1, 0 ,1\}\).</li> <li> <strong>Dynamics</strong>: The dynamics are deterministic—taking action \(a\) in state \(s\) always produces the same state, \(s^\prime\). Thus, \(p(s,a,s^\prime) \in \{0, 1\}\). The dynamics are characterized by:</li> </ul> \[\begin{aligned} v_{t+1} &amp;= v_t + 0.001 a_t - 0.0025 \cos(3x_t) \\ x_{t+1} &amp;= x_t + v_{t+1} \end{aligned}\] <p>After the next state, \(s^\prime = [x_{t+1}, v_{t+1}]\) has been computed,</p> <ul> <li>the value of \(x_{t+1}\) is clipped so that it stays in the closed interval \([-1.2, 0.5]\).</li> <li>the value of \(v_{t+1}\) is clipped so that it stays in the closed interval \([-0.7, 0.7]\).</li> <li> <p>if \(x_{t+1}\) reaches the left or right bound (\(x_{t+1} = -1.2\) or \(x_{t+1} = 0.5\)), then the car’s velocity is reset to zero (\(v_{t+1} = 0\)).</p> </li> <li> <strong>Initial State</strong>: \(S_0 = (X_0, 0)\), where \(X_0\) is an initial position drawn uniformly at random from the interval \([-0.6, -0.4]\).</li> <li> <strong>Terminal States</strong>: If \(x_t = 0.5\), then the state is terminal (it always transitions to \(s_\infty\)).</li> <li> <strong>Rewards</strong>: \(R_t = -1\) always, except when transitioning to \(s_\infty\) (from \(s_\infty\) or from a terminal state), in which case \(R_t = 0\).</li> <li> <strong>Discount</strong>: \(\gamma = 1.0\).</li> </ul> <hr> <h3 id="additional-terminology-notation-and-assumptions">Additional Terminology, Notation, and Assumptions</h3> <ul> <li>A <em>history</em>, \(H_t\), is a recording of what has happened up to time \(t\) in an episode:</li> </ul> \[H_t := (S_0, A_0, R_0, S_1, A_1, R_1, \ldots, S_t, A_t, R_t)\] <ul> <li>A <em>trajectory</em> is the history of an entire episode: \(H_\infty\)</li> <li>The <em>return</em> or <em>discounted return</em> of a trajectory is the discounted sum of rewards \(G := \sum_{t = 0}^{\infty} \gamma^t R_t\)</li> <li>The <em>expected return</em> or <em>expected discounted return</em> can be written as \(J(\pi) := \mathbf{E}[G\vert\pi]\)</li> <li>The <em>return from time</em> \(t\) or <em>discounted return from time</em> \(t\), \(G_t\), is the discounted sum of rewards starting from time \(t\)</li> </ul> \[G_t := \sum_{k=1}^{\infty} \gamma^k R_{t+k}\] <ul> <li>The <em>horizon</em>, \(L\), of an MDP is the smallest integer such that \(\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\) <ul> <li>if \(L &lt; \infty\) for all policies, then we say that the MDP is <em>finite horizon</em> </li> <li>if \(L = \infty\) then the domain may be <em>indefinite horizon</em> (the agent will always enter \(s_\infty\)) or <em>infinite horizon</em> (the agent may never enter \(s_\infty\))</li> </ul> </li> </ul> <hr> <h3 id="markov-property">Markov Property</h3> <div class="callout"> <p><strong>Markov Property (<em>Markov Assumption</em>)</strong></p> <p>In short: <strong><em>given the present, the future does not depend on the past</em></strong>.</p> <p>Formally, \(S_{t+1}\) is conditionally independent of \(H_{t-1}\) given \(S_t\). That is, for all \(h, s, a, s^\prime, t\):</p> <p>\(\text{Pr}(S_{t+1} = s^\prime | H_{t-1} = h, S_{t}=s, A_{t}=a) = \text{Pr}(S_{t+1}=s^\prime | S_{t}=s, A_{t}=a)\)</p> </div> <p>If a model (environment, reward …) holds the Markov assumption, we say it has the Markov property, or say the model is <strong><em>Markovian</em></strong>.</p> <hr> <h2 id="why-use-mdp-in-rl">Why use MDP in RL?</h2> <p>MDP is not only powerful enough to model the interaction between a learning agent and its environment, it also brings some critical guarantees that make our “reinforcement learning” actually work.</p> <p>For now, let’s skip the derivation and jump straight to the conclusions.</p> <div class="callout"> <p><strong>Existence of an Optimal Policy</strong></p> <p>For all MDPs where \(|\mathcal{S}| &lt; \infty\), \(|\mathcal{A}| &lt; \infty\), \(R_\text{max} &lt; \infty\), and \(\gamma &lt; 1\), there exists at least one optimal policy, \(\pi^*\).</p> </div> <p>Later when we introduce the <em>Bellman equation</em> and the <em>Bellman optimality equation</em>, we will further establish that:</p> <ol> <li>if a policy \(\pi\) achieves a state where its expected future rewards cannot be improved further by any other action or decision at each step (<em>Bellman optimality equation</em>), then it is an optimal policy.</li> <li>if there are only a finite number of possible states and actions, rewards are bounded, and future rewards are discounted (with a discount factor \(\gamma &lt; 1\)), then there exists a policy \(\pi\) that satisfies the Bellman optimality equation.</li> </ol> <p>Furthermore, we can perform policy/value iteration (will be covered later) using the Bellman and Bellman optimality equation. As a result, we can not only get better policies iteration after iteration, but also, under some constraints, prove that the final policy will converge to the optimal policy.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Tianyi Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"publications",description:"\u201c\u5f88\u60ed\u6127\uff0c\u5c31\u505a\u4e86\u4e00\u70b9\u5fae\u5c0f\u7684\u5de5\u4f5c\uff0c\u8c22\u8c22\u5927\u5bb6\u3002\u201d",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-repositories",title:"repositories",description:"\u201c\u6211\u4eec\u5fc5\u987b\u53bb\u52aa\u529b\u5b66\u4e60\u65b0\u4e1c\u897f\u3002\u5373\u4f7f\u6211\u4eec\u9047\u5230\u4e86\u95ee\u9898\uff0c\u5927\u5bb6\u4e5f\u4f1a\u7406\u89e3\u7684\u3002\u201d",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-reinforcement-learning-value-functions",title:"Reinforcement Learning \u2014 Value Functions",description:"A deep dive into value functions and the Bellman equation in Reinforcement Learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/value-functions/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u4ef7\u503c\u51fd\u6570",title:"\u5f3a\u5316\u5b66\u4e60 \u2014 \u4ef7\u503c\u51fd\u6570",description:"\u6df1\u5165\u63a2\u8ba8\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4ef7\u503c\u51fd\u6570\u4e0e\u8d1d\u5c14\u66fc\u65b9\u7a0b\u3002",section:"Posts",handler:()=>{window.location.href="/blog/2025/value-functions-zh/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e0e\u5f3a\u5316\u5b66\u4e60",title:"\u5f3a\u5316\u5b66\u4e60 - \u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e0e\u5f3a\u5316\u5b66\u4e60",description:"\u5f3a\u5316\u5b66\u4e60\u6982\u5ff5\u5165\u95e8\uff0c\u5305\u62ec\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u3002",section:"Posts",handler:()=>{window.location.href="/blog/2025/mdp-and-rl-zh/"}},{id:"post-reinforcement-learning-mdp-and-rl",title:"Reinforcement Learning - MDP and RL",description:"An introduction to Reinforcement Learning concepts, including Markov Decision Processes (MDPs).",section:"Posts",handler:()=>{window.location.href="/blog/2025/mdp-and-rl/"}},{id:"post-reinforcement-learning-mathematical-foundations",title:"Reinforcement Learning - Mathematical Foundations",description:"A very, very, very large chunk of math",section:"Posts",handler:()=>{window.location.href="/blog/2025/math-foundations-for-RL/"}},{id:"post-\u5f3a\u5316\u5b66\u4e60-\u6570\u5b66\u57fa\u7840",title:"\u5f3a\u5316\u5b66\u4e60 - \u6570\u5b66\u57fa\u7840",description:"\u8d85\u5927\u4e00\u5768\u6570\u5b66\u516c\u5f0f",section:"Posts",handler:()=>{window.location.href="/blog/2025/math-foundations-for-RL-zh/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>