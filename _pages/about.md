---
layout: about
title: about
permalink: /
subtitle: <em>I <s>break</s> study everything language model related. Training, inference, optimization, and deployment.</em>

profile:
  align: right
  image: avatar.jpg
  image_circular: true # crops the image to make it circular
  more_info: >
    <a href="https://github.com/m0gician" title="GitHub"><font size="5"><i class="fa-brands fa-github"></i></font></a>
    <a href="https://orcid.org/0009-0009-6777-2025" title="ORCID"><font size="5"><i class="ai ai-orcid"></i></font></a>
    <a href="https://scholar.google.com/citations?user=mXZqElIAAAAJ" title="Google Scholar"><font size="5"><i class="ai ai-google-scholar"></i></font></a><br>
    <p><tt><font size="3">NLP Engineer@Pinduoduo</tt></p>
    <p><tt><font size="3">Independent Researcher@Home</tt></p>

news: false # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page
---

I am less of an engineer and more of an independent researcher. My research interests are in natural language processing (NLP), particularly in its intersections with machine learning (ML) and reinforcement learning (RL) using Large Language Models (LLMs). These interests have drawn me toward two main research directions:

<div class="callout">
  <strong>ðŸ’­ Does language do reason itself, externalized?</strong><br> How can we utilize language as a medium of thought to elicit better in-context reasoning and planning abilities of a model, and build models that can handle complex open-ended tasks with long trajectories?
</div>
<div class="callout">
  <strong>ðŸ˜± Does human become the lower bound for LLMs?</strong><br> When average human performance starts to hinder the improvement of LLMs, we need better benchmarks that can not only scale with LLM development but also align with our ultimate goals of building safe and useful LLMs.
</div>

Through my experience in NLP, ML, and RL, I am inspired to work towards building LMs that can reason and interact meaningfully with humans, while also contributing to scalable and accurate evaluation methods.
