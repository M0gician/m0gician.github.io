<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://m0gician.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://m0gician.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-15T12:32:12+00:00</updated><id>https://m0gician.github.io/feed.xml</id><title type="html">blank</title><subtitle>I break everything language model related. Training, inference, optimization, and deployment. </subtitle><entry><title type="html">强化学习 - 马尔可夫决策过程与强化学习</title><link href="https://m0gician.github.io/blog/2025/mdp-and-rl-cn/" rel="alternate" type="text/html" title="强化学习 - 马尔可夫决策过程与强化学习"/><published>2025-07-15T04:00:00+00:00</published><updated>2025-07-15T04:00:00+00:00</updated><id>https://m0gician.github.io/blog/2025/mdp-and-rl-cn</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/mdp-and-rl-cn/"><![CDATA[<h2 id="什么是强化学习">什么是强化学习？</h2> <div class="blockquote"> <p>强化学习是机器学习的一个领域，其灵感来自行为主义心理学，研究智能体如何从与环境的互动中学习。 <br/>—Sutton &amp; Barto (1998), Phil, <cite>维基百科</cite></p> </div> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/rl-system-480.webp 480w,/assets/img/rl/rl-system-800.webp 800w,/assets/img/rl/rl-system-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/rl-system.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="强化学习图示" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p>一个典型的强化学习系统由5个部分组成：<strong>智能体</strong>（agent）在<strong>环境</strong>（environment）中的每个<strong>状态</strong>（state）下执行一个<strong>动作</strong>（action），并在满足某些标准时获得<strong>奖励</strong>（reward）。</p> <div class="callout"> <details><summary><strong>监督学习问题可以转化为强化学习问题吗？</strong></summary> <strong>可以</strong>。我们可以将一个监督学习问题转化为一个强化学习问题（状态作为分类器的输入；动作作为标签；如果标签正确，奖励为1，否则为-1）。</details> </div> <div class="callout"> <details><summary><strong>强化学习是监督学习的替代品吗？</strong></summary> <p><strong>不是</strong>。监督学习使用指导性反馈（智能体应该采取什么行动）。任何偏离所提供反馈的行为都会受到惩罚。</p> <p>另一方面，强化学习问题不是以固定的数据集形式提供的，而是以代码或整个环境的描述形式提供的。强化学习中的奖励应该传达智能体的行为有多“好”，而不是最好的行为应该是什么。智能体的目标是最大化总奖励，这可能需要智能体放弃眼前的奖励以获得以后更大的奖励。</p> <p>如果你有一个序列问题或一个只有评估性反馈可用的问题（或两者兼有！），那么你应该考虑使用强化学习。</p></details> </div> <h3 id="示例网格世界">示例：网格世界</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/gridworld-480.webp 480w,/assets/img/rl/gridworld-800.webp 800w,/assets/img/rl/gridworld-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/gridworld.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="网格世界" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p><strong>状态</strong>: 机器人的位置。机器人没有朝向。</p> <p><strong>动作</strong>: <code class="language-plaintext highlighter-rouge">尝试向上</code> (AU), <code class="language-plaintext highlighter-rouge">尝试向下</code> (AD), <code class="language-plaintext highlighter-rouge">尝试向左</code> (AL), <code class="language-plaintext highlighter-rouge">尝试向右</code> (AR)</p> <p><strong>环境动态</strong>:</p> <p><strong>奖励</strong>:</p> <ul> <li>智能体进入有水的状态会得到-10的奖励，进入目标状态会得到+10的奖励。</li> <li>进入任何其他状态的奖励为零。</li> <li>任何导致智能体停留在状态21的动作都将被视为再次进入水域状态，并导致额外的-10奖励。</li> <li>奖励折扣参数 \(\gamma = 0.9\)。</li> </ul> <p><strong>状态数量</strong>: 24</p> <ul> <li>23个正常状态 + 1个终止吸收状态 (\(s_\infty\)) <ul> <li>一旦进入\(s_\infty\)，智能体就永远无法离开（<strong>回合</strong>结束）。</li> <li>\(s_\infty\) 不应被认为是“目标”状态。</li> </ul> </li> </ul> <hr/> <h2 id="用数学方式描述智能体和环境">用数学方式描述智能体和环境</h2> <h3 id="环境的数学定义">环境的数学定义</h3> <p>我们可以使用<strong>马尔可夫决策过程</strong>（MDPs）来形式化强化学习问题的环境。其中的独特术语是\(\mathcal{S}\)（所有可能状态的集合），\(\mathcal{A}\)（所有可能动作的集合），\(p\)（转移函数），\(d_R\)（奖励分布），\(R\)（奖励函数），\(d_0\)（初始状态分布）和\(\gamma\)（奖励折扣参数）。环境的通用定义是</p> \[(\mathcal{S}, \mathcal{A}, p, R, \gamma)\] <h3 id="智能体的数学定义">智能体的数学定义</h3> <p>我们将智能体选择动作的决策规则定义为<strong>策略</strong>。形式上，策略\(\pi\)是一个函数</p> \[\begin{aligned} &amp;\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0,1] \\ &amp;\pi(s,a) := \text{Pr}(A_t=a | S_t=s) \end{aligned}\] <div class="callout"> <p><strong>智能体的目标</strong></p> <p>智能体的目标是找到一个最优策略\(\pi^*\)，以最大化智能体将获得的总奖励的期望值。</p> </div> <h3 id="示例山地车">示例：山地车</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mountain-car-480.webp 480w,/assets/img/rl/mountain-car-800.webp 800w,/assets/img/rl/mountain-car-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mountain-car.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="山地车" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <div class="caption"> 山地车环境 </div> <ul> <li><strong>状态</strong>: \(s=(x,v)\), 其中 \(x \in \mathbb{R}\) 是小车的位置，\(v \in \mathbb{R}\) 是速度。</li> <li><strong>动作</strong>: \(a \in \{\texttt{倒车}, \texttt{空挡}, \texttt{前进}\}\). 这些动作被映射为数值 \(a \in \{-1, 0 ,1\}\)。</li> <li> <p><strong>动态</strong>: 动态是确定性的——在状态\(s\)下采取动作\(a\)总是产生相同的状态\(s^\prime\)。因此，\(p(s,a,s^\prime) \in \{0, 1\}\)。动态特性如下：</p> \[\begin{aligned} v_{t+1} &amp;= v_t + 0.001 a_t - 0.0025 \cos(3x_t) \\ x_{t+1} &amp;= x_t + v_{t+1} \end{aligned}\] <p>在计算出下一个状态 \(s^\prime = [x_{t+1}, v_{t+1}]\) 后，</p> <ul> <li>\(x_{t+1}\) 的值被限制在闭区间 \([-1.2, 0.5]\) 内。</li> <li>\(v_{t+1}\) 的值被限制在闭区间 \([-0.7, 0.7]\) 内。</li> <li>如果 \(x_{t+1}\) 到达左边界或右边界（\(x_{t+1} = -1.2\) 或 \(x_{t+1} = 0.5\)），那么小车的速度将重置为零（\(v_{t+1} = 0\)）。</li> </ul> </li> <li><strong>初始状态</strong>: \(S_0 = (X_0, 0)\), 其中 \(X_0\) 是从区间 \([-0.6, -0.4]\) 中均匀随机抽取的初始位置。</li> <li><strong>终止状态</strong>: 如果 \(x_t = 0.5\)，则该状态为终止状态（它总是转移到 \(s_\infty\)）。</li> <li><strong>奖励</strong>: \(R_t\) 总是为 -1，除非转移到 \(s_\infty\)（从 \(s_\infty\) 或从终止状态），此时 \(R_t = 0\)。</li> <li><strong>折扣</strong>: \(\gamma = 1.0\)。</li> </ul> <hr/> <h3 id="附加术语符号和假设">附加术语、符号和假设</h3> <ul> <li> <p><strong>历史</strong>（history），\(H_t\)，是回合中直到时间\(t\)所发生事件的记录：</p> \[H_t := (S_0, A_0, R_0, S_1, A_1, R_1, \ldots, S_t, A_t, R_t)\] </li> <li><strong>轨迹</strong>（trajectory）是整个回合的历史：\(H_\infty\)</li> <li>轨迹的<strong>回报</strong>（return）或<strong>折扣回报</strong>（discounted return）是奖励的折扣总和 \(G := \sum_{t = 0}^{\infty} \gamma^t R_t\)</li> <li><strong>期望回报</strong>（expected return）或<strong>期望折扣回报</strong>（expected discounted return）可以写成 \(J(\pi) := \mathbf{E}[G\vert\pi]\)</li> <li>从时间\(t\)开始的<strong>回报</strong>或从时间\(t\)开始的<strong>折扣回报</strong>，\(G_t\)，是从时间\(t\)开始的奖励的折扣总和</li> </ul> \[G_t := \sum_{k=1}^{\infty} \gamma^k R_{t+k}\] <ul> <li> <p>MDP的<strong>范围</strong>（horizon），\(L\)，是满足以下条件的最小整数</p> \[\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\] <ul> <li>如果对于所有策略 \(L &lt; \infty\)，我们称该MDP为<strong>有限范围</strong>（finite horizon）</li> <li>如果 \(L = \infty\)，则该领域可能是<strong>不确定范围</strong>（indefinite horizon）（智能体总是会进入\(s_\infty\)）或<strong>无限范围</strong>（infinite horizon）（智能体可能永远不会进入\(s_\infty\)）</li> </ul> </li> </ul> <hr/> <h3 id="马尔可夫性质">马尔可夫性质</h3> <div class="callout"> <p><strong>马尔可夫性质 (</strong>马尔可夫假设<strong>)</strong></p> <p>简而言之：<strong><em>给定现在，未来与过去无关</em></strong>。</p> <p>形式上，给定\(S_t\)，\(S_{t+1}\) 条件独立于 \(H_{t-1}\)。也就是说，对于所有的 \(h, s, a, s^\prime, t\)：</p> <p>\(\text{Pr}(S_{t+1} = s^\prime | H_{t-1} = h, S_{t}=s, A_{t}=a) = \text{Pr}(S_{t+1}=s^\prime | S_{t}=s, A_{t}=a)\)</p> </div> <p>如果一个模型（环境、奖励…）满足马尔可夫假设，我们就说它具有马尔可夫性质，或者说这个模型是 <strong><em>Markovian</em></strong>的。</p> <hr/> <h2 id="为什么在强化学习中使用mdp">为什么在强化学习中使用MDP？</h2> <p>MDP不仅功能强大，足以模拟学习智能体与其环境之间的交互，它还带来了一些关键的保证，使我们的“强化学习”能够真正起作用。</p> <p>现在，让我们跳过推导，直接看结论。</p> <div class="callout"> <p><strong>最优策略的存在性</strong></p> <p>对于所有满足 \(|\mathcal{S}| &lt; \infty\), \(|\mathcal{A}| &lt; \infty\), \(R_\text{max} &lt; \infty\) 和 \(\gamma &lt; 1\) 的MDP，至少存在一个最优策略 \(\pi^*\)。</p> </div> <p>稍后当我们介绍<strong>贝尔曼方程</strong>和<strong>贝尔曼最优方程</strong>时，我们将进一步确定：</p> <ol> <li>如果一个策略\(\pi\)在每个步骤中都达到了一个状态，其期望的未来奖励无法通过任何其他行动或决策进一步提高（<strong>贝尔曼最优方程</strong>），那么它就是一个最优策略。</li> <li>如果只有有限数量的可能状态和动作，奖励有界，并且未来奖励被折扣（折扣因子\(\gamma &lt; 1\)），那么存在一个满足贝尔曼最优方程的策略\(\pi\)。</li> </ol> <p>此外，我们可以使用贝尔曼方程和贝尔曼最优方程执行策略/价值迭代（稍后将介绍）。因此，我们不仅可以一次又一次地迭代得到更好的策略，而且在某些约束下，还可以证明最终策略将收敛到最优策略。</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[强化学习概念入门，包括马尔可夫决策过程（MDP）。]]></summary></entry><entry><title type="html">Reinforcement Learning - MDP and RL</title><link href="https://m0gician.github.io/blog/2025/mdp-and-rl/" rel="alternate" type="text/html" title="Reinforcement Learning - MDP and RL"/><published>2025-07-14T20:00:00+00:00</published><updated>2025-07-14T20:00:00+00:00</updated><id>https://m0gician.github.io/blog/2025/mdp-and-rl</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/mdp-and-rl/"><![CDATA[<h2 id="what-is-reinforcement-learning">What is Reinforcement Learning?</h2> <div class="citation"> <p>"Reinforcement Learning is an area of machine learning, inspired by behaviorist psychology, concerned with how an agent can learn from interactions with an environment." <br/>Sutton &amp; Barto (1998), Phil, <cite>Wikipedia</cite></p> </div> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/rl-system-480.webp 480w,/assets/img/rl/rl-system-800.webp 800w,/assets/img/rl/rl-system-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/rl-system.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="RL System" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p>A typical Reinforcement Learning system consists of 5 components: an <strong>agent</strong> takes an <strong>action</strong> at each <strong>state</strong> in an <strong>environment</strong> and receives a <strong>reward</strong> if some criteria are met.</p> <div class="callout"> <details><summary><strong>Can a Supervised Learning problem be converted into a RL problem?</strong></summary> <strong>Yes</strong>. One might take a supervised learning problem and convert it into an RL problem (the state as the input to a classifier; the action as a label; and the reward as 1 if the label is correct and -1 otherwise).</details> </div> <div class="callout"> <details><summary><strong>Is RL an alternative to Supervised Learning?</strong></summary> <p><strong>No</strong>. Supervised learning uses instructive feedback (what action the agent should have taken). Anything deviates the provided feedback will be penalized.</p> <p>RL problems on the other hand aren’t provided as fixed data sets but as code or descriptions of the entire environment. Rewards in RL should convey how “good” an agent’s actions are, not what the best actions would have been. The goal of the agent is to maximize the total reward and this might require the agent forgoing immediate reward to obtain larger reward later.</p> <p>If you have a sequential problem or a problem where only evaluative feedback is available (or both!), then you should consider use RL.</p></details> </div> <h3 id="example-gridworld">Example: Gridworld</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/gridworld-480.webp 480w,/assets/img/rl/gridworld-800.webp 800w,/assets/img/rl/gridworld-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/gridworld.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Gridworld" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p><strong>State</strong>: Position of robot. The robot does not have a direction that it is facing.</p> <p><strong>Actions</strong>: <code class="language-plaintext highlighter-rouge">Attemp_Up</code> (AU), <code class="language-plaintext highlighter-rouge">Attemp_Down</code> (AD), <code class="language-plaintext highlighter-rouge">Attemp_Left</code> (AL), <code class="language-plaintext highlighter-rouge">Attemp_Right</code> (AR)</p> <p><strong>Environment Dynamics</strong>:</p> <p><strong>Rewards</strong>:</p> <ul> <li>The agent receives a reward of -10 for entering the state with the water and a record of +10 for entering the goal state.</li> <li>Entering any other state results in a reward of zero.</li> <li>Any actions that cause the agent stays in state 21 will count as “entering” the water state again and result in an additional reward of -10.</li> <li>Reward discount parameter \(\gamma = 0.9\).</li> </ul> <p><strong>Number of States</strong>: 24</p> <ul> <li>23 normal states + 1 terminal absorbing state (\(s_\infty\)) <ul> <li>Once in \(s_\infty\), the agent can never leave (<em>episode</em> ends).</li> <li>\(s_\infty\) should not be thought as “goal” state.</li> </ul> </li> </ul> <hr/> <h2 id="describe-the-agent-and-environment-mathematically">Describe the Agent and Environment Mathematically</h2> <h3 id="math-definition-for-environment">Math Definition for Environment</h3> <p>We can use <em>Markov Decision Processes</em> (MDPs) to formalize the environment of an RL problem. The unique terms are \(\mathcal{S}\) (the set of all possible states), \(\mathcal{A}\) (the set of all possible actions), \(p\) (transition function), \(d_R\) (reward distribution), \(R\) (reward function), \(d_0\) (initial state distribution), and \(\gamma\) (reward discount parameter). The common definition of the environment is</p> \[(\mathcal{S}, \mathcal{A}, p, R, \gamma)\] <h3 id="math-definition-for-agent">Math Definition for Agent</h3> <p>We define the decision rule that the agent selects actions as a <strong>policy</strong>. Formally, a policy \(\pi\) is a function</p> \[\begin{aligned} &amp;\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0,1] \\ &amp;\pi(s,a) := \text{Pr}(A_t=a | S_t=s) \end{aligned}\] <div class="callout"> <p><strong>Agent’s Goal</strong></p> <p>The agent’s goal is to find an optimal policy \(\pi^*\) that maximizes the expected total amount of reward that the agent will obtain.</p> </div> <h3 id="example-mountain-car">Example: Mountain Car</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mountain-car-480.webp 480w,/assets/img/rl/mountain-car-800.webp 800w,/assets/img/rl/mountain-car-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mountain-car.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Mountain Car" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <ul> <li><strong>State</strong>: \(s=(x,v)\), where \(x \in \mathbb{R}\) is the position of the car and \(v \in \mathbb{R}\) is the velocity.</li> <li><strong>Actions</strong>: \(a \in \{\texttt{reverse}, \texttt{neutral}, \texttt{forward}\}\). These actions are mapped to numerical values as \(a \in \{-1, 0 ,1\}\).</li> <li><strong>Dynamics</strong>: The dynamics are deterministic—taking action \(a\) in state \(s\) always produces the same state, \(s^\prime\). Thus, \(p(s,a,s^\prime) \in \{0, 1\}\). The dynamics are characterized by:</li> </ul> \[\begin{aligned} v_{t+1} &amp;= v_t + 0.001 a_t - 0.0025 \cos(3x_t) \\ x_{t+1} &amp;= x_t + v_{t+1} \end{aligned}\] <p>After the next state, \(s^\prime = [x_{t+1}, v_{t+1}]\) has been computed,</p> <ul> <li>the value of \(x_{t+1}\) is clipped so that it stays in the closed interval \([-1.2, 0.5]\).</li> <li>the value of \(v_{t+1}\) is clipped so that it stays in the closed interval \([-0.7, 0.7]\).</li> <li> <p>if \(x_{t+1}\) reaches the left or right bound (\(x_{t+1} = -1.2\) or \(x_{t+1} = 0.5\)), then the car’s velocity is reset to zero (\(v_{t+1} = 0\)).</p> </li> <li><strong>Initial State</strong>: \(S_0 = (X_0, 0)\), where \(X_0\) is an initial position drawn uniformly at random from the interval \([-0.6, -0.4]\).</li> <li><strong>Terminal States</strong>: If \(x_t = 0.5\), then the state is terminal (it always transitions to \(s_\infty\)).</li> <li><strong>Rewards</strong>: \(R_t = -1\) always, except when transitioning to \(s_\infty\) (from \(s_\infty\) or from a terminal state), in which case \(R_t = 0\).</li> <li><strong>Discount</strong>: \(\gamma = 1.0\).</li> </ul> <hr/> <h3 id="additional-terminology-notation-and-assumptions">Additional Terminology, Notation, and Assumptions</h3> <ul> <li>A <em>history</em>, \(H_t\), is a recording of what has happened up to time \(t\) in an episode:</li> </ul> \[H_t := (S_0, A_0, R_0, S_1, A_1, R_1, \ldots, S_t, A_t, R_t)\] <ul> <li>A <em>trajectory</em> is the history of an entire episode: \(H_\infty\)</li> <li>The <em>return</em> or <em>discounted return</em> of a trajectory is the discounted sum of rewards \(G := \sum_{t = 0}^{\infty} \gamma^t R_t\)</li> <li>The <em>expected return</em> or <em>expected discounted return</em> can be written as \(J(\pi) := \mathbf{E}[G\vert\pi]\)</li> <li>The <em>return from time</em> \(t\) or <em>discounted return from time</em> \(t\), \(G_t\), is the discounted sum of rewards starting from time \(t\)</li> </ul> \[G_t := \sum_{k=1}^{\infty} \gamma^k R_{t+k}\] <ul> <li>The <em>horizon</em>, \(L\), of an MDP is the smallest integer such that \(\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\) <ul> <li>if \(L &lt; \infty\) for all policies, then we say that the MDP is <em>finite horizon</em></li> <li>if \(L = \infty\) then the domain may be <em>indefinite horizon</em> (the agent will always enter \(s_\infty\)) or <em>infinite horizon</em> (the agent may never enter \(s_\infty\))</li> </ul> </li> </ul> <hr/> <h3 id="markov-property">Markov Property</h3> <div class="callout"> <p><strong>Markov Property (<em>Markov Assumption</em>)</strong></p> <p>In short: <strong><em>given the present, the future does not depend on the past</em></strong>.</p> <p>Formally, \(S_{t+1}\) is conditionally independent of \(H_{t-1}\) given \(S_t\). That is, for all \(h, s, a, s^\prime, t\):</p> <p>\(\text{Pr}(S_{t+1} = s^\prime | H_{t-1} = h, S_{t}=s, A_{t}=a) = \text{Pr}(S_{t+1}=s^\prime | S_{t}=s, A_{t}=a)\)</p> </div> <p>If a model (environment, reward …) holds the Markov assumption, we say it has the Markov property, or say the model is <strong><em>Markovian</em></strong>.</p> <hr/> <h2 id="why-use-mdp-in-rl">Why use MDP in RL?</h2> <p>MDP is not only powerful enough to model the interaction between a learning agent and its environment, it also brings some critical guarantees that make our “reinforcement learning” actually work.</p> <p>For now, let’s skip the derivation and jump straight to the conclusions.</p> <div class="callout"> <p><strong>Existence of an Optimal Policy</strong></p> <p>For all MDPs where \(|\mathcal{S}| &lt; \infty\), \(|\mathcal{A}| &lt; \infty\), \(R_\text{max} &lt; \infty\), and \(\gamma &lt; 1\), there exists at least one optimal policy, \(\pi^*\).</p> </div> <p>Later when we introduce the <em>Bellman equation</em> and the <em>Bellman optimality equation</em>, we will further establish that:</p> <ol> <li>if a policy \(\pi\) achieves a state where its expected future rewards cannot be improved further by any other action or decision at each step (<em>Bellman optimality equation</em>), then it is an optimal policy.</li> <li>if there are only a finite number of possible states and actions, rewards are bounded, and future rewards are discounted (with a discount factor \(\gamma &lt; 1\)), then there exists a policy \(\pi\) that satisfies the Bellman optimality equation.</li> </ol> <p>Furthermore, we can perform policy/value iteration (will be covered later) using the Bellman and Bellman optimality equation. As a result, we can not only get better policies iteration after iteration, but also, under some constraints, prove that the final policy will converge to the optimal policy.</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[An introduction to Reinforcement Learning concepts, including Markov Decision Processes (MDPs).]]></summary></entry><entry><title type="html">强化学习—数学基础</title><link href="https://m0gician.github.io/blog/2025/math-foundations-for-RL-cn/" rel="alternate" type="text/html" title="强化学习—数学基础"/><published>2025-07-04T19:12:00+00:00</published><updated>2025-07-04T19:12:00+00:00</updated><id>https://m0gician.github.io/blog/2025/math-foundations-for-RL-cn</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/math-foundations-for-RL-cn/"><![CDATA[<h2 id="mdp马尔可夫决策过程">MDP（马尔可夫决策过程）</h2> <p>我们通常把一个 MDP 定义为一个元组 \((\mathcal{S}, \mathcal{A}, p, R, \gamma)\)。</p> <div class="callout"> <p><strong>状态集合（\(\mathcal{S}\)）：</strong> 环境所有可能状态的集合。</p> <ul> <li>时刻 \(t\) 的状态 \(S_t\) 总是取值于 \(\mathcal{S}\)。</li> </ul> </div> <div class="callout"> <p><strong>动作集合（\(\mathcal{A}\)）：</strong> 智能体可以采取的所有可能动作的集合。</p> <ul> <li>时刻 \(t\) 的动作 \(A_t\) 总是取值于 \(\mathcal{A}\)。</li> </ul> </div> <div class="callout"> <p><strong>转移函数（\(p\)）：</strong> 描述环境状态如何变化。</p> \[p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]\] <p>对于所有 \(s \in \mathcal{S}\)、\(a \in \mathcal{A}\)、\(s' \in \mathcal{S}\) 以及 \(t \in \mathbb{N}_{\ge 0}\)：</p> \[p(s,a,s') := \Pr(S_{t+1}=s' \mid S_t=s, A_t=a)\] <p>当 \(p(s,a,s') \in \{0,1\}\) 对所有的 \(s,a,s'\) 成立时，转移函数是确定性的。</p> </div> <div class="callout"> <p>\(d_R\) 描述奖励的生成方式。</p> \[R_t \sim d_R(S_t, A_t, S_{t+1})\] </div> <div class="callout"> <p><strong>奖励函数（\(R\)）：</strong> 由奖励分布 \(d_R\) 隐式定义的函数，描述奖励如何生成。</p> \[R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\] \[R(s,a) := \mathrm{E}[R_t \mid S_t = s, A_t = a]\] </div> <div class="callout"> <p><strong>初始状态分布（\(d_0\)）：</strong></p> \[d_0: \mathcal{S} \rightarrow [0,1]\] \[d_0(s) = \Pr(S_0 = s)\] </div> <div class="callout"> <p><strong>折扣因子（\(\gamma\)）：</strong> 取值范围 \([0,1]\)，用于折扣未来奖励。</p> </div> <hr/> <h3 id="目标">目标</h3> <p>我们的目标是找到一条最优策略 \(\pi^*\)，使得期望累计折扣奖励最大化。</p> <ul> <li>\(G^i\) 表示第 \(i\) 个回合的回报（return）。</li> <li>\(R_t^i\) 表示第 \(i\) 个回合时刻 \(t\) 的奖励。</li> </ul> <div class="callout"> <p><strong>目标函数（\(J\)）：</strong></p> \[J : \Pi \rightarrow \mathbb{R}, \quad \text{对于所有 }\pi \in \Pi\] \[\begin{aligned} J(\pi) &amp;:= \mathrm{E}\Bigg[\sum_{t=1}^{\infty} \gamma^t R_t \,\Big|\, \pi\Bigg] \\[0.2cm] \hat{J}(\pi) &amp;:= \frac{1}{N}\sum_{i=1}^{N} G^i = \frac{1}{N}\sum_{i=1}^{N}\sum_{t=0}^{\infty}\gamma^t R_t^i \end{aligned}\] </div> <div class="callout"> <p><strong>最优策略（\(\pi^*\)）：</strong></p> \[\pi^* \in \arg\max_{\pi \in \Pi} J(\pi)\] </div> <details> <summary>当最优策略存在时它总是唯一的吗？</summary> 不一定，可能存在多条同样优秀的最优策略。 </details> <hr/> <h3 id="性质">性质</h3> <div class="callout"> <p><strong>时限（Horizon，\(L\)）：</strong> 最小的整数 \(L\)，使得对所有 \(t \ge L\)，处于终止状态 \(s_\infty\) 的概率为 1。</p> \[\forall t \ge L,\; \Pr(S_t = s_\infty) = 1\] <ul> <li>若 \(L &lt; \infty\)（对所有策略均成立），则 MDP 为 <strong>有限时限</strong>（回合式）。</li> <li>若 \(L = \infty\)，则 MDP 为 <strong>无限时限</strong>（连续式）。</li> </ul> </div> <div class="callout"> <p><strong>马尔可夫性（Markov Property）：</strong> 一种关于状态表示的性质，假设在给定当前状态的条件下，未来与过去独立。</p> <ul> <li>给定当前状态 \(S_t\)，\(S_{t+1}\) 与历史 \(H_{t-1}\) 条件独立。</li> </ul> </div> <hr/> <h2 id="策略policy">策略（Policy）</h2> <div class="callout"> <p><strong>策略</strong> 是一种决策规则——智能体选择动作的方式。</p> \[\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]\] \[\pi(s,a) := \Pr(A_t = a \mid S_t = s)\] </div> <hr/> <h2 id="价值函数value-functions">价值函数（Value Functions）</h2> <div class="callout"> <p><strong>状态价值函数（\(v^\pi\)）</strong></p> <p>状态价值函数 \(v^\pi : \mathcal{S} \rightarrow \mathbb{R}\) 衡量从状态 \(s\) 出发并遵循策略 \(\pi\) 时的期望回报。</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\Bigg[\sum_{k=1}^{\infty}\gamma^k R_{t+k} \,\Big|\, S_t = s, \pi\Bigg] \\[0.2cm] &amp;:= \mathbf{E}[G_t \mid S_t = s, \pi] \end{aligned}\] </div> <div class="callout"> <p><strong>动作价值函数（Q 函数，\(q^\pi\)）</strong></p> <p>动作价值函数 \(q^\pi : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) 衡量在状态 \(s\) 采取动作 \(a\) 后再遵循策略 \(\pi\) 所得到的期望回报。</p> \[q^\pi(s,a) := \mathbf{E}[G_t \mid S_t = s, A_t = a, \pi]\] </div> <hr/> <h3 id="bellman-方程">Bellman 方程</h3> <div class="callout"> <p><strong>状态价值函数的 Bellman 方程（\(v^\pi\)）</strong></p> \[\begin{aligned} v^\pi(s) &amp;= \mathbf{E}\Big[R(s,A_t) + \gamma v^\pi(S_{t+1}) \,\Big|\, S_t = s, \pi\Big] \\[0.3cm] &amp;= \sum_{a \in \mathcal{A}} \pi(s,a) \sum_{s' \in \mathcal{S}} p(s,a,s')\big(R(s,a) + \gamma v^\pi(s')\big) \end{aligned}\] <ul> <li>Bellman 方程只需向前看一步。</li> <li>最优状态价值函数 \(v^*\) 是唯一的——所有最优策略共享同一 \(v^*\)。</li> </ul> </div> <div class="callout"> <p><strong>动作价值函数的 Bellman 方程（\(q^\pi\)）</strong></p> \[q^\pi(s,a) = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} p(s,a,s') \sum_{a' \in \mathcal{A}} \pi(s',a') q^\pi(s',a')\] <ul> <li>最优动作价值函数 \(q^*\) 对所有最优策略也是唯一的。</li> </ul> </div> <h3 id="bellman-最优方程">Bellman 最优方程</h3> <ol> <li>若一条策略 \(\pi\) 满足 Bellman 最优方程，则 \(\pi\) 是最优策略。</li> <li>若状态、动作集合有限，奖励有界且 \(\gamma &lt; 1\)，那么存在满足 Bellman 最优方程的策略 \(\pi\)。</li> </ol> <div class="callout"> <p><strong>\(v^*\) 的 Bellman 最优方程</strong></p> <p>对所有状态 \(s \in \mathcal{S}\)：</p> \[v^\pi(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} p(s,a,s') \big[R(s,a) + \gamma v^\pi(s')\big]\] </div> <div class="callout"> <p><strong>\(q^*\) 的 Bellman 最优方程</strong></p> \[q^*(s,a) = \sum_{s' \in \mathcal{S}} p(s,a,s') \big[R(s,a) + \gamma \max_{a' \in \mathcal{A}} q^*(s',a')\big]\] </div> <hr/> <h2 id="策略迭代policy-iteration">策略迭代（Policy Iteration）</h2> <p>策略迭代通过交替执行两步——策略评估与策略改进——来寻找最优策略。</p> <ul> <li>通过动态规划进行的策略评估虽然保证收敛到 \(v^\pi\)，但并不保证在有限计算内就能到达。</li> </ul> <div class="callout"> <p><strong>策略改进定理</strong></p> <p>对于任意策略 \(\pi\)，若存在确定性策略 \(\pi'\) 使得 \(\forall s \in \mathcal{S}\)：</p> \[q^\pi(s, \pi'(s)) \ge v^\pi(s)\] <p>则有 \(\pi' \ge \pi\)。</p> </div> <div class="callout"> <p><strong>随机策略的策略改进定理</strong></p> <p>对于任意策略 \(\pi\)，若 \(\pi'\) 满足：</p> \[\sum_{a \in \mathcal{A}} \pi'(s,a) q^\pi(s,a) \ge v^\pi(s),\] <p>则 \(\forall s \in \mathcal{S}\)，都有 \(\pi' \ge \pi\)。</p> </div> <hr/> <h2 id="价值迭代value-iteration">价值迭代（Value Iteration）</h2> <p>价值迭代通过迭代应用 Bellman 最优更新来寻找最优状态价值函数。</p> <div class="callout"> <p><strong>Banach 不动点定理</strong></p> <p>若映射 \(f\) 在非空完备赋范向量空间上是收缩映射，则存在唯一不动点 \(x^*\)，且以任意 \(x_0\) 为初始点、按照 \(x_{k+1} = f(x_k)\) 生成的序列收敛到 \(x^*\)。</p> </div> <div class="callout"> <p><strong>Bellman 算子是收缩映射</strong></p> <p>当 \(\gamma &lt; 1\) 时，在度量 \(d(v,v') := \max_{s \in \mathcal{S}} |v(s)-v'(s)|\) 下，Bellman 算子在 \(\mathbb{R}^{|\mathcal{S}|}\) 上是收缩映射。</p> </div> <ul> <li>对于有限状态动作集、有界奖励且 \(\gamma &lt; 1\) 的 MDP，价值迭代 <strong>收敛</strong> 到唯一的固定点 \(v^\infty\)。</li> <li>这类 MDP <strong>至少存在</strong> 一条最优策略。</li> </ul> <hr/> <h2 id="大数定律law-of-large-numbers">大数定律（Law of Large Numbers）</h2> <div class="callout"> <p><strong>辛钦强大数定律（Khintchine’s Strong Law of Large Numbers）</strong></p> <p>设 \(\{X_i\}_{i=1}^{\infty}\) 为 <strong>独立同分布（i.i.d.）随机变量</strong>。则样本平均序列 \((\frac{1}{n} \sum_{i=1}^{n} X_i)_{n=1}^\infty\) <strong>几乎必然</strong> 收敛到期望 \(\mathbf{E}[X_1]\)。</p> <p>即 \(\displaystyle \frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{a.s.} \mathbf{E}[X_1]\)</p> </div> <div class="callout"> <p><strong>Kolmogorov 强大数定律</strong></p> <p>设 \(\{X_i\}_{i=1}^{\infty}\) 为 <strong>独立（不要求同分布）随机变量</strong>。若所有 \(X_i\) 具有 <strong>相同均值且方差有界</strong>，则样本平均序列 \((\frac{1}{n}\sum_{i=1}^{n} X_i)^\infty_{n=1}\) 亦几乎必然收敛到 \(\mathbf{E}[X_1]\)。</p> </div>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[超大一坨数学公式]]></summary></entry><entry><title type="html">Reinforcement Learning—Mathematical Foundations</title><link href="https://m0gician.github.io/blog/2025/math-foundations-for-RL/" rel="alternate" type="text/html" title="Reinforcement Learning—Mathematical Foundations"/><published>2025-07-04T19:12:00+00:00</published><updated>2025-07-04T19:12:00+00:00</updated><id>https://m0gician.github.io/blog/2025/math-foundations-for-RL</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/math-foundations-for-RL/"><![CDATA[<h2 id="mdp-markov-decision-process">MDP (Markov Decision Process)</h2> <p>We usually define an MDP as a tuple \((\mathcal{S}, \mathcal{A}, p, R, \gamma)\).</p> <div class="callout"> <p><strong>State Set (\(\mathcal{S}\)):</strong> The set of all possible states of the environment.</p> <ul> <li>The state at time \(t\), \(S_t\), always takes values in \(\mathcal{S}\).</li> </ul> </div> <div class="callout"> <p><strong>Action Set (\(\mathcal{A}\)):</strong> The set of all possible actions the agent can take.</p> <ul> <li>The action at time \(t\), \(A_t\), always takes values in \(\mathcal{A}\).</li> </ul> </div> <div class="callout"> <p><strong>Transition Function (\(p\)):</strong> Describes how the state of the environment changes.</p> \[p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]\] <p>For all \(s \in \mathcal{S}\), \(a \in \mathcal{A}\), \(s’ \in \mathcal{S}\), and \(t \in \mathbb{N}_{\geq 0}\):</p> \[p(s,a,s') := \text{Pr}(S_{t+1}=s' | S_t=s, A_t=a)\] <p>A transition function is deterministic if \(p(s,a,s’) \in \{0,1\}\) for all s, a, and s’</p> </div> <div class="callout"> <p>\(d_R\) describes how rewards are generated.</p> \[R_t \sim d_r(S_t, A_t, S_{t+1})\] </div> <div class="callout"> <p><strong>Reward Function (\(R\)):</strong> A function implicitly defined by the reward distribution \(d_R\), which describes how rewards are generated.</p> \[R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\] \[R(s,a) := \mathrm{R}[R_t|S_t = s, A_t = a]\] </div> <div class="callout"> <p><strong>Initial State Distribution (\(d_0\)):</strong></p> \[d_0: \mathcal{S} \rightarrow [0,1]\] \[d_0(s) = \text{Pr}(S_0=s)\] </div> <div class="callout"> <p><strong>Discount Factor (\(\gamma\)):</strong> A parameter in \([0,1]\) that discounts future rewards.</p> </div> <hr/> <h3 id="objective">Objective</h3> <p>The goal is to find an optimal policy \(\pi^*\) that maximizes the expected total amount of discounted reward.</p> <ul> <li>\(G^i\) denotes the return of the i-th episode.</li> <li>\(R^i_t\) denotes the reward at time \(t\) during episode \(i\).</li> </ul> <div class="callout"> <p><strong>Objective function (\(J\)):</strong></p> \[J : \Pi \rightarrow \mathbb{R}, \text{where for all } \pi \in \Pi\] \[\begin{aligned} &amp;J(\pi) := \mathrm{E}\left[\sum_{t=1}^{\infty} \gamma^tR_t \bigg| \pi\right] \\ &amp;\hat{J}(\pi) := \frac{1}{N}\sum_{i=1}^{N}G^i = \frac{1}{n}\sum_{i=1}^{N}\sum_{t=0}^{\infty}\gamma^t R_t^i \end{aligned}\] </div> <div class="callout"> <p><strong>Optimal Policy (\(\pi^*\)):</strong></p> \[\pi^* \in \underset{\pi \in \Pi}{\text{argmax}}\,J(\pi)\] </div> <details> <summary>Is the optimal policy always unique when it exists?</summary> No. There can exist multiple optimal policies that are equally good. </details> <hr/> <h3 id="properties">Properties</h3> <div class="callout"> <p><strong>Horizon (\(L\)):</strong> The smallest integer \(L\) such that for all \(t \geq L\), the probability of being in a terminal state \(s_\infty\) is 1.</p> \[\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\] <ul> <li>The MDP is <strong>finite horizon</strong> (episodic) if \(L &lt; \infty\) for all policies.</li> <li>The MDP is <strong>infinite horizon</strong> (continuous) when \(L = \infty\).</li> </ul> </div> <div class="callout"> <p><strong>Markov Property:</strong> A property of the state representation. It assumes that the future is independent of the past given the present.</p> <ul> <li>\(S_{t+1}\) is conditionally independent of the history \(H_{t-1}\) given the current state \(S_t\).</li> </ul> </div> <hr/> <h2 id="policy">Policy</h2> <div class="callout"> <p>A <strong>policy</strong> is a decision rule—a way that the agent can select actions.</p> \[\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]\] \[\pi(s,a) := \text{Pr}(A_t=a | S_t=s)\] </div> <hr/> <h2 id="value-functions">Value Functions</h2> <div class="callout"> <p><strong>State-Value Function (\(v^\pi\))</strong></p> <p>The state-value function \(v^\pi : \mathcal{S} \rightarrow \mathbb{R}\) measures the expected return starting from a state \(s\) and following policy \(\pi\).</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\left[\sum_{k=1}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, \pi\right] \\ &amp;:= \mathbf{E}[G_t|S_t=s, \pi] \end{aligned}\] </div> <div class="callout"> <p><strong>Action-Value Function (Q-function, \(q^\pi\))</strong></p> <p>The action-value function \(q^\pi : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) measures the expected return starting from state \(s\), taking action \(a\), and then following policy \(\pi\).</p> \[q^\pi(s,a) := \mathbf{E}[G_t | S_t=s, A_t=a, \pi]\] </div> <hr/> <h3 id="bellman-equations">Bellman Equations</h3> <div class="callout"> <p><strong>Bellman Equation for the State-Value Function (\(v^\pi\))</strong></p> \[\begin{aligned} v^\pi(s) &amp;= \mathbf{E}\left[\underbrace{R(s,A_t)}_{\text{immediate reward}} + \gamma \underbrace{v^\pi(S_{t+1}) }_{\text{value of next state}} \bigg| S_t = s, \pi\right] \\[0.3cm] &amp;= \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s')(R(s,a) + \gamma v^\pi(s')) \end{aligned}\] <ul> <li>The Bellman equation only needs to look forward one time step into the future.</li> <li>The optimal state-value function, \(v^*\), is unique—all optimal policies share the same state-value function.</li> </ul> </div> <div class="callout"> <p><strong>Bellman Equation for the Action-Value Function (\(q^\pi\))</strong></p> \[q^\pi(s,a) = R(s,a) + \gamma\sum_{s' \in \mathcal{S}}p(s,a,s')\sum_{a' \in \mathcal{A}}\pi(s',a')q^\pi(s',a')\] <ul> <li>The optimal action-value function, \(q^*\), is also unique among all optimal policies.</li> </ul> </div> <h3 id="bellman-optimality-equations">Bellman Optimality Equations</h3> <ol> <li>If a policy \(\pi\) satisfies the Bellman optimality equation, then \(\pi\) is an optimal policy.</li> <li>If the state and action sets are finite, rewards are bounded, and \(\gamma &lt; 1\), then there exists a policy \(\pi\) that satisfies the Bellman optimality equation.</li> </ol> <div class="callout"> <p><strong>Bellman Optimality Equation for \(v^*\)</strong></p> <p>A policy \(\pi\) satisfies the Bellman optimality equation if for all states \(s\in\mathcal{S}\):</p> \[v^\pi(s) = \max_{a\in\mathcal{A}}\sum_{s'\in\mathcal{S}}p(s,a,s')[R(s,a)+\gamma v^\pi(s')]\] </div> <div class="callout"> <p><strong>Bellman Optimality Equation for \(q^*\)</strong></p> \[q^*(s,a) = \sum_{s'\in\mathcal{S}}p(s,a,s')\left[R(s,a) + \gamma\max_{a'\in\mathcal{A}}q^*(s',a')\right]\] </div> <hr/> <h2 id="policy-iteration">Policy Iteration</h2> <p>Policy iteration is an algorithm that finds an optimal policy by alternating between two steps: policy evaluation and policy improvement.</p> <ul> <li>Even though policy evaluation using dynamic programming is guaranteed to converge to \(v^\pi\), it is not guaranteed to reach \(v^\pi\) in a finite amount of computation.</li> </ul> <div class="callout"> <p><strong>Policy Improvement Theorem</strong></p> <p>For any policy \(\pi\), if \(\pi’\) is a deterministic policy such that \(\forall s \in \mathcal{S}\):</p> \[q^\pi(s, \pi'(s)) \geq v^\pi(s)\] <p>then \(\pi’ \geq \pi\).</p> </div> <div class="callout"> <p><strong>Policy Improvement Theorem for Stochastic Policies</strong></p> <p>For any policy \(\pi\), if \(\pi’\) satisfies:</p> \[\sum_{a\in\mathcal{A}}\pi'(s,a) q^\pi(s,a) \geq v^\pi(s),\] <p>for all \(s \in \mathcal{S}\), then \(\pi' \geq \pi\).</p> </div> <hr/> <h2 id="value-iteration">Value Iteration</h2> <p>Value iteration is an algorithm that finds the optimal state-value function by iteratively applying the Bellman optimality update.</p> <div class="callout"> <p><strong>Banach Fixed-Point Theorem</strong></p> <p>If \(f\) is a contraction mapping on a non-empty complete normed vector space, then \(f\) has a unique fixed point, \(x^*\), and the sequence defined by \(x_{k+1} = f(x_k)\), with \(x_0\) chosen arbitrarily, converges to \(x^*\).</p> </div> <div class="callout"> <p><strong>Bellman Operator is a Contraction Mapping</strong></p> <p>The Bellman operator is a contraction mapping on \(\mathbb{R}^{\vert\mathcal{S}\vert}\) with distance metric \(d(v,v’) := \max_{s\in\mathcal{S}}\vert v(s)-v’(s) \vert\) if \(\gamma &lt; 1\).</p> </div> <ul> <li>Value iteration <strong>converges</strong> to a unique fixed point \(v^\infty\) for all MDPs with finite state and action sets, bounded rewards, and \(\gamma &lt; 1\).</li> <li>All MDPs with finite state and action sets, bounded rewards, and \(\gamma &lt; 1\) <strong>have at least one optimal policy</strong>.</li> </ul> <hr/> <h2 id="law-of-large-numbers">Law of Large Numbers</h2> <div class="callout"> <p><strong>Khintchine’s Strong Law of Large Numbers</strong></p> <p>Let \(\{X_i\}_{i=1}^{\infty}\) be <strong>independent and identically distributed (i.i.d.) random variables</strong>. Then the sequence of sample averages \((\frac{1}{n} \sum_{i=1}^{n} X_i)_{n=1}^\infty\) converges <strong>almost surely</strong> to the expected value \(\mathbf{E}[X_1]\).</p> <p>i.e., \(\displaystyle \frac{1}{n}\sum_{i=1}^{\infty} X_i \overset{a.s.}{\rightarrow}\mathbf{E}[X_1]\)</p> </div> <div class="callout"> <p><strong>Kolmogorov’s Strong Law of Large Numbers</strong></p> <p>Let \(\{X_i\}^\infty_{i=1}\) be <strong>independent (not necessarily identically distributed) random variables</strong>. If all \(X_i\) have the <strong>same mean and bounded variance</strong>, then the sequence of sample averages \((\frac{1}{n}\sum_{i=1}^n X_i)^\infty_{n=1}\) converges almost surely to \(\mathbf{E}[X_1]\).</p> </div>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[A very, very, very large chunk of math]]></summary></entry></feed>