<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://m0gician.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://m0gician.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-07T19:05:09+00:00</updated><id>https://m0gician.github.io/feed.xml</id><title type="html">blank</title><subtitle>I break everything language model related. Training, inference, optimization, and deployment. </subtitle><entry><title type="html">强化学习—数学基础</title><link href="https://m0gician.github.io/blog/2025/math-foundations-for-RL-cn/" rel="alternate" type="text/html" title="强化学习—数学基础"/><published>2025-07-04T19:12:00+00:00</published><updated>2025-07-04T19:12:00+00:00</updated><id>https://m0gician.github.io/blog/2025/math-foundations-for-RL-cn</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/math-foundations-for-RL-cn/"><![CDATA[<h2 id="mdp马尔可夫决策过程">MDP（马尔可夫决策过程）</h2> <p>我们通常把一个 MDP 定义为一个元组 \((\mathcal{S}, \mathcal{A}, p, R, \gamma)\)。</p> <div class="callout"> <p><strong>状态集合（\(\mathcal{S}\)）：</strong> 环境所有可能状态的集合。</p> <ul> <li>时刻 \(t\) 的状态 \(S_t\) 总是取值于 \(\mathcal{S}\)。</li> </ul> </div> <div class="callout"> <p><strong>动作集合（\(\mathcal{A}\)）：</strong> 智能体可以采取的所有可能动作的集合。</p> <ul> <li>时刻 \(t\) 的动作 \(A_t\) 总是取值于 \(\mathcal{A}\)。</li> </ul> </div> <div class="callout"> <p><strong>转移函数（\(p\)）：</strong> 描述环境状态如何变化。</p> \[p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]\] <p>对于所有 \(s \in \mathcal{S}\)、\(a \in \mathcal{A}\)、\(s' \in \mathcal{S}\) 以及 \(t \in \mathbb{N}_{\ge 0}\)：</p> \[p(s,a,s') := \Pr(S_{t+1}=s' \mid S_t=s, A_t=a)\] <p>当 \(p(s,a,s') \in \{0,1\}\) 对所有的 \(s,a,s'\) 成立时，转移函数是确定性的。</p> </div> <div class="callout"> <p>\(d_R\) 描述奖励的生成方式。</p> \[R_t \sim d_R(S_t, A_t, S_{t+1})\] </div> <div class="callout"> <p><strong>奖励函数（\(R\)）：</strong> 由奖励分布 \(d_R\) 隐式定义的函数，描述奖励如何生成。</p> \[R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\] \[R(s,a) := \mathrm{E}[R_t \mid S_t = s, A_t = a]\] </div> <div class="callout"> <p><strong>初始状态分布（\(d_0\)）：</strong></p> \[d_0: \mathcal{S} \rightarrow [0,1]\] \[d_0(s) = \Pr(S_0 = s)\] </div> <div class="callout"> <p><strong>折扣因子（\(\gamma\)）：</strong> 取值范围 \([0,1]\)，用于折扣未来奖励。</p> </div> <hr/> <h3 id="目标">目标</h3> <p>我们的目标是找到一条最优策略 \(\pi^*\)，使得期望累计折扣奖励最大化。</p> <ul> <li>\(G^i\) 表示第 \(i\) 个回合的回报（return）。</li> <li>\(R_t^i\) 表示第 \(i\) 个回合时刻 \(t\) 的奖励。</li> </ul> <div class="callout"> <p><strong>目标函数（\(J\)）：</strong></p> \[J : \Pi \rightarrow \mathbb{R}, \quad \text{对于所有 }\pi \in \Pi\] \[\begin{aligned} J(\pi) &amp;:= \mathrm{E}\Bigg[\sum_{t=1}^{\infty} \gamma^t R_t \,\Big|\, \pi\Bigg] \\[0.2cm] \hat{J}(\pi) &amp;:= \frac{1}{N}\sum_{i=1}^{N} G^i = \frac{1}{N}\sum_{i=1}^{N}\sum_{t=0}^{\infty}\gamma^t R_t^i \end{aligned}\] </div> <div class="callout"> <p><strong>最优策略（\(\pi^*\)）：</strong></p> \[\pi^* \in \arg\max_{\pi \in \Pi} J(\pi)\] </div> <details> <summary>当最优策略存在时它总是唯一的吗？</summary> 不一定，可能存在多条同样优秀的最优策略。 </details> <hr/> <h3 id="性质">性质</h3> <div class="callout"> <p><strong>时限（Horizon，\(L\)）：</strong> 最小的整数 \(L\)，使得对所有 \(t \ge L\)，处于终止状态 \(s_\infty\) 的概率为 1。</p> \[\forall t \ge L,\; \Pr(S_t = s_\infty) = 1\] <ul> <li>若 \(L &lt; \infty\)（对所有策略均成立），则 MDP 为 <strong>有限时限</strong>（回合式）。</li> <li>若 \(L = \infty\)，则 MDP 为 <strong>无限时限</strong>（连续式）。</li> </ul> </div> <div class="callout"> <p><strong>马尔可夫性（Markov Property）：</strong> 一种关于状态表示的性质，假设在给定当前状态的条件下，未来与过去独立。</p> <ul> <li>给定当前状态 \(S_t\)，\(S_{t+1}\) 与历史 \(H_{t-1}\) 条件独立。</li> </ul> </div> <hr/> <h2 id="策略policy">策略（Policy）</h2> <div class="callout"> <p><strong>策略</strong> 是一种决策规则——智能体选择动作的方式。</p> \[\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]\] \[\pi(s,a) := \Pr(A_t = a \mid S_t = s)\] </div> <hr/> <h2 id="价值函数value-functions">价值函数（Value Functions）</h2> <div class="callout"> <p><strong>状态价值函数（\(v^\pi\)）</strong></p> <p>状态价值函数 \(v^\pi : \mathcal{S} \rightarrow \mathbb{R}\) 衡量从状态 \(s\) 出发并遵循策略 \(\pi\) 时的期望回报。</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\Bigg[\sum_{k=1}^{\infty}\gamma^k R_{t+k} \,\Big|\, S_t = s, \pi\Bigg] \\[0.2cm] &amp;:= \mathbf{E}[G_t \mid S_t = s, \pi] \end{aligned}\] </div> <div class="callout"> <p><strong>动作价值函数（Q 函数，\(q^\pi\)）</strong></p> <p>动作价值函数 \(q^\pi : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) 衡量在状态 \(s\) 采取动作 \(a\) 后再遵循策略 \(\pi\) 所得到的期望回报。</p> \[q^\pi(s,a) := \mathbf{E}[G_t \mid S_t = s, A_t = a, \pi]\] </div> <hr/> <h3 id="bellman-方程">Bellman 方程</h3> <div class="callout"> <p><strong>状态价值函数的 Bellman 方程（\(v^\pi\)）</strong></p> \[\begin{aligned} v^\pi(s) &amp;= \mathbf{E}\Big[R(s,A_t) + \gamma v^\pi(S_{t+1}) \,\Big|\, S_t = s, \pi\Big] \\[0.3cm] &amp;= \sum_{a \in \mathcal{A}} \pi(s,a) \sum_{s' \in \mathcal{S}} p(s,a,s')\big(R(s,a) + \gamma v^\pi(s')\big) \end{aligned}\] <ul> <li>Bellman 方程只需向前看一步。</li> <li>最优状态价值函数 \(v^*\) 是唯一的——所有最优策略共享同一 \(v^*\)。</li> </ul> </div> <div class="callout"> <p><strong>动作价值函数的 Bellman 方程（\(q^\pi\)）</strong></p> \[q^\pi(s,a) = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} p(s,a,s') \sum_{a' \in \mathcal{A}} \pi(s',a') q^\pi(s',a')\] <ul> <li>最优动作价值函数 \(q^*\) 对所有最优策略也是唯一的。</li> </ul> </div> <h3 id="bellman-最优方程">Bellman 最优方程</h3> <ol> <li>若一条策略 \(\pi\) 满足 Bellman 最优方程，则 \(\pi\) 是最优策略。</li> <li>若状态、动作集合有限，奖励有界且 \(\gamma &lt; 1\)，那么存在满足 Bellman 最优方程的策略 \(\pi\)。</li> </ol> <div class="callout"> <p><strong>\(v^*\) 的 Bellman 最优方程</strong></p> <p>对所有状态 \(s \in \mathcal{S}\)：</p> \[v^\pi(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} p(s,a,s') \big[R(s,a) + \gamma v^\pi(s')\big]\] </div> <div class="callout"> <p><strong>\(q^*\) 的 Bellman 最优方程</strong></p> \[q^*(s,a) = \sum_{s' \in \mathcal{S}} p(s,a,s') \big[R(s,a) + \gamma \max_{a' \in \mathcal{A}} q^*(s',a')\big]\] </div> <hr/> <h2 id="策略迭代policy-iteration">策略迭代（Policy Iteration）</h2> <p>策略迭代通过交替执行两步——策略评估与策略改进——来寻找最优策略。</p> <ul> <li>通过动态规划进行的策略评估虽然保证收敛到 \(v^\pi\)，但并不保证在有限计算内就能到达。</li> </ul> <div class="callout"> <p><strong>策略改进定理</strong></p> <p>对于任意策略 \(\pi\)，若存在确定性策略 \(\pi'\) 使得 \(\forall s \in \mathcal{S}\)：</p> \[q^\pi(s, \pi'(s)) \ge v^\pi(s)\] <p>则有 \(\pi' \ge \pi\)。</p> </div> <div class="callout"> <p><strong>随机策略的策略改进定理</strong></p> <p>对于任意策略 \(\pi\)，若 \(\pi'\) 满足：</p> \[\sum_{a \in \mathcal{A}} \pi'(s,a) q^\pi(s,a) \ge v^\pi(s),\] <p>则 \(\forall s \in \mathcal{S}\)，都有 \(\pi' \ge \pi\)。</p> </div> <hr/> <h2 id="价值迭代value-iteration">价值迭代（Value Iteration）</h2> <p>价值迭代通过迭代应用 Bellman 最优更新来寻找最优状态价值函数。</p> <div class="callout"> <p><strong>Banach 不动点定理</strong></p> <p>若映射 \(f\) 在非空完备赋范向量空间上是收缩映射，则存在唯一不动点 \(x^*\)，且以任意 \(x_0\) 为初始点、按照 \(x_{k+1} = f(x_k)\) 生成的序列收敛到 \(x^*\)。</p> </div> <div class="callout"> <p><strong>Bellman 算子是收缩映射</strong></p> <p>当 \(\gamma &lt; 1\) 时，在度量 \(d(v,v') := \max_{s \in \mathcal{S}} |v(s)-v'(s)|\) 下，Bellman 算子在 \(\mathbb{R}^{|\mathcal{S}|}\) 上是收缩映射。</p> </div> <ul> <li>对于有限状态动作集、有界奖励且 \(\gamma &lt; 1\) 的 MDP，价值迭代 <strong>收敛</strong> 到唯一的固定点 \(v^\infty\)。</li> <li>这类 MDP <strong>至少存在</strong> 一条最优策略。</li> </ul> <hr/> <h2 id="大数定律law-of-large-numbers">大数定律（Law of Large Numbers）</h2> <div class="callout"> <p><strong>辛钦强大数定律（Khintchine’s Strong Law of Large Numbers）</strong></p> <p>设 \(\{X_i\}_{i=1}^{\infty}\) 为 <strong>独立同分布（i.i.d.）随机变量</strong>。则样本平均序列 \((\frac{1}{n} \sum_{i=1}^{n} X_i)_{n=1}^\infty\) <strong>几乎必然</strong> 收敛到期望 \(\mathbf{E}[X_1]\)。</p> <p>即 \(\displaystyle \frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{a.s.} \mathbf{E}[X_1]\)</p> </div> <div class="callout"> <p><strong>Kolmogorov 强大数定律</strong></p> <p>设 \(\{X_i\}_{i=1}^{\infty}\) 为 <strong>独立（不要求同分布）随机变量</strong>。若所有 \(X_i\) 具有 <strong>相同均值且方差有界</strong>，则样本平均序列 \((\frac{1}{n}\sum_{i=1}^{n} X_i)^\infty_{n=1}\) 亦几乎必然收敛到 \(\mathbf{E}[X_1]\)。</p> </div>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[超大一坨数学公式]]></summary></entry><entry><title type="html">Reinforcement Learning—Mathematical Foundations</title><link href="https://m0gician.github.io/blog/2025/math-foundations-for-RL/" rel="alternate" type="text/html" title="Reinforcement Learning—Mathematical Foundations"/><published>2025-07-04T19:12:00+00:00</published><updated>2025-07-04T19:12:00+00:00</updated><id>https://m0gician.github.io/blog/2025/math-foundations-for-RL</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/math-foundations-for-RL/"><![CDATA[<h2 id="mdp-markov-decision-process">MDP (Markov Decision Process)</h2> <p>We usually define an MDP as a tuple \((\mathcal{S}, \mathcal{A}, p, R, \gamma)\).</p> <div class="callout"> <p><strong>State Set (\(\mathcal{S}\)):</strong> The set of all possible states of the environment.</p> <ul> <li>The state at time \(t\), \(S_t\), always takes values in \(\mathcal{S}\).</li> </ul> </div> <div class="callout"> <p><strong>Action Set (\(\mathcal{A}\)):</strong> The set of all possible actions the agent can take.</p> <ul> <li>The action at time \(t\), \(A_t\), always takes values in \(\mathcal{A}\).</li> </ul> </div> <div class="callout"> <p><strong>Transition Function (\(p\)):</strong> Describes how the state of the environment changes.</p> \[p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]\] <p>For all \(s \in \mathcal{S}\), \(a \in \mathcal{A}\), \(s’ \in \mathcal{S}\), and \(t \in \mathbb{N}_{\geq 0}\):</p> \[p(s,a,s') := \text{Pr}(S_{t+1}=s' | S_t=s, A_t=a)\] <p>A transition function is deterministic if \(p(s,a,s’) \in \{0,1\}\) for all s, a, and s’</p> </div> <div class="callout"> <p>\(d_R\) describes how rewards are generated.</p> \[R_t \sim d_r(S_t, A_t, S_{t+1})\] </div> <div class="callout"> <p><strong>Reward Function (\(R\)):</strong> A function implicitly defined by the reward distribution \(d_R\), which describes how rewards are generated.</p> \[R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\] \[R(s,a) := \mathrm{R}[R_t|S_t = s, A_t = a]\] </div> <div class="callout"> <p><strong>Initial State Distribution (\(d_0\)):</strong></p> \[d_0: \mathcal{S} \rightarrow [0,1]\] \[d_0(s) = \text{Pr}(S_0=s)\] </div> <div class="callout"> <p><strong>Discount Factor (\(\gamma\)):</strong> A parameter in \([0,1]\) that discounts future rewards.</p> </div> <hr/> <h3 id="objective">Objective</h3> <p>The goal is to find an optimal policy \(\pi^*\) that maximizes the expected total amount of discounted reward.</p> <ul> <li>\(G^i\) denotes the return of the i-th episode.</li> <li>\(R^i_t\) denotes the reward at time \(t\) during episode \(i\).</li> </ul> <div class="callout"> <p><strong>Objective function (\(J\)):</strong></p> \[J : \Pi \rightarrow \mathbb{R}, \text{where for all } \pi \in \Pi\] \[\begin{aligned} &amp;J(\pi) := \mathrm{E}\left[\sum_{t=1}^{\infty} \gamma^tR_t \bigg| \pi\right] \\ &amp;\hat{J}(\pi) := \frac{1}{N}\sum_{i=1}^{N}G^i = \frac{1}{n}\sum_{i=1}^{N}\sum_{t=0}^{\infty}\gamma^t R_t^i \end{aligned}\] </div> <div class="callout"> <p><strong>Optimal Policy (\(\pi^*\)):</strong></p> \[\pi^* \in \underset{\pi \in \Pi}{\text{argmax}}\,J(\pi)\] </div> <details> <summary>Is the optimal policy always unique when it exists?</summary> No. There can exist multiple optimal policies that are equally good. </details> <hr/> <h3 id="properties">Properties</h3> <div class="callout"> <p><strong>Horizon (\(L\)):</strong> The smallest integer \(L\) such that for all \(t \geq L\), the probability of being in a terminal state \(s_\infty\) is 1.</p> \[\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\] <ul> <li>The MDP is <strong>finite horizon</strong> (episodic) if \(L &lt; \infty\) for all policies.</li> <li>The MDP is <strong>infinite horizon</strong> (continuous) when \(L = \infty\).</li> </ul> </div> <div class="callout"> <p><strong>Markov Property:</strong> A property of the state representation. It assumes that the future is independent of the past given the present.</p> <ul> <li>\(S_{t+1}\) is conditionally independent of the history \(H_{t-1}\) given the current state \(S_t\).</li> </ul> </div> <hr/> <h2 id="policy">Policy</h2> <div class="callout"> <p>A <strong>policy</strong> is a decision rule—a way that the agent can select actions.</p> \[\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]\] \[\pi(s,a) := \text{Pr}(A_t=a | S_t=s)\] </div> <hr/> <h2 id="value-functions">Value Functions</h2> <div class="callout"> <p><strong>State-Value Function (\(v^\pi\))</strong></p> <p>The state-value function \(v^\pi : \mathcal{S} \rightarrow \mathbb{R}\) measures the expected return starting from a state \(s\) and following policy \(\pi\).</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\left[\sum_{k=1}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, \pi\right] \\ &amp;:= \mathbf{E}[G_t|S_t=s, \pi] \end{aligned}\] </div> <div class="callout"> <p><strong>Action-Value Function (Q-function, \(q^\pi\))</strong></p> <p>The action-value function \(q^\pi : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) measures the expected return starting from state \(s\), taking action \(a\), and then following policy \(\pi\).</p> \[q^\pi(s,a) := \mathbf{E}[G_t | S_t=s, A_t=a, \pi]\] </div> <hr/> <h3 id="bellman-equations">Bellman Equations</h3> <div class="callout"> <p><strong>Bellman Equation for the State-Value Function (\(v^\pi\))</strong></p> \[\begin{aligned} v^\pi(s) &amp;= \mathbf{E}\left[\underbrace{R(s,A_t)}_{\text{immediate reward}} + \gamma \underbrace{v^\pi(S_{t+1}) }_{\text{value of next state}} \bigg| S_t = s, \pi\right] \\[0.3cm] &amp;= \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s')(R(s,a) + \gamma v^\pi(s')) \end{aligned}\] <ul> <li>The Bellman equation only needs to look forward one time step into the future.</li> <li>The optimal state-value function, \(v^*\), is unique—all optimal policies share the same state-value function.</li> </ul> </div> <div class="callout"> <p><strong>Bellman Equation for the Action-Value Function (\(q^\pi\))</strong></p> \[q^\pi(s,a) = R(s,a) + \gamma\sum_{s' \in \mathcal{S}}p(s,a,s')\sum_{a' \in \mathcal{A}}\pi(s',a')q^\pi(s',a')\] <ul> <li>The optimal action-value function, \(q^*\), is also unique among all optimal policies.</li> </ul> </div> <h3 id="bellman-optimality-equations">Bellman Optimality Equations</h3> <ol> <li>If a policy \(\pi\) satisfies the Bellman optimality equation, then \(\pi\) is an optimal policy.</li> <li>If the state and action sets are finite, rewards are bounded, and \(\gamma &lt; 1\), then there exists a policy \(\pi\) that satisfies the Bellman optimality equation.</li> </ol> <div class="callout"> <p><strong>Bellman Optimality Equation for \(v^*\)</strong></p> <p>A policy \(\pi\) satisfies the Bellman optimality equation if for all states \(s\in\mathcal{S}\):</p> \[v^\pi(s) = \max_{a\in\mathcal{A}}\sum_{s'\in\mathcal{S}}p(s,a,s')[R(s,a)+\gamma v^\pi(s')]\] </div> <div class="callout"> <p><strong>Bellman Optimality Equation for \(q^*\)</strong></p> \[q^*(s,a) = \sum_{s'\in\mathcal{S}}p(s,a,s')\left[R(s,a) + \gamma\max_{a'\in\mathcal{A}}q^*(s',a')\right]\] </div> <hr/> <h2 id="policy-iteration">Policy Iteration</h2> <p>Policy iteration is an algorithm that finds an optimal policy by alternating between two steps: policy evaluation and policy improvement.</p> <ul> <li>Even though policy evaluation using dynamic programming is guaranteed to converge to \(v^\pi\), it is not guaranteed to reach \(v^\pi\) in a finite amount of computation.</li> </ul> <div class="callout"> <p><strong>Policy Improvement Theorem</strong></p> <p>For any policy \(\pi\), if \(\pi’\) is a deterministic policy such that \(\forall s \in \mathcal{S}\):</p> \[q^\pi(s, \pi'(s)) \geq v^\pi(s)\] <p>then \(\pi’ \geq \pi\).</p> </div> <div class="callout"> <p><strong>Policy Improvement Theorem for Stochastic Policies</strong></p> <p>For any policy \(\pi\), if \(\pi’\) satisfies:</p> \[\sum_{a\in\mathcal{A}}\pi'(s,a) q^\pi(s,a) \geq v^\pi(s),\] <p>for all \(s \in \mathcal{S}\), then \(\pi' \geq \pi\).</p> </div> <hr/> <h2 id="value-iteration">Value Iteration</h2> <p>Value iteration is an algorithm that finds the optimal state-value function by iteratively applying the Bellman optimality update.</p> <div class="callout"> <p><strong>Banach Fixed-Point Theorem</strong></p> <p>If \(f\) is a contraction mapping on a non-empty complete normed vector space, then \(f\) has a unique fixed point, \(x^*\), and the sequence defined by \(x_{k+1} = f(x_k)\), with \(x_0\) chosen arbitrarily, converges to \(x^*\).</p> </div> <div class="callout"> <p><strong>Bellman Operator is a Contraction Mapping</strong></p> <p>The Bellman operator is a contraction mapping on \(\mathbb{R}^{\vert\mathcal{S}\vert}\) with distance metric \(d(v,v’) := \max_{s\in\mathcal{S}}\vert v(s)-v’(s) \vert\) if \(\gamma &lt; 1\).</p> </div> <ul> <li>Value iteration <strong>converges</strong> to a unique fixed point \(v^\infty\) for all MDPs with finite state and action sets, bounded rewards, and \(\gamma &lt; 1\).</li> <li>All MDPs with finite state and action sets, bounded rewards, and \(\gamma &lt; 1\) <strong>have at least one optimal policy</strong>.</li> </ul> <hr/> <h2 id="law-of-large-numbers">Law of Large Numbers</h2> <div class="callout"> <p><strong>Khintchine’s Strong Law of Large Numbers</strong></p> <p>Let \(\{X_i\}_{i=1}^{\infty}\) be <strong>independent and identically distributed (i.i.d.) random variables</strong>. Then the sequence of sample averages \((\frac{1}{n} \sum_{i=1}^{n} X_i)_{n=1}^\infty\) converges <strong>almost surely</strong> to the expected value \(\mathbf{E}[X_1]\).</p> <p>i.e., \(\displaystyle \frac{1}{n}\sum_{i=1}^{\infty} X_i \overset{a.s.}{\rightarrow}\mathbf{E}[X_1]\)</p> </div> <div class="callout"> <p><strong>Kolmogorov’s Strong Law of Large Numbers</strong></p> <p>Let \(\{X_i\}^\infty_{i=1}\) be <strong>independent (not necessarily identically distributed) random variables</strong>. If all \(X_i\) have the <strong>same mean and bounded variance</strong>, then the sequence of sample averages \((\frac{1}{n}\sum_{i=1}^n X_i)^\infty_{n=1}\) converges almost surely to \(\mathbf{E}[X_1]\).</p> </div>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[A very, very, very large chunk of math]]></summary></entry></feed>