<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://m0gician.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://m0gician.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-03T01:57:58+00:00</updated><id>https://m0gician.github.io/feed.xml</id><title type="html">blank</title><subtitle>I break everything language model related. Training, inference, optimization, and deployment. </subtitle><entry><title type="html">强化学习 — 价值函数</title><link href="https://m0gician.github.io/blog/2025/value-functions-zh/" rel="alternate" type="text/html" title="强化学习 — 价值函数"/><published>2025-08-02T07:00:00+00:00</published><updated>2025-08-02T07:00:00+00:00</updated><id>https://m0gician.github.io/blog/2025/value-functions-zh</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/value-functions-zh/"><![CDATA[<p>在上一节中，我们介绍了马尔可夫决策过程（MDP）如何融入强化学习。本章将基于 MDP 的定义，展示如何从数学角度评估 RL 智能体。</p> <h2 id="状态价值函数">状态价值函数</h2> <p><em>状态价值函数</em> \(v^\pi(s)\) 表示当智能体从状态 \(s\) 出发并按照策略 \(\pi\) 行动时，其期望折扣回报。通俗地说，它衡量在采用策略 \(\pi\) 时身处状态 \(s\) “有多好”。我们称 \(v^\pi(s)\) 为状态 \(s\) 的价值。</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\ \Bigg[\underbrace{\sum_{k=0}^{\infty}\gamma^k R_{t+k}}_{G_t} \bigg| S_t=s, \pi\ \Bigg] \\ &amp;:= \mathbf{E}[G_t|S_t=s, \pi] \\ &amp;:= \mathbf{E}\ \Bigg[\sum_{t=0}^{\infty}\gamma^k R_{t} \bigg| S_0=s, \pi\ \Bigg] \end{aligned}\] <p>回顾我们在上一节中使用的 \(G_t\)（<em>从时间步 \(t\) 开始的折扣回报</em>）记号，可以发现这正是其等价形式。</p> <h3 id="一个简单的mdp示例">一个简单的 MDP 示例</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mdp-simple-480.webp 480w,/assets/img/rl/mdp-simple-800.webp 800w,/assets/img/rl/mdp-simple-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mdp-simple.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Simple MDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p>在上图所示的 MDP 中，智能体每次可选择两个动作中的一个：<code class="language-plaintext highlighter-rouge">Left</code> 或 <code class="language-plaintext highlighter-rouge">Right</code>。在状态 \(s_1\) 与 \(s_6\) 中，无论采取何种动作都会直接转移至终止状态 \(s_\infty\)。只有在发生 \(s_2 \to s_1\) 或 \(s_5 \to s_6\) 的转移时，智能体才能获得奖励。为简化计算，设折扣因子 \(\gamma = 0.5\)。</p> <p>我们为该 MDP 尝试两种策略：策略 \(\pi_1\) 始终选择 <code class="language-plaintext highlighter-rouge">Left</code>；策略 \(\pi_2\) 始终选择 <code class="language-plaintext highlighter-rouge">Right</code>。</p> <p><strong>策略 1（\(\pi_1\)）：始终选择 <code class="language-plaintext highlighter-rouge">Left</code></strong></p> <ul> <li>\(v^{\pi_1}(s_1) = 0\) （始终直接进入终止状态）</li> <li>\(v^{\pi_1}(s_2) = 12\gamma^0 = 12\).</li> <li>\(v^{\pi_1}(s_3) = 0\gamma^0 + 12\gamma^1 = 6\).</li> <li>\(v^{\pi_1}(s_4) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(v^{\pi_1}(s_5) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(v^{\pi_1}(s_6) = 0\).</li> </ul> <p><strong>策略 2（\(\pi_2\)）：始终选择 <code class="language-plaintext highlighter-rouge">Right</code></strong></p> <ul> <li>\(v^{\pi_2}(s_1) = 0\).</li> <li>\(v^{\pi_2}(s_2) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(v^{\pi_2}(s_3) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(v^{\pi_2}(s_4) = 0\gamma^0 + 2\gamma^1 = 1\).</li> <li>\(v^{\pi_2}(s_5) = 2\gamma^0 = 2\).</li> <li>\(v^{\pi_2}(s_6) = 0\).</li> </ul> <h2 id="行动价值函数">行动价值函数</h2> <p><em>行动价值函数</em> \(q^\pi(s,a)\)（亦称 <em>Q‑函数</em>）表示当智能体在状态 \(s\) 采取动作 \(a\) 并随后按照策略 \(\pi\) 行动时，其期望折扣回报。</p> \[\begin{aligned} q^\pi(s,a) &amp;:= \mathbf{E}\ \Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, A_t = a, \pi\ \Bigg] \\ &amp;:= \mathbf{E}[G_t|S_t=s, A_t=a, \pi] \\ &amp;:= \mathbf{E}\ \Bigg[\sum_{t=0}^{\infty}\gamma^k R_{t} \bigg| S_0=s, A_0=a, \pi\ \Bigg] \end{aligned}\] <h3 id="再看mdp">👀再看MDP</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mdp-simple-480.webp 480w,/assets/img/rl/mdp-simple-800.webp 800w,/assets/img/rl/mdp-simple-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mdp-simple.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Simple MDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p><strong>策略 1（\(\pi_1\)）：始终选择 <code class="language-plaintext highlighter-rouge">Left</code></strong></p> <ul> <li>\(q^{\pi_1}(s_1, L) = 0\).</li> <li>\(q^{\pi_1}(s_1, R) = 0\).</li> <li>\(q^{\pi_1}(s_2, L) = 12\gamma^0 =12\).</li> <li>\(q^{\pi_1}(s_2, R) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(q^{\pi_1}(s_3, L) = 0\gamma^0 + 12\gamma^1 = 6\).</li> <li>\(q^{\pi_1}(s_3, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(q^{\pi_1}(s_4, L) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(q^{\pi_1}(s_4, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 0\gamma^3 + 12\gamma^4 = 0.75\).</li> <li>\(q^{\pi_1}(s_5, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(q^{\pi_1}(s_5, R) = 2\gamma^0 = 2\).</li> <li>\(q^{\pi_1}(s_6, L) = 0\).</li> <li>\(q^{\pi_1}(s_6, R) = 0\).</li> </ul> <p><strong>策略 2（\(\pi_2\)）：始终选择 <code class="language-plaintext highlighter-rouge">Right</code></strong></p> <ul> <li>\(q^{\pi_2}(s_1, L) = 0\).</li> <li>\(q^{\pi_2}(s_1, R) = 0\).</li> <li>\(q^{\pi_2}(s_2, L) = 12\gamma^0 =12\).</li> <li>\(q^{\pi_2}(s_2, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(q^{\pi_2}(s_3, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 0\gamma^3 + 2\gamma^4 = 0.125\).</li> <li>\(q^{\pi_2}(s_3, R) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(q^{\pi_2}(s_4, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(q^{\pi_2}(s_4, R) = 0\gamma^0 + 2\gamma^1 = 1\).</li> <li>\(q^{\pi_2}(s_5, L) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(q^{\pi_2}(s_5, R) = 2\gamma^0 = 2\).</li> <li>\(q^{\pi_2}(s_6, L) = 0\).</li> <li>\(q^{\pi_2}(s_6, R) = 0\).</li> </ul> <h2 id="vpi的贝尔曼方程">\(v^\pi\) 的贝尔曼方程</h2> <p><em>状态价值函数的贝尔曼方程</em>是 \(v^\pi\) 的递归表达式。为推导该方程，我们首先将即时奖励从价值函数中分离出来：</p> \[\begin{aligned} v^\pi(s) &amp;:= \textbf{E}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg\vert S_t=s, \pi\right] \\ &amp;= \textbf{E}\left[R_t + \sum_{k=1}^{\infty}\gamma^k R_{t+k} \bigg\vert S_t=s, \pi\right] \\ &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=1}^{\infty}\gamma^{k-1} R_{t+k} \bigg\vert S_t=s, \pi\right] \end{aligned}\] <p>通过将求和索引调整为从 0 开始（即将所有 \(k\) 替换为 \(k+1\)），可得到</p> \[\begin{aligned} \textbf{E}\left[\gamma\sum_{k=1}^{\infty}\gamma^{k-1} R_{t+k} \bigg\vert S_t=s, \pi\right] = \textbf{E}\left[\gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \end{aligned}\] <p>根据全概率公式 \(\textbf{E}[X] = \textbf{E}[\textbf{E}[X \vert Y]]\)，令首次动作 \(A_t=a\) 及下一状态 \(S_{t+1}=s'\) 作为条件变量，可得</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ = &amp;\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s') \, \textbf{E}\left[\gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t=a, S_{t+1}=s', \pi\right] \end{aligned}\] <p>利用马尔可夫性质，可将条件中的 \(S_t\) 和 \(A_t\) 去掉：</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, \pi\right] \\ = &amp;\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s') \, \gamma\textbf{E}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s', \pi\right] \end{aligned}\] <p>根据状态价值函数定义，最后一项正是 \(v^\pi(s')\)：</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ = &amp; \gamma\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) v^\pi(s^\prime) \end{aligned}\] <p>由于对任意给定 \(s\)、\(a\)，转移到下一状态 \(s'\) 的概率之和为 1，可将即时奖励写成</p> \[\begin{aligned} R_t = \sum_{a\in\mathcal{A}}\pi(s,a)R(s,a) = \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)R(s,a) \end{aligned}\] <p>综合可得</p> \[\begin{aligned} v^\pi(s) &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ &amp;= \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)R(s,a) + \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) \gamma v^\pi(s^\prime) \end{aligned}\] <p>最终，可得到简洁形式</p> \[v^\pi(s) = \boxed{\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\big(R(s,a) +\gamma v^\pi(s^\prime)\big)}\] <h3 id="关于贝尔曼方程的优点">关于贝尔曼方程的优点</h3> <div class="callout"> <p>我们可以将贝尔曼方程视为把期望回报拆分为两部分：</p> <ol> <li>下一时间步获得的奖励（<em>即时奖励</em>）</li> <li>下一状态的价值</li> </ol> \[v^\pi(s) = \textbf{E}\left[\underbrace{R(s,A_t)}_{\text{即时奖励}} + \gamma\underbrace{v^\pi(S_{t+1})}_{\text{下一状态价值}}\Bigg\vert S_t=s, \pi\right]\] <p>原始定义需要考虑整条状态序列，而贝尔曼方程<strong>只需向前看一步</strong>。</p> <ul> <li>贝尔曼方程的递归性质使其在计算上更有帮助</li> </ul> </div> <h2 id="qpi的贝尔曼方程">\(q^\pi\) 的贝尔曼方程</h2> <p>就如 \(v^\pi\) 的贝尔曼方程给出了 \(v^\pi\) 的递归关系一样，\(q^\pi\) 的贝尔曼方程则给出了行动价值函数 \(q^\pi\) 的递归关系：</p> \[\begin{aligned} q^\pi(s,a) &amp;:= \mathbf{E}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, A_t = a, \pi\Bigg] \\ &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t = a, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}} \pi(s^\prime,a^\prime)\times\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t = a, S_{t+1}=s^\prime, A_{t+1}=a^\prime, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime) \times\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, A_{t+1}=a^\prime, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)\gamma q^\pi(s^\prime, s^\prime) \\ &amp;= \boxed{R(s,a) + \gamma\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)q^\pi(s^\prime, a^\prime)} \end{aligned}\] <p>或简写为</p> \[q^\pi(s,a) = \boxed{R(s,a) + \gamma\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)q^\pi(s^\prime, a^\prime)}\] <h2 id="最优价值函数">最优价值函数</h2> <div class="callout"> <p><strong>最优策略</strong> \(\pi^*\)<br/> 若某策略 \(\pi^*\) 至少与所有其他策略一样好，则称其为最优策略。即</p> \[\forall \pi \in \Pi, \; \pi^* \ge \pi\] </div> <div class="callout"> <p>即便最优策略可能不唯一，最优价值函数 \(v^*\) 与 \(q^*\) 却是唯一的——所有最优策略共享同一状态价值函数与行动价值函数。</p> </div> <div class="callout"> <details> <summary>已知最优状态价值函数，若未知转移概率及奖励函数，能否求得最优策略？</summary> <br/><strong>不能</strong>。 $$ \arg\max_{a\in\mathcal{A}}\sum_{s'}p(s,a,s')\big[R(s,a) + \gamma v^\pi(s')\big] $$ 的计算仍依赖于 p 和 R。 </details> </div> <div class="callout"> <details> <summary>已知最优行动价值函数，若未知转移概率及奖励函数，能否求得最优策略？</summary> <br/><strong>可以</strong>。 $$ \arg\max_{a\in\mathcal{A}}q^*(s,a) $$ 即为状态 s 下的最优动作。 </details> </div> <h3 id="v的贝尔曼最优方程">\(v^*\) 的贝尔曼最优方程</h3> <p>从贝尔曼方程出发，</p> \[v^*(s) = \sum_{a\in\mathcal{A}}\pi^*(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s')\big[R(s,a) + \gamma v^*(s')\big]\] <p>由于最优策略 \(\pi^*\) 仅选择能最大化 \(q^*(s,a)\) 的动作，可写为</p> \[v^*(s) = \max_{a\in\mathcal{A}}\sum_{s' \in \mathcal{S}}p(s,a,s')\big[R(s,a) + \gamma v^*(s')\big]\] <p>这就是 <em>\(v^*\) 的贝尔曼最优方程</em>。</p> <div class="callout"> <p>若一个策略 \(\pi\) 满足贝尔曼最优方程，则对所有状态 \(s \in \mathcal{S}\) 有</p> \[v^*(s) = \max_{a\in\mathcal{A}}\sum_{s' \in \mathcal{S}}p(s,a,s')\big[R(s,a) + \gamma v^*(s')\big]\] </div> <h3 id="q的贝尔曼最优方程">\(q^*\) 的贝尔曼最优方程</h3> <div class="callout"> <p>若一个策略 \(\pi\) 满足贝尔曼最优方程，则对所有动作 \(a \in \mathcal{A}\) 有</p> \[q^*(s,a) = \sum_{s' \in \mathcal{S}} p(s,a,s')\left[ R(s,a) + \gamma \max_{a'\in\mathcal{A}}q^*(s', a')\right]\] </div> <h3 id="贝尔曼最优方程与最优策略">贝尔曼最优方程与最优策略</h3> <div class="callout"> <p><em>若策略 \(\pi\) 满足贝尔曼最优方程，则 \(\pi\) 为最优策略。</em></p> </div> <p><em>证明：</em></p> <p>假设一个策略 \(\pi\) 满足贝尔曼最优方程，那么对于所有状态 \(s\)：</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^\pi(s^\prime)]\] <p>我们可以将贝尔曼最优方程递归地代入表达式中，并替换 \(v^\pi(s^\prime)\)：</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma v^\pi(s^{\prime\prime})\right)\right]\] <p>我们可以无限地继续这个过程，直到 \(\pi\) 从表达式中完全消失：</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right]\] <p>在每个时间步 \(t\)，选择的动作都是最大化未来期望折扣回报的动作，前提是未来的动作也是为了最大化未来折扣回报。</p> <p>现在，让我们考虑任何一个新的策略 \(\pi^\prime\)。如果我们将 \(\max_{a \in \mathcal{A}}\) 替换为 \(\sum_{a \in \mathcal{A}}\pi^\prime(s,a)\)，关系会怎样？我们认为表达式的值不会变得比之前更大。也就是说，对于任何策略 \(\pi^\prime\)：</p> \[\begin{aligned} v^\pi(s) &amp;= \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \\ &amp;\geq \sum_{a \in \mathcal{A}}\pi^\prime(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\sum_{a^\prime \in \mathcal{A}}\pi^\prime(s^\prime,a^\prime)\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \end{aligned}\] <p>鉴于上述不等式对所有策略 \(\pi^\prime\) 都成立，我们得出对于所有状态 \(s \in \mathcal{S}\) 和所有策略 \(\pi^\prime \in \Pi\)：</p> \[\begin{aligned} v^\pi(s) &amp;= \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \\ &amp;\geq \mathbf{E}[G_t | S_t = s, \pi^\prime] \\ &amp;= v^{\pi^\prime}(s) \end{aligned}\] <p>因此，对于所有状态 \(s \in \mathcal{S}\) 和所有策略 \(\pi^\prime \in \Pi\)，\(v^\pi(s) \geq v^{\pi^\prime}(s)\)。换句话说，对于所有策略 \(\pi^\prime \in \Pi\)，我们有 \(\pi \geq \pi^\prime\)，因此 \(\pi\) 是一个最优策略。</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[深入探讨强化学习中的价值函数与贝尔曼方程。]]></summary></entry><entry><title type="html">Reinforcement Learning — Value Functions</title><link href="https://m0gician.github.io/blog/2025/value-functions/" rel="alternate" type="text/html" title="Reinforcement Learning — Value Functions"/><published>2025-08-02T07:00:00+00:00</published><updated>2025-08-02T07:00:00+00:00</updated><id>https://m0gician.github.io/blog/2025/value-functions</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/value-functions/"><![CDATA[<p>Last time, we covered how MDPs are integrated into Reinforcement Learning. In this chapter, we will see how we can evaluate an RL agent mathematically based on the definitions of MDPs.</p> <h2 id="state-value-function">State-Value Function</h2> <p>The <em>State-Value function</em> \(v^\pi(s)\) calculates the expected discounted return if the agent starts in state \(s\) and follows policy \(\pi\). Informally, it tells us how “good” it is for the agent to be in state \(s\) when using policy \(\pi\). We call \(v^\pi(s)\) the <em>value of state</em> \(s\).</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\ \Bigg[\underbrace{\sum_{k=0}^{\infty}\gamma^k R_{t+k}}_{G_t} \bigg| S_t=s, \pi\ \Bigg] \\ &amp;:= \mathbf{E}[G_t|S_t=s, \pi] \\ &amp;:= \mathbf{E}\ \Bigg[\sum_{t=0}^{\infty}\gamma^k R_{t} \bigg| S_0=s, \pi\ \Bigg] \end{aligned}\] <p>Recalling the \(G_t\) notation (<em>discounted return from time</em> \(t\)) we covered last time, we can see that this is an equivalent definition.</p> <h3 id="a-simple-mdp-example">A Simple MDP Example</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mdp-simple-480.webp 480w,/assets/img/rl/mdp-simple-800.webp 800w,/assets/img/rl/mdp-simple-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mdp-simple.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Simple MDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p>In the MDP above, the agent can choose between two actions: <code class="language-plaintext highlighter-rouge">Left</code> or <code class="language-plaintext highlighter-rouge">Right</code>. In states \(s_1\) and \(s_6\), any action will cause a transition to the terminal state \(s_\infty\). The agent only gets a reward when transitioning from \(s_2 \to s_1\) or \(s_5 \to s_6\). For simplicity, we will use \(\gamma = 0.5\).</p> <p>Let’s test two policies for this MDP. One policy, \(\pi_1\), will always select <code class="language-plaintext highlighter-rouge">Left</code> action; another policy, \(\pi_2\), will always select the <code class="language-plaintext highlighter-rouge">Right</code> action.</p> <p><strong>Policy 1 (\(\pi_1\)): Select <code class="language-plaintext highlighter-rouge">Left</code> Action Always</strong></p> <ul> <li>\(v^{\pi_1}(s_1) = 0\) (goes to the terminal state always)</li> <li>\(v^{\pi_1}(s_2) = 12\gamma^0 = 12\).</li> <li>\(v^{\pi_1}(s_3) = 0\gamma^0 + 12\gamma^1 = 6\).</li> <li>\(v^{\pi_1}(s_4) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(v^{\pi_1}(s_5) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(v^{\pi_1}(s_6) = 0\).</li> </ul> <p><strong>Policy 2 (\(\pi_2\)): Select <code class="language-plaintext highlighter-rouge">Right</code> Action Always</strong></p> <ul> <li>\(v^{\pi_2}(s_1) = 0\).</li> <li>\(v^{\pi_2}(s_2) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(v^{\pi_2}(s_3) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(v^{\pi_2}(s_4) = 0\gamma^0 + 2\gamma^1 = 1\).</li> <li>\(v^{\pi_2}(s_5) = 2\gamma^0 = 2\).</li> <li>\(v^{\pi_2}(s_6) = 0\).</li> </ul> <h2 id="action-value-function">Action-Value Function</h2> <p>The <em>Action-Value Function</em> \(q^\pi(s,a)\), or <em>Q-function</em>, evaluates the expected discounted return if the agent takes action \(a\) in state \(s\) and follows policy \(\pi\) thereafter.</p> \[\begin{aligned} q^\pi(s,a) &amp;:= \mathbf{E}\ \Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, A_t = a, \pi\ \Bigg] \\ &amp;:= \mathbf{E}[G_t|S_t=s, A_t=a, \pi] \\ &amp;:= \mathbf{E}\ \Bigg[\sum_{t=0}^{\infty}\gamma^k R_{t} \bigg| S_0=s, A_0=a, \pi\ \Bigg] \end{aligned}\] <h3 id="a-simple-mdp-example-again">A Simple MDP Example, Again</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mdp-simple-480.webp 480w,/assets/img/rl/mdp-simple-800.webp 800w,/assets/img/rl/mdp-simple-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mdp-simple.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Simple MDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p><strong>Policy 1 (\(\pi_1\)): Select <code class="language-plaintext highlighter-rouge">Left</code> Action Always</strong></p> <ul> <li>\(q^{\pi_1}(s_1, L) = 0\).</li> <li>\(q^{\pi_1}(s_1, R) = 0\).</li> <li>\(q^{\pi_1}(s_2, L) = 12\gamma^0 =12\).</li> <li>\(q^{\pi_1}(s_2, R) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(q^{\pi_1}(s_3, L) = 0\gamma^0 + 12\gamma^1 = 6\).</li> <li>\(q^{\pi_1}(s_3, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(q^{\pi_1}(s_4, L) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(q^{\pi_1}(s_4, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 0\gamma^3 + 12\gamma^4 = 0.75\).</li> <li>\(q^{\pi_1}(s_5, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(q^{\pi_1}(s_5, R) = 2\gamma^0 = 2\).</li> <li>\(q^{\pi_1}(s_6, L) = 0\).</li> <li>\(q^{\pi_1}(s_6, R) = 0\).</li> </ul> <p><strong>Policy 2 (\(\pi_2\)): Select <code class="language-plaintext highlighter-rouge">Right</code> Action Always</strong></p> <ul> <li>\(q^{\pi_2}(s_1, L) = 0\).</li> <li>\(q^{\pi_2}(s_1, R) = 0\).</li> <li>\(q^{\pi_2}(s_2, L) = 12\gamma^0 =12\).</li> <li>\(q^{\pi_2}(s_2, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(q^{\pi_2}(s_3, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 0\gamma^3 + 2\gamma^4 = 0.125\).</li> <li>\(q^{\pi_2}(s_3, R) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(q^{\pi_2}(s_4, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(q^{\pi_2}(s_4, R) = 0\gamma^0 + 2\gamma^1 = 1\).</li> <li>\(q^{\pi_2}(s_5, L) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(q^{\pi_2}(s_5, R) = 2\gamma^0 = 2\).</li> <li>\(q^{\pi_2}(s_6, L) = 0\).</li> <li>\(q^{\pi_2}(s_6, R) = 0\).</li> </ul> <h2 id="the-bellman-equation-for-vpi">The Bellman Equation for \(v^\pi\)</h2> <p>The <em>Bellman Equation for</em> \(v^\pi\) is a recursive expression for the state-value function. To derive the Bellman equation, we first isolate the immediate reward from the state-value function:</p> \[\begin{aligned} v^\pi(s) &amp;:= \textbf{E}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg\vert S_t=s, \pi\right] \\ &amp;= \textbf{E}\left[R_t + \sum_{k=1}^{\infty}\gamma^k R_{t+k} \bigg\vert S_t=s, \pi\right] \\ &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=1}^{\infty}\gamma^{k-1} R_{t+k} \bigg\vert S_t=s, \pi\right] \end{aligned}\] <p>We can perform a simple transformation by modifying the indexing of the sum to start at zero instead of one, which changes all uses of \(k\) within the sum to \(k+1\):</p> \[\begin{aligned} \textbf{E}\left[\gamma\sum_{k=1}^{\infty}\gamma^{k-1} R_{t+k} \bigg\vert S_t=s, \pi\right] = \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \end{aligned}\] <p>Recalling the law of total probability, \(\textbf{E}[X] = \textbf{E}[\textbf{E}[X\vert Y]]\), if we use the first action \(A_t=a\) and the first next state \(S_{t+1}=s^\prime\) as intermediate conditioning variables, we can rewrite the expected reward after the first action term as:</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ = &amp;\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) \times \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t=a, S_{t+1}=s^\prime, \pi\right] \end{aligned}\] <p>Furthermore, the Markov property tells us the future is independent of everything before \(t+1\). Hence, for the expected next-step reward, we can safely remove \(S_t\) and \(A_t\) from the conditioning:</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ = &amp;\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) \times \gamma\textbf{E}\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, \pi\right] \end{aligned}\] <p>Recall the definition of the state-value function, the last term is exactly \(v^\pi(s^\prime)\):</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, \pi\right] \\ = &amp; \gamma\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) v^\pi(s^\prime) \end{aligned}\] <p>Similarly, since for any fixed \(s\) and \(a\) the transition probability to the next state \(s^\prime\) must be 1, we can multiply the immediate reward \(R_t\) by “one”:</p> \[\begin{aligned} R_t = \sum_{a\in\mathcal{A}}\pi(s,a)R(s,a) = \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)R(s,a) \end{aligned}\] <p>Now, we put everything together:</p> \[\begin{aligned} v^\pi(s) &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ &amp;= \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)R(s,a) + \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) \gamma v^\pi(s^\prime) \end{aligned}\] <p>Now, we can combine the common terms to get the final simplified form of the state-value function:</p> \[v^\pi(s) = \boxed{\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\big(R(s,a) +\gamma v^\pi(s^\prime)\big)}\] <h3 id="pros-about-bellman-equation">Pros about Bellman Equation</h3> <div class="callout"> <p>We can view the Bellman equation as breaking the expected return that will occur into two parts:</p> <ol> <li>the reward that we will obtain during the next time step (<em>immediate reward</em>)</li> <li>the value of the next state that we end up in</li> </ol> \[v^\pi(s) = \textbf{E}\left[\underbrace{R(s,A_t)}_{\text{immediate reward}} + \gamma\underbrace{v^\pi(S_{t+1})}_{\text{value of next state}}\Bigg\vert S_t=s, \pi\right]\] <p>While the original definition of the value function must consider the entire sequence of states, the Bellman equation on the other hand, <strong>only needs to look forward one time step into the future</strong>.</p> <ul> <li>the recurrent nature of the Bellman equation makes it more computationally helpful</li> </ul> </div> <h2 id="the-bellman-equation-for-qpi">The Bellman Equation for \(q^\pi\)</h2> <p>While the Bellman equation for \(v^\pi\) is a recursive expression for \(v^\pi\), the Bellman equation for \(q^\pi\) is a recursive expression for the action-value function \(q^\pi\).</p> \[\begin{aligned} q^\pi(s,a) &amp;:= \mathbf{E}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, A_t = a, \pi\Bigg] \\ &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t = a, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}} \pi(s^\prime,a^\prime)\times\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t = a, S_{t+1}=s^\prime, A_{t+1}=a^\prime, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime) \times\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, A_{t+1}=a^\prime, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)\gamma q^\pi(s^\prime, s^\prime) \\ &amp;= \boxed{R(s,a) + \gamma\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)q^\pi(s^\prime, a^\prime)} \end{aligned}\] <h2 id="optimal-value-functions">Optimal Value Functions</h2> <div class="callout"> <p><strong>Optimal Policy</strong> \(\pi^*\) An optimal policy, \(\pi^*\) is any policy that is at least as good as all other policies. In other words, \(\pi^*\) is an optimal policy if and only if</p> \[\forall \pi \in \Pi, \pi^* \geq \pi\] </div> <div class="callout"> <p>Notice that even when \(\pi^*\) is not unique, the optimal value functions \(v^*\) and \(q^*\) are unique—all optimal policies share the same state-value function and action-value function.</p> </div> <div class="callout"> <details> <summary> Given the optimal state-value function, can you compute the optimal policy if you do not know the transition probabilities and reward function? </summary> <br/><strong>No</strong>. $$ \arg\max_{a\in\mathcal{A}}\sum_{s^\prime}p(s,a,s^\prime)[R(s,a) + \gamma v^\pi(s^\prime)] $$ is an optimal action in state s. Computing these actions requires knowledge of p and R </details> </div> <div class="callout"> <details> <summary> Given the optimal action-value function, can you compute the optimal policy if you do not know the transition probabilities and reward function? </summary> <br/><strong>Yes</strong>. $$ \arg\max_{a\in\mathcal{A}}q^*(s,a) $$ is an optimal action in state s </details> </div> <h3 id="bellman-optimality-equation-for-v">Bellman Optimality Equation for \(v^*\)</h3> <p>The <em>Bellman Optimality Equation for</em> \(v^*\) is a recursive expression for \(v^*\). Let’s start with the Bellman equation:</p> \[v^*(s) = \sum_{a\in\mathcal{A}}\pi^*(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^*(s^\prime)]\] <p>Since the optimal policy \(\pi^*\) only picks the action that maximizes \(q^*(s,a)\), we do not need to consider all possible actions \(a\), but only those that cause the \(q^*(s,a)\) term to be maximized:</p> \[v^*(s) = \max_{a\in\mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^*(s^\prime)]\] <p>The equation above is the <em>Bellman optimality equation</em> <em>for</em> \(v^*\).</p> <div class="callout"> <p>A policy \(\pi\), <em>satisfies the Bellman optimality equation</em> if for all states \(s \in \mathcal{S}\):</p> \[v^*(s) = \max_{a\in\mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^*(s^\prime)]\] </div> <h3 id="bellman-optimality-equation-for-q">Bellman Optimality Equation for \(q^*\)</h3> <div class="callout"> <p>A policy \(\pi\), satisfies the Bellman optimality equation if for all actions \(a \in \mathcal{A}\):</p> \[q^*(s,a) = \sum_{s^\prime \in \mathcal{S}} p(s,a,s^\prime)\left[ R(s,a) + \gamma \max_{a^\prime\in\mathcal{A}}q^*(s^\prime, a^\prime)\right]\] </div> <h3 id="bellman-optimality-equation-and-the-optimal-policy">Bellman Optimality Equation and the Optimal Policy</h3> <div class="callout"> <p><em>If a policy <strong>\(\pi\)</strong> satisfies the Bellman optimality equation, then <strong>\(\pi\)</strong> is an optimal policy.</em></p> </div> <p><em>Proof:</em></p> <p>Assuming a policy \(\pi\) satisfies the Bellman optimality equation, we have for all states \(s\):</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^\pi(s^\prime)]\] <p>We can apply the Bellman optimality equation recursively into the expression and replace \(v^\pi(s^\prime)\):</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma v^\pi(s^{\prime\prime})\right)\right]\] <p>We could continue this process indefinitely until \(\pi\) is completely eliminated from the expression:</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right]\] <p>At each time \(t\), the action is chosen that maximizes the expected discounted sum of future rewards, given that future actions are also chosen to maximize the discounted sum of future rewards.</p> <p>Now, let’s consider any new policy \(\pi^\prime\). What will be the relationship if we replace \(\max_{a \in \mathcal{A}}\) with \(\sum_{a \in \mathcal{A}}\pi^\prime(s,a)\)? We argue that the expression could not become bigger than the previous one. That is, for any policy \(\pi^\prime\):</p> \[\begin{aligned} v^\pi(s) &amp;= \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \\ &amp;\geq \sum_{a \in \mathcal{A}}\pi^\prime(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\sum_{a^\prime \in \mathcal{A}}\pi^\prime(s^\prime,a^\prime)\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \end{aligned}\] <p>Given that the above holds for all policies \(\pi^\prime\), we have that for all states \(s \in \mathcal{S}\) and all policies \(\pi^\prime \in \Pi\):</p> \[\begin{aligned} v^\pi(s) &amp;= \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \\ &amp;\geq \mathbf{E}[G_t | S_t = s, \pi^\prime] \\ &amp;= v^{\pi^\prime}(s) \end{aligned}\] <p>Hence, for all states \(s \in \mathcal{S}\), and all policies \(\pi^\prime \in \Pi\), \(v^\pi(s) \geq v^{\pi^\prime}(s)\). In other words, for all policies \(\pi^\prime \in \Pi\), we have that \(\pi \geq \pi^\prime\), and hence \(\pi\) is an optimal policy.</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[A deep dive into value functions and the Bellman equation in Reinforcement Learning.]]></summary></entry><entry><title type="html">强化学习 - 马尔可夫决策过程与强化学习</title><link href="https://m0gician.github.io/blog/2025/mdp-and-rl-zh/" rel="alternate" type="text/html" title="强化学习 - 马尔可夫决策过程与强化学习"/><published>2025-07-15T04:00:00+00:00</published><updated>2025-07-15T04:00:00+00:00</updated><id>https://m0gician.github.io/blog/2025/mdp-and-rl-zh</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/mdp-and-rl-zh/"><![CDATA[<h2 id="什么是强化学习">什么是强化学习？</h2> <div class="blockquote"> <p>强化学习是机器学习的一个领域，其灵感来自行为主义心理学，研究智能体如何从与环境的互动中学习。 <br/>—Sutton &amp; Barto (1998), Phil, <cite>维基百科</cite></p> </div> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/rl-system-480.webp 480w,/assets/img/rl/rl-system-800.webp 800w,/assets/img/rl/rl-system-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/rl-system.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="强化学习图示" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p>一个典型的强化学习系统由5个部分组成：<strong>智能体</strong>（agent）在<strong>环境</strong>（environment）中的每个<strong>状态</strong>（state）下执行一个<strong>动作</strong>（action），并在满足某些标准时获得<strong>奖励</strong>（reward）。</p> <div class="callout"> <details><summary><strong>监督学习问题可以转化为强化学习问题吗？</strong></summary> <strong>可以</strong>。我们可以将一个监督学习问题转化为一个强化学习问题（状态作为分类器的输入；动作作为标签；如果标签正确，奖励为1，否则为-1）。</details> </div> <div class="callout"> <details><summary><strong>强化学习是监督学习的替代品吗？</strong></summary> <p><strong>不是</strong>。监督学习使用指导性反馈（智能体应该采取什么行动）。任何偏离所提供反馈的行为都会受到惩罚。</p> <p>另一方面，强化学习问题不是以固定的数据集形式提供的，而是以代码或整个环境的描述形式提供的。强化学习中的奖励应该传达智能体的行为有多“好”，而不是最好的行为应该是什么。智能体的目标是最大化总奖励，这可能需要智能体放弃眼前的奖励以获得以后更大的奖励。</p> <p>如果你有一个序列问题或一个只有评估性反馈可用的问题（或两者兼有！），那么你应该考虑使用强化学习。</p></details> </div> <h3 id="示例网格世界">示例：网格世界</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/gridworld-480.webp 480w,/assets/img/rl/gridworld-800.webp 800w,/assets/img/rl/gridworld-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/gridworld.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="网格世界" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p><strong>状态</strong>: 机器人的位置。机器人没有朝向。</p> <p><strong>动作</strong>: <code class="language-plaintext highlighter-rouge">尝试向上</code> (AU), <code class="language-plaintext highlighter-rouge">尝试向下</code> (AD), <code class="language-plaintext highlighter-rouge">尝试向左</code> (AL), <code class="language-plaintext highlighter-rouge">尝试向右</code> (AR)</p> <p><strong>环境动态</strong>:</p> <p><strong>奖励</strong>:</p> <ul> <li>智能体进入有水的状态会得到-10的奖励，进入目标状态会得到+10的奖励。</li> <li>进入任何其他状态的奖励为零。</li> <li>任何导致智能体停留在状态21的动作都将被视为再次进入水域状态，并导致额外的-10奖励。</li> <li>奖励折扣参数 \(\gamma = 0.9\)。</li> </ul> <p><strong>状态数量</strong>: 24</p> <ul> <li>23个正常状态 + 1个终止吸收状态 (\(s_\infty\)) <ul> <li>一旦进入\(s_\infty\)，智能体就永远无法离开（<strong>回合</strong>结束）。</li> <li>\(s_\infty\) 不应被认为是“目标”状态。</li> </ul> </li> </ul> <hr/> <h2 id="用数学方式描述智能体和环境">用数学方式描述智能体和环境</h2> <h3 id="环境的数学定义">环境的数学定义</h3> <p>我们可以使用<strong>马尔可夫决策过程</strong>（MDPs）来形式化强化学习问题的环境。其中的独特术语是\(\mathcal{S}\)（所有可能状态的集合），\(\mathcal{A}\)（所有可能动作的集合），\(p\)（转移函数），\(d_R\)（奖励分布），\(R\)（奖励函数），\(d_0\)（初始状态分布）和\(\gamma\)（奖励折扣参数）。环境的通用定义是</p> \[(\mathcal{S}, \mathcal{A}, p, R, \gamma)\] <h3 id="智能体的数学定义">智能体的数学定义</h3> <p>我们将智能体选择动作的决策规则定义为<strong>策略</strong>。形式上，策略\(\pi\)是一个函数</p> \[\begin{aligned} &amp;\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0,1] \\ &amp;\pi(s,a) := \text{Pr}(A_t=a | S_t=s) \end{aligned}\] <div class="callout"> <p><strong>智能体的目标</strong></p> <p>智能体的目标是找到一个最优策略\(\pi^*\)，以最大化智能体将获得的总奖励的期望值。</p> </div> <h3 id="示例山地车">示例：山地车</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mountain-car-480.webp 480w,/assets/img/rl/mountain-car-800.webp 800w,/assets/img/rl/mountain-car-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mountain-car.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="山地车" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <div class="caption"> 山地车环境 </div> <ul> <li><strong>状态</strong>: \(s=(x,v)\), 其中 \(x \in \mathbb{R}\) 是小车的位置，\(v \in \mathbb{R}\) 是速度。</li> <li><strong>动作</strong>: \(a \in \{\texttt{倒车}, \texttt{空挡}, \texttt{前进}\}\). 这些动作被映射为数值 \(a \in \{-1, 0 ,1\}\)。</li> <li> <p><strong>动态</strong>: 动态是确定性的——在状态\(s\)下采取动作\(a\)总是产生相同的状态\(s^\prime\)。因此，\(p(s,a,s^\prime) \in \{0, 1\}\)。动态特性如下：</p> \[\begin{aligned} v_{t+1} &amp;= v_t + 0.001 a_t - 0.0025 \cos(3x_t) \\ x_{t+1} &amp;= x_t + v_{t+1} \end{aligned}\] <p>在计算出下一个状态 \(s^\prime = [x_{t+1}, v_{t+1}]\) 后，</p> <ul> <li>\(x_{t+1}\) 的值被限制在闭区间 \([-1.2, 0.5]\) 内。</li> <li>\(v_{t+1}\) 的值被限制在闭区间 \([-0.7, 0.7]\) 内。</li> <li>如果 \(x_{t+1}\) 到达左边界或右边界（\(x_{t+1} = -1.2\) 或 \(x_{t+1} = 0.5\)），那么小车的速度将重置为零（\(v_{t+1} = 0\)）。</li> </ul> </li> <li><strong>初始状态</strong>: \(S_0 = (X_0, 0)\), 其中 \(X_0\) 是从区间 \([-0.6, -0.4]\) 中均匀随机抽取的初始位置。</li> <li><strong>终止状态</strong>: 如果 \(x_t = 0.5\)，则该状态为终止状态（它总是转移到 \(s_\infty\)）。</li> <li><strong>奖励</strong>: \(R_t\) 总是为 -1，除非转移到 \(s_\infty\)（从 \(s_\infty\) 或从终止状态），此时 \(R_t = 0\)。</li> <li><strong>折扣</strong>: \(\gamma = 1.0\)。</li> </ul> <hr/> <h3 id="附加术语符号和假设">附加术语、符号和假设</h3> <ul> <li> <p><strong>历史</strong>（history），\(H_t\)，是回合中直到时间\(t\)所发生事件的记录：</p> \[H_t := (S_0, A_0, R_0, S_1, A_1, R_1, \ldots, S_t, A_t, R_t)\] </li> <li><strong>轨迹</strong>（trajectory）是整个回合的历史：\(H_\infty\)</li> <li>轨迹的<strong>回报</strong>（return）或<strong>折扣回报</strong>（discounted return）是奖励的折扣总和 \(G := \sum_{t = 0}^{\infty} \gamma^t R_t\)</li> <li><strong>期望回报</strong>（expected return）或<strong>期望折扣回报</strong>（expected discounted return）可以写成 \(J(\pi) := \mathbf{E}[G\vert\pi]\)</li> <li>从时间\(t\)开始的<strong>回报</strong>或从时间\(t\)开始的<strong>折扣回报</strong>，\(G_t\)，是从时间\(t\)开始的奖励的折扣总和</li> </ul> \[G_t := \sum_{k=1}^{\infty} \gamma^k R_{t+k}\] <ul> <li> <p>MDP的<strong>范围</strong>（horizon），\(L\)，是满足以下条件的最小整数</p> \[\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\] <ul> <li>如果对于所有策略 \(L &lt; \infty\)，我们称该MDP为<strong>有限范围</strong>（finite horizon）</li> <li>如果 \(L = \infty\)，则该领域可能是<strong>不确定范围</strong>（indefinite horizon）（智能体总是会进入\(s_\infty\)）或<strong>无限范围</strong>（infinite horizon）（智能体可能永远不会进入\(s_\infty\)）</li> </ul> </li> </ul> <hr/> <h3 id="马尔可夫性质">马尔可夫性质</h3> <div class="callout"> <p><strong>马尔可夫性质 (</strong>马尔可夫假设<strong>)</strong></p> <p>简而言之：<strong><em>给定现在，未来与过去无关</em></strong>。</p> <p>形式上，给定\(S_t\)，\(S_{t+1}\) 条件独立于 \(H_{t-1}\)。也就是说，对于所有的 \(h, s, a, s^\prime, t\)：</p> <p>\(\text{Pr}(S_{t+1} = s^\prime | H_{t-1} = h, S_{t}=s, A_{t}=a) = \text{Pr}(S_{t+1}=s^\prime | S_{t}=s, A_{t}=a)\)</p> </div> <p>如果一个模型（环境、奖励…）满足马尔可夫假设，我们就说它具有马尔可夫性质，或者说这个模型是 <strong><em>Markovian</em></strong>的。</p> <hr/> <h2 id="为什么在强化学习中使用mdp">为什么在强化学习中使用MDP？</h2> <p>MDP不仅功能强大，足以模拟学习智能体与其环境之间的交互，它还带来了一些关键的保证，使我们的“强化学习”能够真正起作用。</p> <p>现在，让我们跳过推导，直接看结论。</p> <div class="callout"> <p><strong>最优策略的存在性</strong></p> <p>对于所有满足 \(|\mathcal{S}| &lt; \infty\), \(|\mathcal{A}| &lt; \infty\), \(R_\text{max} &lt; \infty\) 和 \(\gamma &lt; 1\) 的MDP，至少存在一个最优策略 \(\pi^*\)。</p> </div> <p>稍后当我们介绍<strong>贝尔曼方程</strong>和<strong>贝尔曼最优方程</strong>时，我们将进一步确定：</p> <ol> <li>如果一个策略\(\pi\)在每个步骤中都达到了一个状态，其期望的未来奖励无法通过任何其他行动或决策进一步提高（<strong>贝尔曼最优方程</strong>），那么它就是一个最优策略。</li> <li>如果只有有限数量的可能状态和动作，奖励有界，并且未来奖励被折扣（折扣因子\(\gamma &lt; 1\)），那么存在一个满足贝尔曼最优方程的策略\(\pi\)。</li> </ol> <p>此外，我们可以使用贝尔曼方程和贝尔曼最优方程执行策略/价值迭代（稍后将介绍）。因此，我们不仅可以一次又一次地迭代得到更好的策略，而且在某些约束下，还可以证明最终策略将收敛到最优策略。</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[强化学习概念入门，包括马尔可夫决策过程（MDP）。]]></summary></entry><entry><title type="html">Reinforcement Learning - MDP and RL</title><link href="https://m0gician.github.io/blog/2025/mdp-and-rl/" rel="alternate" type="text/html" title="Reinforcement Learning - MDP and RL"/><published>2025-07-14T20:00:00+00:00</published><updated>2025-07-14T20:00:00+00:00</updated><id>https://m0gician.github.io/blog/2025/mdp-and-rl</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/mdp-and-rl/"><![CDATA[<h2 id="what-is-reinforcement-learning">What is Reinforcement Learning?</h2> <div class="blockquote"> <p>"Reinforcement Learning is an area of machine learning, inspired by behaviorist psychology, concerned with how an agent can learn from interactions with an environment." <br/>Sutton &amp; Barto (1998), Phil, <cite>Wikipedia</cite></p> </div> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/rl-system-480.webp 480w,/assets/img/rl/rl-system-800.webp 800w,/assets/img/rl/rl-system-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/rl-system.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="RL System" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p>A typical Reinforcement Learning system consists of 5 components: an <strong>agent</strong> takes an <strong>action</strong> at each <strong>state</strong> in an <strong>environment</strong> and receives a <strong>reward</strong> if some criteria are met.</p> <div class="callout"> <details><summary><strong>Can a Supervised Learning problem be converted into a RL problem?</strong></summary> <strong>Yes</strong>. One might take a supervised learning problem and convert it into an RL problem (the state as the input to a classifier; the action as a label; and the reward as 1 if the label is correct and -1 otherwise).</details> </div> <div class="callout"> <details><summary><strong>Is RL an alternative to Supervised Learning?</strong></summary> <p><strong>No</strong>. Supervised learning uses instructive feedback (what action the agent should have taken). Anything deviates the provided feedback will be penalized.</p> <p>RL problems on the other hand aren’t provided as fixed data sets but as code or descriptions of the entire environment. Rewards in RL should convey how “good” an agent’s actions are, not what the best actions would have been. The goal of the agent is to maximize the total reward and this might require the agent forgoing immediate reward to obtain larger reward later.</p> <p>If you have a sequential problem or a problem where only evaluative feedback is available (or both!), then you should consider use RL.</p></details> </div> <h3 id="example-gridworld">Example: Gridworld</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/gridworld-480.webp 480w,/assets/img/rl/gridworld-800.webp 800w,/assets/img/rl/gridworld-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/gridworld.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Gridworld" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p><strong>State</strong>: Position of robot. The robot does not have a direction that it is facing.</p> <p><strong>Actions</strong>: <code class="language-plaintext highlighter-rouge">Attemp_Up</code> (AU), <code class="language-plaintext highlighter-rouge">Attemp_Down</code> (AD), <code class="language-plaintext highlighter-rouge">Attemp_Left</code> (AL), <code class="language-plaintext highlighter-rouge">Attemp_Right</code> (AR)</p> <p><strong>Environment Dynamics</strong>:</p> <p><strong>Rewards</strong>:</p> <ul> <li>The agent receives a reward of -10 for entering the state with the water and a record of +10 for entering the goal state.</li> <li>Entering any other state results in a reward of zero.</li> <li>Any actions that cause the agent stays in state 21 will count as “entering” the water state again and result in an additional reward of -10.</li> <li>Reward discount parameter \(\gamma = 0.9\).</li> </ul> <p><strong>Number of States</strong>: 24</p> <ul> <li>23 normal states + 1 terminal absorbing state (\(s_\infty\)) <ul> <li>Once in \(s_\infty\), the agent can never leave (<em>episode</em> ends).</li> <li>\(s_\infty\) should not be thought as “goal” state.</li> </ul> </li> </ul> <hr/> <h2 id="describe-the-agent-and-environment-mathematically">Describe the Agent and Environment Mathematically</h2> <h3 id="math-definition-for-environment">Math Definition for Environment</h3> <p>We can use <em>Markov Decision Processes</em> (MDPs) to formalize the environment of an RL problem. The unique terms are \(\mathcal{S}\) (the set of all possible states), \(\mathcal{A}\) (the set of all possible actions), \(p\) (transition function), \(d_R\) (reward distribution), \(R\) (reward function), \(d_0\) (initial state distribution), and \(\gamma\) (reward discount parameter). The common definition of the environment is</p> \[(\mathcal{S}, \mathcal{A}, p, R, \gamma)\] <h3 id="math-definition-for-agent">Math Definition for Agent</h3> <p>We define the decision rule that the agent selects actions as a <strong>policy</strong>. Formally, a policy \(\pi\) is a function</p> \[\begin{aligned} &amp;\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0,1] \\ &amp;\pi(s,a) := \text{Pr}(A_t=a | S_t=s) \end{aligned}\] <div class="callout"> <p><strong>Agent’s Goal</strong></p> <p>The agent’s goal is to find an optimal policy \(\pi^*\) that maximizes the expected total amount of reward that the agent will obtain.</p> </div> <h3 id="example-mountain-car">Example: Mountain Car</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mountain-car-480.webp 480w,/assets/img/rl/mountain-car-800.webp 800w,/assets/img/rl/mountain-car-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mountain-car.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Mountain Car" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <ul> <li><strong>State</strong>: \(s=(x,v)\), where \(x \in \mathbb{R}\) is the position of the car and \(v \in \mathbb{R}\) is the velocity.</li> <li><strong>Actions</strong>: \(a \in \{\texttt{reverse}, \texttt{neutral}, \texttt{forward}\}\). These actions are mapped to numerical values as \(a \in \{-1, 0 ,1\}\).</li> <li><strong>Dynamics</strong>: The dynamics are deterministic—taking action \(a\) in state \(s\) always produces the same state, \(s^\prime\). Thus, \(p(s,a,s^\prime) \in \{0, 1\}\). The dynamics are characterized by:</li> </ul> \[\begin{aligned} v_{t+1} &amp;= v_t + 0.001 a_t - 0.0025 \cos(3x_t) \\ x_{t+1} &amp;= x_t + v_{t+1} \end{aligned}\] <p>After the next state, \(s^\prime = [x_{t+1}, v_{t+1}]\) has been computed,</p> <ul> <li>the value of \(x_{t+1}\) is clipped so that it stays in the closed interval \([-1.2, 0.5]\).</li> <li>the value of \(v_{t+1}\) is clipped so that it stays in the closed interval \([-0.7, 0.7]\).</li> <li> <p>if \(x_{t+1}\) reaches the left or right bound (\(x_{t+1} = -1.2\) or \(x_{t+1} = 0.5\)), then the car’s velocity is reset to zero (\(v_{t+1} = 0\)).</p> </li> <li><strong>Initial State</strong>: \(S_0 = (X_0, 0)\), where \(X_0\) is an initial position drawn uniformly at random from the interval \([-0.6, -0.4]\).</li> <li><strong>Terminal States</strong>: If \(x_t = 0.5\), then the state is terminal (it always transitions to \(s_\infty\)).</li> <li><strong>Rewards</strong>: \(R_t = -1\) always, except when transitioning to \(s_\infty\) (from \(s_\infty\) or from a terminal state), in which case \(R_t = 0\).</li> <li><strong>Discount</strong>: \(\gamma = 1.0\).</li> </ul> <hr/> <h3 id="additional-terminology-notation-and-assumptions">Additional Terminology, Notation, and Assumptions</h3> <ul> <li>A <em>history</em>, \(H_t\), is a recording of what has happened up to time \(t\) in an episode:</li> </ul> \[H_t := (S_0, A_0, R_0, S_1, A_1, R_1, \ldots, S_t, A_t, R_t)\] <ul> <li>A <em>trajectory</em> is the history of an entire episode: \(H_\infty\)</li> <li>The <em>return</em> or <em>discounted return</em> of a trajectory is the discounted sum of rewards \(G := \sum_{t = 0}^{\infty} \gamma^t R_t\)</li> <li>The <em>expected return</em> or <em>expected discounted return</em> can be written as \(J(\pi) := \mathbf{E}[G\vert\pi]\)</li> <li>The <em>return from time</em> \(t\) or <em>discounted return from time</em> \(t\), \(G_t\), is the discounted sum of rewards starting from time \(t\)</li> </ul> \[G_t := \sum_{k=1}^{\infty} \gamma^k R_{t+k}\] <ul> <li>The <em>horizon</em>, \(L\), of an MDP is the smallest integer such that \(\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\) <ul> <li>if \(L &lt; \infty\) for all policies, then we say that the MDP is <em>finite horizon</em></li> <li>if \(L = \infty\) then the domain may be <em>indefinite horizon</em> (the agent will always enter \(s_\infty\)) or <em>infinite horizon</em> (the agent may never enter \(s_\infty\))</li> </ul> </li> </ul> <hr/> <h3 id="markov-property">Markov Property</h3> <div class="callout"> <p><strong>Markov Property (<em>Markov Assumption</em>)</strong></p> <p>In short: <strong><em>given the present, the future does not depend on the past</em></strong>.</p> <p>Formally, \(S_{t+1}\) is conditionally independent of \(H_{t-1}\) given \(S_t\). That is, for all \(h, s, a, s^\prime, t\):</p> <p>\(\text{Pr}(S_{t+1} = s^\prime | H_{t-1} = h, S_{t}=s, A_{t}=a) = \text{Pr}(S_{t+1}=s^\prime | S_{t}=s, A_{t}=a)\)</p> </div> <p>If a model (environment, reward …) holds the Markov assumption, we say it has the Markov property, or say the model is <strong><em>Markovian</em></strong>.</p> <hr/> <h2 id="why-use-mdp-in-rl">Why use MDP in RL?</h2> <p>MDP is not only powerful enough to model the interaction between a learning agent and its environment, it also brings some critical guarantees that make our “reinforcement learning” actually work.</p> <p>For now, let’s skip the derivation and jump straight to the conclusions.</p> <div class="callout"> <p><strong>Existence of an Optimal Policy</strong></p> <p>For all MDPs where \(|\mathcal{S}| &lt; \infty\), \(|\mathcal{A}| &lt; \infty\), \(R_\text{max} &lt; \infty\), and \(\gamma &lt; 1\), there exists at least one optimal policy, \(\pi^*\).</p> </div> <p>Later when we introduce the <em>Bellman equation</em> and the <em>Bellman optimality equation</em>, we will further establish that:</p> <ol> <li>if a policy \(\pi\) achieves a state where its expected future rewards cannot be improved further by any other action or decision at each step (<em>Bellman optimality equation</em>), then it is an optimal policy.</li> <li>if there are only a finite number of possible states and actions, rewards are bounded, and future rewards are discounted (with a discount factor \(\gamma &lt; 1\)), then there exists a policy \(\pi\) that satisfies the Bellman optimality equation.</li> </ol> <p>Furthermore, we can perform policy/value iteration (will be covered later) using the Bellman and Bellman optimality equation. As a result, we can not only get better policies iteration after iteration, but also, under some constraints, prove that the final policy will converge to the optimal policy.</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[An introduction to Reinforcement Learning concepts, including Markov Decision Processes (MDPs).]]></summary></entry><entry><title type="html">强化学习 - 数学基础</title><link href="https://m0gician.github.io/blog/2025/math-foundations-for-RL-zh/" rel="alternate" type="text/html" title="强化学习 - 数学基础"/><published>2025-07-04T19:12:00+00:00</published><updated>2025-07-04T19:12:00+00:00</updated><id>https://m0gician.github.io/blog/2025/math-foundations-for-RL-zh</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/math-foundations-for-RL-zh/"><![CDATA[<h2 id="mdp马尔可夫决策过程">MDP（马尔可夫决策过程）</h2> <p>我们通常把一个 MDP 定义为一个元组 \((\mathcal{S}, \mathcal{A}, p, R, \gamma)\)。</p> <div class="callout"> <p><strong>状态集合（\(\mathcal{S}\)）：</strong> 环境所有可能状态的集合。</p> <ul> <li>时刻 \(t\) 的状态 \(S_t\) 总是取值于 \(\mathcal{S}\)。</li> </ul> </div> <div class="callout"> <p><strong>动作集合（\(\mathcal{A}\)）：</strong> 智能体可以采取的所有可能动作的集合。</p> <ul> <li>时刻 \(t\) 的动作 \(A_t\) 总是取值于 \(\mathcal{A}\)。</li> </ul> </div> <div class="callout"> <p><strong>转移函数（\(p\)）：</strong> 描述环境状态如何变化。</p> \[p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]\] <p>对于所有 \(s \in \mathcal{S}\)、\(a \in \mathcal{A}\)、\(s' \in \mathcal{S}\) 以及 \(t \in \mathbb{N}_{\ge 0}\)：</p> \[p(s,a,s') := \Pr(S_{t+1}=s' \mid S_t=s, A_t=a)\] <p>当 \(p(s,a,s') \in \{0,1\}\) 对所有的 \(s,a,s'\) 成立时，转移函数是确定性的。</p> </div> <div class="callout"> <p>\(d_R\) 描述奖励的生成方式。</p> \[R_t \sim d_R(S_t, A_t, S_{t+1})\] </div> <div class="callout"> <p><strong>奖励函数（\(R\)）：</strong> 由奖励分布 \(d_R\) 隐式定义的函数，描述奖励如何生成。</p> \[R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\] \[R(s,a) := \mathrm{E}[R_t \mid S_t = s, A_t = a]\] </div> <div class="callout"> <p><strong>初始状态分布（\(d_0\)）：</strong></p> \[d_0: \mathcal{S} \rightarrow [0,1]\] \[d_0(s) = \Pr(S_0 = s)\] </div> <div class="callout"> <p><strong>折扣因子（\(\gamma\)）：</strong> 取值范围 \([0,1]\)，用于折扣未来奖励。</p> </div> <hr/> <h3 id="目标">目标</h3> <p>我们的目标是找到一条最优策略 \(\pi^*\)，使得期望累计折扣奖励最大化。</p> <ul> <li>\(G^i\) 表示第 \(i\) 个回合的回报（return）。</li> <li>\(R_t^i\) 表示第 \(i\) 个回合时刻 \(t\) 的奖励。</li> </ul> <div class="callout"> <p><strong>目标函数（\(J\)）：</strong></p> \[J : \Pi \rightarrow \mathbb{R}, \quad \text{对于所有 }\pi \in \Pi\] \[\begin{aligned} J(\pi) &amp;:= \mathrm{E}\Bigg[\sum_{t=1}^{\infty} \gamma^t R_t \,\Big|\, \pi\Bigg] \\[0.2cm] \hat{J}(\pi) &amp;:= \frac{1}{N}\sum_{i=1}^{N} G^i = \frac{1}{N}\sum_{i=1}^{N}\sum_{t=0}^{\infty}\gamma^t R_t^i \end{aligned}\] </div> <div class="callout"> <p><strong>最优策略（\(\pi^*\)）：</strong></p> \[\pi^* \in \arg\max_{\pi \in \Pi} J(\pi)\] </div> <details> <summary>当最优策略存在时它总是唯一的吗？</summary> 不一定，可能存在多条同样优秀的最优策略。 </details> <hr/> <h3 id="性质">性质</h3> <div class="callout"> <p><strong>时限（Horizon，\(L\)）：</strong> 最小的整数 \(L\)，使得对所有 \(t \ge L\)，处于终止状态 \(s_\infty\) 的概率为 1。</p> \[\forall t \ge L,\; \Pr(S_t = s_\infty) = 1\] <ul> <li>若 \(L &lt; \infty\)（对所有策略均成立），则 MDP 为 <strong>有限时限</strong>（回合式）。</li> <li>若 \(L = \infty\)，则 MDP 为 <strong>无限时限</strong>（连续式）。</li> </ul> </div> <div class="callout"> <p><strong>马尔可夫性（Markov Property）：</strong> 一种关于状态表示的性质，假设在给定当前状态的条件下，未来与过去独立。</p> <ul> <li>给定当前状态 \(S_t\)，\(S_{t+1}\) 与历史 \(H_{t-1}\) 条件独立。</li> </ul> </div> <hr/> <h2 id="策略policy">策略（Policy）</h2> <div class="callout"> <p><strong>策略</strong> 是一种决策规则——智能体选择动作的方式。</p> \[\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]\] \[\pi(s,a) := \Pr(A_t = a \mid S_t = s)\] </div> <hr/> <h2 id="价值函数value-functions">价值函数（Value Functions）</h2> <div class="callout"> <p><strong>状态价值函数（\(v^\pi\)）</strong></p> <p>状态价值函数 \(v^\pi : \mathcal{S} \rightarrow \mathbb{R}\) 衡量从状态 \(s\) 出发并遵循策略 \(\pi\) 时的期望回报。</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\Bigg[\sum_{k=1}^{\infty}\gamma^k R_{t+k} \,\Big|\, S_t = s, \pi\Bigg] \\[0.2cm] &amp;:= \mathbf{E}[G_t \mid S_t = s, \pi] \end{aligned}\] </div> <div class="callout"> <p><strong>动作价值函数（Q 函数，\(q^\pi\)）</strong></p> <p>动作价值函数 \(q^\pi : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) 衡量在状态 \(s\) 采取动作 \(a\) 后再遵循策略 \(\pi\) 所得到的期望回报。</p> \[q^\pi(s,a) := \mathbf{E}[G_t \mid S_t = s, A_t = a, \pi]\] </div> <hr/> <h3 id="bellman-方程">Bellman 方程</h3> <div class="callout"> <p><strong>状态价值函数的 Bellman 方程（\(v^\pi\)）</strong></p> \[\begin{aligned} v^\pi(s) &amp;= \mathbf{E}\Big[R(s,A_t) + \gamma v^\pi(S_{t+1}) \,\Big|\, S_t = s, \pi\Big] \\[0.3cm] &amp;= \sum_{a \in \mathcal{A}} \pi(s,a) \sum_{s' \in \mathcal{S}} p(s,a,s')\big(R(s,a) + \gamma v^\pi(s')\big) \end{aligned}\] <ul> <li>Bellman 方程只需向前看一步。</li> <li>最优状态价值函数 \(v^*\) 是唯一的——所有最优策略共享同一 \(v^*\)。</li> </ul> </div> <div class="callout"> <p><strong>动作价值函数的 Bellman 方程（\(q^\pi\)）</strong></p> \[q^\pi(s,a) = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} p(s,a,s') \sum_{a' \in \mathcal{A}} \pi(s',a') q^\pi(s',a')\] <ul> <li>最优动作价值函数 \(q^*\) 对所有最优策略也是唯一的。</li> </ul> </div> <h3 id="bellman-最优方程">Bellman 最优方程</h3> <ol> <li>若一条策略 \(\pi\) 满足 Bellman 最优方程，则 \(\pi\) 是最优策略。</li> <li>若状态、动作集合有限，奖励有界且 \(\gamma &lt; 1\)，那么存在满足 Bellman 最优方程的策略 \(\pi\)。</li> </ol> <div class="callout"> <p><strong>\(v^*\) 的 Bellman 最优方程</strong></p> <p>对所有状态 \(s \in \mathcal{S}\)：</p> \[v^\pi(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} p(s,a,s') \big[R(s,a) + \gamma v^\pi(s')\big]\] </div> <div class="callout"> <p><strong>\(q^*\) 的 Bellman 最优方程</strong></p> \[q^*(s,a) = \sum_{s' \in \mathcal{S}} p(s,a,s') \big[R(s,a) + \gamma \max_{a' \in \mathcal{A}} q^*(s',a')\big]\] </div> <hr/> <h2 id="策略迭代policy-iteration">策略迭代（Policy Iteration）</h2> <p>策略迭代通过交替执行两步——策略评估与策略改进——来寻找最优策略。</p> <ul> <li>通过动态规划进行的策略评估虽然保证收敛到 \(v^\pi\)，但并不保证在有限计算内就能到达。</li> </ul> <div class="callout"> <p><strong>策略改进定理</strong></p> <p>对于任意策略 \(\pi\)，若存在确定性策略 \(\pi'\) 使得 \(\forall s \in \mathcal{S}\)：</p> \[q^\pi(s, \pi'(s)) \ge v^\pi(s)\] <p>则有 \(\pi' \ge \pi\)。</p> </div> <div class="callout"> <p><strong>随机策略的策略改进定理</strong></p> <p>对于任意策略 \(\pi\)，若 \(\pi'\) 满足：</p> \[\sum_{a \in \mathcal{A}} \pi'(s,a) q^\pi(s,a) \ge v^\pi(s),\] <p>则 \(\forall s \in \mathcal{S}\)，都有 \(\pi' \ge \pi\)。</p> </div> <hr/> <h2 id="价值迭代value-iteration">价值迭代（Value Iteration）</h2> <p>价值迭代通过迭代应用 Bellman 最优更新来寻找最优状态价值函数。</p> <div class="callout"> <p><strong>Banach 不动点定理</strong></p> <p>若映射 \(f\) 在非空完备赋范向量空间上是收缩映射，则存在唯一不动点 \(x^*\)，且以任意 \(x_0\) 为初始点、按照 \(x_{k+1} = f(x_k)\) 生成的序列收敛到 \(x^*\)。</p> </div> <div class="callout"> <p><strong>Bellman 算子是收缩映射</strong></p> <p>当 \(\gamma &lt; 1\) 时，在度量 \(d(v,v') := \max_{s \in \mathcal{S}} |v(s)-v'(s)|\) 下，Bellman 算子在 \(\mathbb{R}^{|\mathcal{S}|}\) 上是收缩映射。</p> </div> <ul> <li>对于有限状态动作集、有界奖励且 \(\gamma &lt; 1\) 的 MDP，价值迭代 <strong>收敛</strong> 到唯一的固定点 \(v^\infty\)。</li> <li>这类 MDP <strong>至少存在</strong> 一条最优策略。</li> </ul> <hr/> <h2 id="大数定律law-of-large-numbers">大数定律（Law of Large Numbers）</h2> <div class="callout"> <p><strong>辛钦强大数定律（Khintchine’s Strong Law of Large Numbers）</strong></p> <p>设 \(\{X_i\}_{i=1}^{\infty}\) 为 <strong>独立同分布（i.i.d.）随机变量</strong>。则样本平均序列 \((\frac{1}{n} \sum_{i=1}^{n} X_i)_{n=1}^\infty\) <strong>几乎必然</strong> 收敛到期望 \(\mathbf{E}[X_1]\)。</p> <p>即 \(\displaystyle \frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{a.s.} \mathbf{E}[X_1]\)</p> </div> <div class="callout"> <p><strong>Kolmogorov 强大数定律</strong></p> <p>设 \(\{X_i\}_{i=1}^{\infty}\) 为 <strong>独立（不要求同分布）随机变量</strong>。若所有 \(X_i\) 具有 <strong>相同均值且方差有界</strong>，则样本平均序列 \((\frac{1}{n}\sum_{i=1}^{n} X_i)^\infty_{n=1}\) 亦几乎必然收敛到 \(\mathbf{E}[X_1]\)。</p> </div>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[超大一坨数学公式]]></summary></entry><entry><title type="html">Reinforcement Learning - Mathematical Foundations</title><link href="https://m0gician.github.io/blog/2025/math-foundations-for-RL/" rel="alternate" type="text/html" title="Reinforcement Learning - Mathematical Foundations"/><published>2025-07-04T19:12:00+00:00</published><updated>2025-07-04T19:12:00+00:00</updated><id>https://m0gician.github.io/blog/2025/math-foundations-for-RL</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/math-foundations-for-RL/"><![CDATA[<h2 id="mdp-markov-decision-process">MDP (Markov Decision Process)</h2> <p>We usually define an MDP as a tuple \((\mathcal{S}, \mathcal{A}, p, R, \gamma)\).</p> <div class="callout"> <p><strong>State Set (\(\mathcal{S}\)):</strong> The set of all possible states of the environment.</p> <ul> <li>The state at time \(t\), \(S_t\), always takes values in \(\mathcal{S}\).</li> </ul> </div> <div class="callout"> <p><strong>Action Set (\(\mathcal{A}\)):</strong> The set of all possible actions the agent can take.</p> <ul> <li>The action at time \(t\), \(A_t\), always takes values in \(\mathcal{A}\).</li> </ul> </div> <div class="callout"> <p><strong>Transition Function (\(p\)):</strong> Describes how the state of the environment changes.</p> \[p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]\] <p>For all \(s \in \mathcal{S}\), \(a \in \mathcal{A}\), \(s’ \in \mathcal{S}\), and \(t \in \mathbb{N}_{\geq 0}\):</p> \[p(s,a,s') := \text{Pr}(S_{t+1}=s' | S_t=s, A_t=a)\] <p>A transition function is deterministic if \(p(s,a,s’) \in \{0,1\}\) for all s, a, and s’</p> </div> <div class="callout"> <p>\(d_R\) describes how rewards are generated.</p> \[R_t \sim d_r(S_t, A_t, S_{t+1})\] </div> <div class="callout"> <p><strong>Reward Function (\(R\)):</strong> A function implicitly defined by the reward distribution \(d_R\), which describes how rewards are generated.</p> \[R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\] \[R(s,a) := \mathrm{R}[R_t|S_t = s, A_t = a]\] </div> <div class="callout"> <p><strong>Initial State Distribution (\(d_0\)):</strong></p> \[d_0: \mathcal{S} \rightarrow [0,1]\] \[d_0(s) = \text{Pr}(S_0=s)\] </div> <div class="callout"> <p><strong>Discount Factor (\(\gamma\)):</strong> A parameter in \([0,1]\) that discounts future rewards.</p> </div> <hr/> <h3 id="objective">Objective</h3> <p>The goal is to find an optimal policy \(\pi^*\) that maximizes the expected total amount of discounted reward.</p> <ul> <li>\(G^i\) denotes the return of the i-th episode.</li> <li>\(R^i_t\) denotes the reward at time \(t\) during episode \(i\).</li> </ul> <div class="callout"> <p><strong>Objective function (\(J\)):</strong></p> \[J : \Pi \rightarrow \mathbb{R}, \text{where for all } \pi \in \Pi\] \[\begin{aligned} &amp;J(\pi) := \mathrm{E}\left[\sum_{t=1}^{\infty} \gamma^tR_t \bigg| \pi\right] \\ &amp;\hat{J}(\pi) := \frac{1}{N}\sum_{i=1}^{N}G^i = \frac{1}{n}\sum_{i=1}^{N}\sum_{t=0}^{\infty}\gamma^t R_t^i \end{aligned}\] </div> <div class="callout"> <p><strong>Optimal Policy (\(\pi^*\)):</strong></p> \[\pi^* \in \underset{\pi \in \Pi}{\text{argmax}}\,J(\pi)\] </div> <details> <summary>Is the optimal policy always unique when it exists?</summary> No. There can exist multiple optimal policies that are equally good. </details> <hr/> <h3 id="properties">Properties</h3> <div class="callout"> <p><strong>Horizon (\(L\)):</strong> The smallest integer \(L\) such that for all \(t \geq L\), the probability of being in a terminal state \(s_\infty\) is 1.</p> \[\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\] <ul> <li>The MDP is <strong>finite horizon</strong> (episodic) if \(L &lt; \infty\) for all policies.</li> <li>The MDP is <strong>infinite horizon</strong> (continuous) when \(L = \infty\).</li> </ul> </div> <div class="callout"> <p><strong>Markov Property:</strong> A property of the state representation. It assumes that the future is independent of the past given the present.</p> <ul> <li>\(S_{t+1}\) is conditionally independent of the history \(H_{t-1}\) given the current state \(S_t\).</li> </ul> </div> <hr/> <h2 id="policy">Policy</h2> <div class="callout"> <p>A <strong>policy</strong> is a decision rule—a way that the agent can select actions.</p> \[\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]\] \[\pi(s,a) := \text{Pr}(A_t=a | S_t=s)\] </div> <hr/> <h2 id="value-functions">Value Functions</h2> <div class="callout"> <p><strong>State-Value Function (\(v^\pi\))</strong></p> <p>The state-value function \(v^\pi : \mathcal{S} \rightarrow \mathbb{R}\) measures the expected return starting from a state \(s\) and following policy \(\pi\).</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\left[\sum_{k=1}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, \pi\right] \\ &amp;:= \mathbf{E}[G_t|S_t=s, \pi] \end{aligned}\] </div> <div class="callout"> <p><strong>Action-Value Function (Q-function, \(q^\pi\))</strong></p> <p>The action-value function \(q^\pi : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) measures the expected return starting from state \(s\), taking action \(a\), and then following policy \(\pi\).</p> \[q^\pi(s,a) := \mathbf{E}[G_t | S_t=s, A_t=a, \pi]\] </div> <hr/> <h3 id="bellman-equations">Bellman Equations</h3> <div class="callout"> <p><strong>Bellman Equation for the State-Value Function (\(v^\pi\))</strong></p> \[\begin{aligned} v^\pi(s) &amp;= \mathbf{E}\left[\underbrace{R(s,A_t)}_{\text{immediate reward}} + \gamma \underbrace{v^\pi(S_{t+1}) }_{\text{value of next state}} \bigg| S_t = s, \pi\right] \\[0.3cm] &amp;= \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s')(R(s,a) + \gamma v^\pi(s')) \end{aligned}\] <ul> <li>The Bellman equation only needs to look forward one time step into the future.</li> <li>The optimal state-value function, \(v^*\), is unique—all optimal policies share the same state-value function.</li> </ul> </div> <div class="callout"> <p><strong>Bellman Equation for the Action-Value Function (\(q^\pi\))</strong></p> \[q^\pi(s,a) = R(s,a) + \gamma\sum_{s' \in \mathcal{S}}p(s,a,s')\sum_{a' \in \mathcal{A}}\pi(s',a')q^\pi(s',a')\] <ul> <li>The optimal action-value function, \(q^*\), is also unique among all optimal policies.</li> </ul> </div> <h3 id="bellman-optimality-equations">Bellman Optimality Equations</h3> <ol> <li>If a policy \(\pi\) satisfies the Bellman optimality equation, then \(\pi\) is an optimal policy.</li> <li>If the state and action sets are finite, rewards are bounded, and \(\gamma &lt; 1\), then there exists a policy \(\pi\) that satisfies the Bellman optimality equation.</li> </ol> <div class="callout"> <p><strong>Bellman Optimality Equation for \(v^*\)</strong></p> <p>A policy \(\pi\) satisfies the Bellman optimality equation if for all states \(s\in\mathcal{S}\):</p> \[v^\pi(s) = \max_{a\in\mathcal{A}}\sum_{s'\in\mathcal{S}}p(s,a,s')[R(s,a)+\gamma v^\pi(s')]\] </div> <div class="callout"> <p><strong>Bellman Optimality Equation for \(q^*\)</strong></p> \[q^*(s,a) = \sum_{s'\in\mathcal{S}}p(s,a,s')\left[R(s,a) + \gamma\max_{a'\in\mathcal{A}}q^*(s',a')\right]\] </div> <hr/> <h2 id="policy-iteration">Policy Iteration</h2> <p>Policy iteration is an algorithm that finds an optimal policy by alternating between two steps: policy evaluation and policy improvement.</p> <ul> <li>Even though policy evaluation using dynamic programming is guaranteed to converge to \(v^\pi\), it is not guaranteed to reach \(v^\pi\) in a finite amount of computation.</li> </ul> <div class="callout"> <p><strong>Policy Improvement Theorem</strong></p> <p>For any policy \(\pi\), if \(\pi’\) is a deterministic policy such that \(\forall s \in \mathcal{S}\):</p> \[q^\pi(s, \pi'(s)) \geq v^\pi(s)\] <p>then \(\pi’ \geq \pi\).</p> </div> <div class="callout"> <p><strong>Policy Improvement Theorem for Stochastic Policies</strong></p> <p>For any policy \(\pi\), if \(\pi’\) satisfies:</p> \[\sum_{a\in\mathcal{A}}\pi'(s,a) q^\pi(s,a) \geq v^\pi(s),\] <p>for all \(s \in \mathcal{S}\), then \(\pi' \geq \pi\).</p> </div> <hr/> <h2 id="value-iteration">Value Iteration</h2> <p>Value iteration is an algorithm that finds the optimal state-value function by iteratively applying the Bellman optimality update.</p> <div class="callout"> <p><strong>Banach Fixed-Point Theorem</strong></p> <p>If \(f\) is a contraction mapping on a non-empty complete normed vector space, then \(f\) has a unique fixed point, \(x^*\), and the sequence defined by \(x_{k+1} = f(x_k)\), with \(x_0\) chosen arbitrarily, converges to \(x^*\).</p> </div> <div class="callout"> <p><strong>Bellman Operator is a Contraction Mapping</strong></p> <p>The Bellman operator is a contraction mapping on \(\mathbb{R}^{\vert\mathcal{S}\vert}\) with distance metric \(d(v,v’) := \max_{s\in\mathcal{S}}\vert v(s)-v’(s) \vert\) if \(\gamma &lt; 1\).</p> </div> <ul> <li>Value iteration <strong>converges</strong> to a unique fixed point \(v^\infty\) for all MDPs with finite state and action sets, bounded rewards, and \(\gamma &lt; 1\).</li> <li>All MDPs with finite state and action sets, bounded rewards, and \(\gamma &lt; 1\) <strong>have at least one optimal policy</strong>.</li> </ul> <hr/> <h2 id="law-of-large-numbers">Law of Large Numbers</h2> <div class="callout"> <p><strong>Khintchine’s Strong Law of Large Numbers</strong></p> <p>Let \(\{X_i\}_{i=1}^{\infty}\) be <strong>independent and identically distributed (i.i.d.) random variables</strong>. Then the sequence of sample averages \((\frac{1}{n} \sum_{i=1}^{n} X_i)_{n=1}^\infty\) converges <strong>almost surely</strong> to the expected value \(\mathbf{E}[X_1]\).</p> <p>i.e., \(\displaystyle \frac{1}{n}\sum_{i=1}^{\infty} X_i \overset{a.s.}{\rightarrow}\mathbf{E}[X_1]\)</p> </div> <div class="callout"> <p><strong>Kolmogorov’s Strong Law of Large Numbers</strong></p> <p>Let \(\{X_i\}^\infty_{i=1}\) be <strong>independent (not necessarily identically distributed) random variables</strong>. If all \(X_i\) have the <strong>same mean and bounded variance</strong>, then the sequence of sample averages \((\frac{1}{n}\sum_{i=1}^n X_i)^\infty_{n=1}\) converges almost surely to \(\mathbf{E}[X_1]\).</p> </div>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[A very, very, very large chunk of math]]></summary></entry></feed>