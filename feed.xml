<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://m0gician.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://m0gician.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-03T01:57:58+00:00</updated><id>https://m0gician.github.io/feed.xml</id><title type="html">blank</title><subtitle>I break everything language model related. Training, inference, optimization, and deployment. </subtitle><entry><title type="html">å¼ºåŒ–å­¦ä¹  â€” ä»·å€¼å‡½æ•°</title><link href="https://m0gician.github.io/blog/2025/value-functions-zh/" rel="alternate" type="text/html" title="å¼ºåŒ–å­¦ä¹  â€” ä»·å€¼å‡½æ•°"/><published>2025-08-02T07:00:00+00:00</published><updated>2025-08-02T07:00:00+00:00</updated><id>https://m0gician.github.io/blog/2025/value-functions-zh</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/value-functions-zh/"><![CDATA[<p>åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å¦‚ä½•èå…¥å¼ºåŒ–å­¦ä¹ ã€‚æœ¬ç« å°†åŸºäºÂ MDPÂ çš„å®šä¹‰ï¼Œå±•ç¤ºå¦‚ä½•ä»æ•°å­¦è§’åº¦è¯„ä¼°Â RLÂ æ™ºèƒ½ä½“ã€‚</p> <h2 id="çŠ¶æ€ä»·å€¼å‡½æ•°">çŠ¶æ€ä»·å€¼å‡½æ•°</h2> <p><em>çŠ¶æ€ä»·å€¼å‡½æ•°</em>Â \(v^\pi(s)\)Â è¡¨ç¤ºå½“æ™ºèƒ½ä½“ä»çŠ¶æ€Â \(s\)Â å‡ºå‘å¹¶æŒ‰ç…§ç­–ç•¥Â \(\pi\)Â è¡ŒåŠ¨æ—¶ï¼Œå…¶æœŸæœ›æŠ˜æ‰£å›æŠ¥ã€‚é€šä¿—åœ°è¯´ï¼Œå®ƒè¡¡é‡åœ¨é‡‡ç”¨ç­–ç•¥Â \(\pi\)Â æ—¶èº«å¤„çŠ¶æ€Â \(s\)Â â€œæœ‰å¤šå¥½â€ã€‚æˆ‘ä»¬ç§°Â \(v^\pi(s)\)Â ä¸ºçŠ¶æ€Â \(s\)Â çš„ä»·å€¼ã€‚</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\ \Bigg[\underbrace{\sum_{k=0}^{\infty}\gamma^k R_{t+k}}_{G_t} \bigg| S_t=s, \pi\ \Bigg] \\ &amp;:= \mathbf{E}[G_t|S_t=s, \pi] \\ &amp;:= \mathbf{E}\ \Bigg[\sum_{t=0}^{\infty}\gamma^k R_{t} \bigg| S_0=s, \pi\ \Bigg] \end{aligned}\] <p>å›é¡¾æˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­ä½¿ç”¨çš„Â \(G_t\)ï¼ˆ<em>ä»æ—¶é—´æ­¥Â \(t\)Â å¼€å§‹çš„æŠ˜æ‰£å›æŠ¥</em>ï¼‰è®°å·ï¼Œå¯ä»¥å‘ç°è¿™æ­£æ˜¯å…¶ç­‰ä»·å½¢å¼ã€‚</p> <h3 id="ä¸€ä¸ªç®€å•çš„mdpç¤ºä¾‹">ä¸€ä¸ªç®€å•çš„Â MDPÂ ç¤ºä¾‹</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mdp-simple-480.webp 480w,/assets/img/rl/mdp-simple-800.webp 800w,/assets/img/rl/mdp-simple-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mdp-simple.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Simple MDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p>åœ¨ä¸Šå›¾æ‰€ç¤ºçš„Â MDPÂ ä¸­ï¼Œæ™ºèƒ½ä½“æ¯æ¬¡å¯é€‰æ‹©ä¸¤ä¸ªåŠ¨ä½œä¸­çš„ä¸€ä¸ªï¼š<code class="language-plaintext highlighter-rouge">Left</code>Â æˆ–Â <code class="language-plaintext highlighter-rouge">Right</code>ã€‚åœ¨çŠ¶æ€Â \(s_1\)Â ä¸Â \(s_6\)Â ä¸­ï¼Œæ— è®ºé‡‡å–ä½•ç§åŠ¨ä½œéƒ½ä¼šç›´æ¥è½¬ç§»è‡³ç»ˆæ­¢çŠ¶æ€Â \(s_\infty\)ã€‚åªæœ‰åœ¨å‘ç”ŸÂ \(s_2 \to s_1\)Â æˆ–Â \(s_5 \to s_6\)Â çš„è½¬ç§»æ—¶ï¼Œæ™ºèƒ½ä½“æ‰èƒ½è·å¾—å¥–åŠ±ã€‚ä¸ºç®€åŒ–è®¡ç®—ï¼Œè®¾æŠ˜æ‰£å› å­Â \(\gamma = 0.5\)ã€‚</p> <p>æˆ‘ä»¬ä¸ºè¯¥Â MDPÂ å°è¯•ä¸¤ç§ç­–ç•¥ï¼šç­–ç•¥Â \(\pi_1\)Â å§‹ç»ˆé€‰æ‹©Â <code class="language-plaintext highlighter-rouge">Left</code>ï¼›ç­–ç•¥Â \(\pi_2\)Â å§‹ç»ˆé€‰æ‹©Â <code class="language-plaintext highlighter-rouge">Right</code>ã€‚</p> <p><strong>ç­–ç•¥Â 1ï¼ˆ\(\pi_1\)ï¼‰ï¼šå§‹ç»ˆé€‰æ‹©Â <code class="language-plaintext highlighter-rouge">Left</code></strong></p> <ul> <li>\(v^{\pi_1}(s_1) = 0\)Â ï¼ˆå§‹ç»ˆç›´æ¥è¿›å…¥ç»ˆæ­¢çŠ¶æ€ï¼‰</li> <li>\(v^{\pi_1}(s_2) = 12\gamma^0 = 12\).</li> <li>\(v^{\pi_1}(s_3) = 0\gamma^0 + 12\gamma^1 = 6\).</li> <li>\(v^{\pi_1}(s_4) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(v^{\pi_1}(s_5) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(v^{\pi_1}(s_6) = 0\).</li> </ul> <p><strong>ç­–ç•¥Â 2ï¼ˆ\(\pi_2\)ï¼‰ï¼šå§‹ç»ˆé€‰æ‹©Â <code class="language-plaintext highlighter-rouge">Right</code></strong></p> <ul> <li>\(v^{\pi_2}(s_1) = 0\).</li> <li>\(v^{\pi_2}(s_2) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(v^{\pi_2}(s_3) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(v^{\pi_2}(s_4) = 0\gamma^0 + 2\gamma^1 = 1\).</li> <li>\(v^{\pi_2}(s_5) = 2\gamma^0 = 2\).</li> <li>\(v^{\pi_2}(s_6) = 0\).</li> </ul> <h2 id="è¡ŒåŠ¨ä»·å€¼å‡½æ•°">è¡ŒåŠ¨ä»·å€¼å‡½æ•°</h2> <p><em>è¡ŒåŠ¨ä»·å€¼å‡½æ•°</em>Â \(q^\pi(s,a)\)ï¼ˆäº¦ç§°Â <em>Qâ€‘å‡½æ•°</em>ï¼‰è¡¨ç¤ºå½“æ™ºèƒ½ä½“åœ¨çŠ¶æ€Â \(s\)Â é‡‡å–åŠ¨ä½œÂ \(a\)Â å¹¶éšåæŒ‰ç…§ç­–ç•¥Â \(\pi\)Â è¡ŒåŠ¨æ—¶ï¼Œå…¶æœŸæœ›æŠ˜æ‰£å›æŠ¥ã€‚</p> \[\begin{aligned} q^\pi(s,a) &amp;:= \mathbf{E}\ \Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, A_t = a, \pi\ \Bigg] \\ &amp;:= \mathbf{E}[G_t|S_t=s, A_t=a, \pi] \\ &amp;:= \mathbf{E}\ \Bigg[\sum_{t=0}^{\infty}\gamma^k R_{t} \bigg| S_0=s, A_0=a, \pi\ \Bigg] \end{aligned}\] <h3 id="å†çœ‹mdp">ğŸ‘€å†çœ‹MDP</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mdp-simple-480.webp 480w,/assets/img/rl/mdp-simple-800.webp 800w,/assets/img/rl/mdp-simple-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mdp-simple.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Simple MDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p><strong>ç­–ç•¥Â 1ï¼ˆ\(\pi_1\)ï¼‰ï¼šå§‹ç»ˆé€‰æ‹©Â <code class="language-plaintext highlighter-rouge">Left</code></strong></p> <ul> <li>\(q^{\pi_1}(s_1, L) = 0\).</li> <li>\(q^{\pi_1}(s_1, R) = 0\).</li> <li>\(q^{\pi_1}(s_2, L) = 12\gamma^0 =12\).</li> <li>\(q^{\pi_1}(s_2, R) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(q^{\pi_1}(s_3, L) = 0\gamma^0 + 12\gamma^1 = 6\).</li> <li>\(q^{\pi_1}(s_3, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(q^{\pi_1}(s_4, L) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(q^{\pi_1}(s_4, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 0\gamma^3 + 12\gamma^4 = 0.75\).</li> <li>\(q^{\pi_1}(s_5, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(q^{\pi_1}(s_5, R) = 2\gamma^0 = 2\).</li> <li>\(q^{\pi_1}(s_6, L) = 0\).</li> <li>\(q^{\pi_1}(s_6, R) = 0\).</li> </ul> <p><strong>ç­–ç•¥Â 2ï¼ˆ\(\pi_2\)ï¼‰ï¼šå§‹ç»ˆé€‰æ‹©Â <code class="language-plaintext highlighter-rouge">Right</code></strong></p> <ul> <li>\(q^{\pi_2}(s_1, L) = 0\).</li> <li>\(q^{\pi_2}(s_1, R) = 0\).</li> <li>\(q^{\pi_2}(s_2, L) = 12\gamma^0 =12\).</li> <li>\(q^{\pi_2}(s_2, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(q^{\pi_2}(s_3, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 0\gamma^3 + 2\gamma^4 = 0.125\).</li> <li>\(q^{\pi_2}(s_3, R) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(q^{\pi_2}(s_4, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(q^{\pi_2}(s_4, R) = 0\gamma^0 + 2\gamma^1 = 1\).</li> <li>\(q^{\pi_2}(s_5, L) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(q^{\pi_2}(s_5, R) = 2\gamma^0 = 2\).</li> <li>\(q^{\pi_2}(s_6, L) = 0\).</li> <li>\(q^{\pi_2}(s_6, R) = 0\).</li> </ul> <h2 id="vpiçš„è´å°”æ›¼æ–¹ç¨‹">\(v^\pi\)Â çš„è´å°”æ›¼æ–¹ç¨‹</h2> <p><em>çŠ¶æ€ä»·å€¼å‡½æ•°çš„è´å°”æ›¼æ–¹ç¨‹</em>æ˜¯Â \(v^\pi\)Â çš„é€’å½’è¡¨è¾¾å¼ã€‚ä¸ºæ¨å¯¼è¯¥æ–¹ç¨‹ï¼Œæˆ‘ä»¬é¦–å…ˆå°†å³æ—¶å¥–åŠ±ä»ä»·å€¼å‡½æ•°ä¸­åˆ†ç¦»å‡ºæ¥ï¼š</p> \[\begin{aligned} v^\pi(s) &amp;:= \textbf{E}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg\vert S_t=s, \pi\right] \\ &amp;= \textbf{E}\left[R_t + \sum_{k=1}^{\infty}\gamma^k R_{t+k} \bigg\vert S_t=s, \pi\right] \\ &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=1}^{\infty}\gamma^{k-1} R_{t+k} \bigg\vert S_t=s, \pi\right] \end{aligned}\] <p>é€šè¿‡å°†æ±‚å’Œç´¢å¼•è°ƒæ•´ä¸ºä»Â 0Â å¼€å§‹ï¼ˆå³å°†æ‰€æœ‰Â \(k\)Â æ›¿æ¢ä¸ºÂ \(k+1\)ï¼‰ï¼Œå¯å¾—åˆ°</p> \[\begin{aligned} \textbf{E}\left[\gamma\sum_{k=1}^{\infty}\gamma^{k-1} R_{t+k} \bigg\vert S_t=s, \pi\right] = \textbf{E}\left[\gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \end{aligned}\] <p>æ ¹æ®å…¨æ¦‚ç‡å…¬å¼Â \(\textbf{E}[X] = \textbf{E}[\textbf{E}[X \vert Y]]\)ï¼Œä»¤é¦–æ¬¡åŠ¨ä½œÂ \(A_t=a\)Â åŠä¸‹ä¸€çŠ¶æ€Â \(S_{t+1}=s'\)Â ä½œä¸ºæ¡ä»¶å˜é‡ï¼Œå¯å¾—</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ = &amp;\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s') \, \textbf{E}\left[\gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t=a, S_{t+1}=s', \pi\right] \end{aligned}\] <p>åˆ©ç”¨é©¬å°”å¯å¤«æ€§è´¨ï¼Œå¯å°†æ¡ä»¶ä¸­çš„Â \(S_t\)Â å’ŒÂ \(A_t\)Â å»æ‰ï¼š</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, \pi\right] \\ = &amp;\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s') \, \gamma\textbf{E}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s', \pi\right] \end{aligned}\] <p>æ ¹æ®çŠ¶æ€ä»·å€¼å‡½æ•°å®šä¹‰ï¼Œæœ€åä¸€é¡¹æ­£æ˜¯Â \(v^\pi(s')\)ï¼š</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ = &amp; \gamma\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) v^\pi(s^\prime) \end{aligned}\] <p>ç”±äºå¯¹ä»»æ„ç»™å®šÂ \(s\)ã€\(a\)ï¼Œè½¬ç§»åˆ°ä¸‹ä¸€çŠ¶æ€Â \(s'\)Â çš„æ¦‚ç‡ä¹‹å’Œä¸ºÂ 1ï¼Œå¯å°†å³æ—¶å¥–åŠ±å†™æˆ</p> \[\begin{aligned} R_t = \sum_{a\in\mathcal{A}}\pi(s,a)R(s,a) = \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)R(s,a) \end{aligned}\] <p>ç»¼åˆå¯å¾—</p> \[\begin{aligned} v^\pi(s) &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ &amp;= \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)R(s,a) + \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) \gamma v^\pi(s^\prime) \end{aligned}\] <p>æœ€ç»ˆï¼Œå¯å¾—åˆ°ç®€æ´å½¢å¼</p> \[v^\pi(s) = \boxed{\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\big(R(s,a) +\gamma v^\pi(s^\prime)\big)}\] <h3 id="å…³äºè´å°”æ›¼æ–¹ç¨‹çš„ä¼˜ç‚¹">å…³äºè´å°”æ›¼æ–¹ç¨‹çš„ä¼˜ç‚¹</h3> <div class="callout"> <p>æˆ‘ä»¬å¯ä»¥å°†è´å°”æ›¼æ–¹ç¨‹è§†ä¸ºæŠŠæœŸæœ›å›æŠ¥æ‹†åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š</p> <ol> <li>ä¸‹ä¸€æ—¶é—´æ­¥è·å¾—çš„å¥–åŠ±ï¼ˆ<em>å³æ—¶å¥–åŠ±</em>ï¼‰</li> <li>ä¸‹ä¸€çŠ¶æ€çš„ä»·å€¼</li> </ol> \[v^\pi(s) = \textbf{E}\left[\underbrace{R(s,A_t)}_{\text{å³æ—¶å¥–åŠ±}} + \gamma\underbrace{v^\pi(S_{t+1})}_{\text{ä¸‹ä¸€çŠ¶æ€ä»·å€¼}}\Bigg\vert S_t=s, \pi\right]\] <p>åŸå§‹å®šä¹‰éœ€è¦è€ƒè™‘æ•´æ¡çŠ¶æ€åºåˆ—ï¼Œè€Œè´å°”æ›¼æ–¹ç¨‹<strong>åªéœ€å‘å‰çœ‹ä¸€æ­¥</strong>ã€‚</p> <ul> <li>è´å°”æ›¼æ–¹ç¨‹çš„é€’å½’æ€§è´¨ä½¿å…¶åœ¨è®¡ç®—ä¸Šæ›´æœ‰å¸®åŠ©</li> </ul> </div> <h2 id="qpiçš„è´å°”æ›¼æ–¹ç¨‹">\(q^\pi\)Â çš„è´å°”æ›¼æ–¹ç¨‹</h2> <p>å°±å¦‚Â \(v^\pi\)Â çš„è´å°”æ›¼æ–¹ç¨‹ç»™å‡ºäº†Â \(v^\pi\)Â çš„é€’å½’å…³ç³»ä¸€æ ·ï¼Œ\(q^\pi\)Â çš„è´å°”æ›¼æ–¹ç¨‹åˆ™ç»™å‡ºäº†è¡ŒåŠ¨ä»·å€¼å‡½æ•°Â \(q^\pi\)Â çš„é€’å½’å…³ç³»ï¼š</p> \[\begin{aligned} q^\pi(s,a) &amp;:= \mathbf{E}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, A_t = a, \pi\Bigg] \\ &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t = a, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}} \pi(s^\prime,a^\prime)\times\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t = a, S_{t+1}=s^\prime, A_{t+1}=a^\prime, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime) \times\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, A_{t+1}=a^\prime, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)\gamma q^\pi(s^\prime, s^\prime) \\ &amp;= \boxed{R(s,a) + \gamma\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)q^\pi(s^\prime, a^\prime)} \end{aligned}\] <p>æˆ–ç®€å†™ä¸º</p> \[q^\pi(s,a) = \boxed{R(s,a) + \gamma\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)q^\pi(s^\prime, a^\prime)}\] <h2 id="æœ€ä¼˜ä»·å€¼å‡½æ•°">æœ€ä¼˜ä»·å€¼å‡½æ•°</h2> <div class="callout"> <p><strong>æœ€ä¼˜ç­–ç•¥</strong>Â \(\pi^*\)<br/> è‹¥æŸç­–ç•¥Â \(\pi^*\)Â è‡³å°‘ä¸æ‰€æœ‰å…¶ä»–ç­–ç•¥ä¸€æ ·å¥½ï¼Œåˆ™ç§°å…¶ä¸ºæœ€ä¼˜ç­–ç•¥ã€‚å³</p> \[\forall \pi \in \Pi, \; \pi^* \ge \pi\] </div> <div class="callout"> <p>å³ä¾¿æœ€ä¼˜ç­–ç•¥å¯èƒ½ä¸å”¯ä¸€ï¼Œæœ€ä¼˜ä»·å€¼å‡½æ•°Â \(v^*\)Â ä¸Â \(q^*\)Â å´æ˜¯å”¯ä¸€çš„â€”â€”æ‰€æœ‰æœ€ä¼˜ç­–ç•¥å…±äº«åŒä¸€çŠ¶æ€ä»·å€¼å‡½æ•°ä¸è¡ŒåŠ¨ä»·å€¼å‡½æ•°ã€‚</p> </div> <div class="callout"> <details> <summary>å·²çŸ¥æœ€ä¼˜çŠ¶æ€ä»·å€¼å‡½æ•°ï¼Œè‹¥æœªçŸ¥è½¬ç§»æ¦‚ç‡åŠå¥–åŠ±å‡½æ•°ï¼Œèƒ½å¦æ±‚å¾—æœ€ä¼˜ç­–ç•¥ï¼Ÿ</summary> <br/><strong>ä¸èƒ½</strong>ã€‚ $$ \arg\max_{a\in\mathcal{A}}\sum_{s'}p(s,a,s')\big[R(s,a) + \gamma v^\pi(s')\big] $$ çš„è®¡ç®—ä»ä¾èµ–äºÂ pÂ å’ŒÂ Rã€‚ </details> </div> <div class="callout"> <details> <summary>å·²çŸ¥æœ€ä¼˜è¡ŒåŠ¨ä»·å€¼å‡½æ•°ï¼Œè‹¥æœªçŸ¥è½¬ç§»æ¦‚ç‡åŠå¥–åŠ±å‡½æ•°ï¼Œèƒ½å¦æ±‚å¾—æœ€ä¼˜ç­–ç•¥ï¼Ÿ</summary> <br/><strong>å¯ä»¥</strong>ã€‚ $$ \arg\max_{a\in\mathcal{A}}q^*(s,a) $$ å³ä¸ºçŠ¶æ€Â sÂ ä¸‹çš„æœ€ä¼˜åŠ¨ä½œã€‚ </details> </div> <h3 id="vçš„è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹">\(v^*\)Â çš„è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹</h3> <p>ä»è´å°”æ›¼æ–¹ç¨‹å‡ºå‘ï¼Œ</p> \[v^*(s) = \sum_{a\in\mathcal{A}}\pi^*(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s')\big[R(s,a) + \gamma v^*(s')\big]\] <p>ç”±äºæœ€ä¼˜ç­–ç•¥Â \(\pi^*\)Â ä»…é€‰æ‹©èƒ½æœ€å¤§åŒ–Â \(q^*(s,a)\)Â çš„åŠ¨ä½œï¼Œå¯å†™ä¸º</p> \[v^*(s) = \max_{a\in\mathcal{A}}\sum_{s' \in \mathcal{S}}p(s,a,s')\big[R(s,a) + \gamma v^*(s')\big]\] <p>è¿™å°±æ˜¯Â <em>\(v^*\)Â çš„è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹</em>ã€‚</p> <div class="callout"> <p>è‹¥ä¸€ä¸ªç­–ç•¥Â \(\pi\)Â æ»¡è¶³è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼Œåˆ™å¯¹æ‰€æœ‰çŠ¶æ€Â \(s \in \mathcal{S}\) æœ‰</p> \[v^*(s) = \max_{a\in\mathcal{A}}\sum_{s' \in \mathcal{S}}p(s,a,s')\big[R(s,a) + \gamma v^*(s')\big]\] </div> <h3 id="qçš„è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹">\(q^*\)Â çš„è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹</h3> <div class="callout"> <p>è‹¥ä¸€ä¸ªç­–ç•¥Â \(\pi\)Â æ»¡è¶³è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼Œåˆ™å¯¹æ‰€æœ‰åŠ¨ä½œÂ \(a \in \mathcal{A}\) æœ‰</p> \[q^*(s,a) = \sum_{s' \in \mathcal{S}} p(s,a,s')\left[ R(s,a) + \gamma \max_{a'\in\mathcal{A}}q^*(s', a')\right]\] </div> <h3 id="è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ä¸æœ€ä¼˜ç­–ç•¥">è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ä¸æœ€ä¼˜ç­–ç•¥</h3> <div class="callout"> <p><em>è‹¥ç­–ç•¥Â \(\pi\)Â æ»¡è¶³è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼Œåˆ™Â \(\pi\)Â ä¸ºæœ€ä¼˜ç­–ç•¥ã€‚</em></p> </div> <p><em>è¯æ˜ï¼š</em></p> <p>å‡è®¾ä¸€ä¸ªç­–ç•¥ \(\pi\) æ»¡è¶³è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼Œé‚£ä¹ˆå¯¹äºæ‰€æœ‰çŠ¶æ€ \(s\)ï¼š</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^\pi(s^\prime)]\] <p>æˆ‘ä»¬å¯ä»¥å°†è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹é€’å½’åœ°ä»£å…¥è¡¨è¾¾å¼ä¸­ï¼Œå¹¶æ›¿æ¢ \(v^\pi(s^\prime)\)ï¼š</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma v^\pi(s^{\prime\prime})\right)\right]\] <p>æˆ‘ä»¬å¯ä»¥æ— é™åœ°ç»§ç»­è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ° \(\pi\) ä»è¡¨è¾¾å¼ä¸­å®Œå…¨æ¶ˆå¤±ï¼š</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right]\] <p>åœ¨æ¯ä¸ªæ—¶é—´æ­¥ \(t\)ï¼Œé€‰æ‹©çš„åŠ¨ä½œéƒ½æ˜¯æœ€å¤§åŒ–æœªæ¥æœŸæœ›æŠ˜æ‰£å›æŠ¥çš„åŠ¨ä½œï¼Œå‰ææ˜¯æœªæ¥çš„åŠ¨ä½œä¹Ÿæ˜¯ä¸ºäº†æœ€å¤§åŒ–æœªæ¥æŠ˜æ‰£å›æŠ¥ã€‚</p> <p>ç°åœ¨ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä»»ä½•ä¸€ä¸ªæ–°çš„ç­–ç•¥ \(\pi^\prime\)ã€‚å¦‚æœæˆ‘ä»¬å°† \(\max_{a \in \mathcal{A}}\) æ›¿æ¢ä¸º \(\sum_{a \in \mathcal{A}}\pi^\prime(s,a)\)ï¼Œå…³ç³»ä¼šæ€æ ·ï¼Ÿæˆ‘ä»¬è®¤ä¸ºè¡¨è¾¾å¼çš„å€¼ä¸ä¼šå˜å¾—æ¯”ä¹‹å‰æ›´å¤§ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºä»»ä½•ç­–ç•¥ \(\pi^\prime\)ï¼š</p> \[\begin{aligned} v^\pi(s) &amp;= \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \\ &amp;\geq \sum_{a \in \mathcal{A}}\pi^\prime(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\sum_{a^\prime \in \mathcal{A}}\pi^\prime(s^\prime,a^\prime)\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \end{aligned}\] <p>é‰´äºä¸Šè¿°ä¸ç­‰å¼å¯¹æ‰€æœ‰ç­–ç•¥ \(\pi^\prime\) éƒ½æˆç«‹ï¼Œæˆ‘ä»¬å¾—å‡ºå¯¹äºæ‰€æœ‰çŠ¶æ€ \(s \in \mathcal{S}\) å’Œæ‰€æœ‰ç­–ç•¥ \(\pi^\prime \in \Pi\)ï¼š</p> \[\begin{aligned} v^\pi(s) &amp;= \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \\ &amp;\geq \mathbf{E}[G_t | S_t = s, \pi^\prime] \\ &amp;= v^{\pi^\prime}(s) \end{aligned}\] <p>å› æ­¤ï¼Œå¯¹äºæ‰€æœ‰çŠ¶æ€ \(s \in \mathcal{S}\) å’Œæ‰€æœ‰ç­–ç•¥ \(\pi^\prime \in \Pi\)ï¼Œ\(v^\pi(s) \geq v^{\pi^\prime}(s)\)ã€‚æ¢å¥è¯è¯´ï¼Œå¯¹äºæ‰€æœ‰ç­–ç•¥ \(\pi^\prime \in \Pi\)ï¼Œæˆ‘ä»¬æœ‰ \(\pi \geq \pi^\prime\)ï¼Œå› æ­¤ \(\pi\) æ˜¯ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥ã€‚</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[æ·±å…¥æ¢è®¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ä»·å€¼å‡½æ•°ä¸è´å°”æ›¼æ–¹ç¨‹ã€‚]]></summary></entry><entry><title type="html">Reinforcement Learning â€” Value Functions</title><link href="https://m0gician.github.io/blog/2025/value-functions/" rel="alternate" type="text/html" title="Reinforcement Learning â€” Value Functions"/><published>2025-08-02T07:00:00+00:00</published><updated>2025-08-02T07:00:00+00:00</updated><id>https://m0gician.github.io/blog/2025/value-functions</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/value-functions/"><![CDATA[<p>Last time, we covered how MDPs are integrated into Reinforcement Learning. In this chapter, we will see how we can evaluate an RL agent mathematically based on the definitions of MDPs.</p> <h2 id="state-value-function">State-Value Function</h2> <p>The <em>State-Value function</em> \(v^\pi(s)\) calculates the expected discounted return if the agent starts in state \(s\) and follows policy \(\pi\). Informally, it tells us how â€œgoodâ€ it is for the agent to be in state \(s\) when using policy \(\pi\). We call \(v^\pi(s)\) the <em>value of state</em> \(s\).</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\ \Bigg[\underbrace{\sum_{k=0}^{\infty}\gamma^k R_{t+k}}_{G_t} \bigg| S_t=s, \pi\ \Bigg] \\ &amp;:= \mathbf{E}[G_t|S_t=s, \pi] \\ &amp;:= \mathbf{E}\ \Bigg[\sum_{t=0}^{\infty}\gamma^k R_{t} \bigg| S_0=s, \pi\ \Bigg] \end{aligned}\] <p>Recalling the \(G_t\) notation (<em>discounted return from time</em> \(t\)) we covered last time, we can see that this is an equivalent definition.</p> <h3 id="a-simple-mdp-example">A Simple MDP Example</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mdp-simple-480.webp 480w,/assets/img/rl/mdp-simple-800.webp 800w,/assets/img/rl/mdp-simple-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mdp-simple.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Simple MDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p>In the MDP above, the agent can choose between two actions: <code class="language-plaintext highlighter-rouge">Left</code> or <code class="language-plaintext highlighter-rouge">Right</code>. In states \(s_1\) and \(s_6\), any action will cause a transition to the terminal state \(s_\infty\). The agent only gets a reward when transitioning from \(s_2 \to s_1\) or \(s_5 \to s_6\). For simplicity, we will use \(\gamma = 0.5\).</p> <p>Letâ€™s test two policies for this MDP. One policy, \(\pi_1\), will always select <code class="language-plaintext highlighter-rouge">Left</code> action; another policy, \(\pi_2\), will always select the <code class="language-plaintext highlighter-rouge">Right</code> action.</p> <p><strong>Policy 1 (\(\pi_1\)): Select <code class="language-plaintext highlighter-rouge">Left</code> Action Always</strong></p> <ul> <li>\(v^{\pi_1}(s_1) = 0\) (goes to the terminal state always)</li> <li>\(v^{\pi_1}(s_2) = 12\gamma^0 = 12\).</li> <li>\(v^{\pi_1}(s_3) = 0\gamma^0 + 12\gamma^1 = 6\).</li> <li>\(v^{\pi_1}(s_4) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(v^{\pi_1}(s_5) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(v^{\pi_1}(s_6) = 0\).</li> </ul> <p><strong>Policy 2 (\(\pi_2\)): Select <code class="language-plaintext highlighter-rouge">Right</code> Action Always</strong></p> <ul> <li>\(v^{\pi_2}(s_1) = 0\).</li> <li>\(v^{\pi_2}(s_2) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(v^{\pi_2}(s_3) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(v^{\pi_2}(s_4) = 0\gamma^0 + 2\gamma^1 = 1\).</li> <li>\(v^{\pi_2}(s_5) = 2\gamma^0 = 2\).</li> <li>\(v^{\pi_2}(s_6) = 0\).</li> </ul> <h2 id="action-value-function">Action-Value Function</h2> <p>The <em>Action-Value Function</em> \(q^\pi(s,a)\), or <em>Q-function</em>, evaluates the expected discounted return if the agent takes action \(a\) in state \(s\) and follows policy \(\pi\) thereafter.</p> \[\begin{aligned} q^\pi(s,a) &amp;:= \mathbf{E}\ \Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, A_t = a, \pi\ \Bigg] \\ &amp;:= \mathbf{E}[G_t|S_t=s, A_t=a, \pi] \\ &amp;:= \mathbf{E}\ \Bigg[\sum_{t=0}^{\infty}\gamma^k R_{t} \bigg| S_0=s, A_0=a, \pi\ \Bigg] \end{aligned}\] <h3 id="a-simple-mdp-example-again">A Simple MDP Example, Again</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mdp-simple-480.webp 480w,/assets/img/rl/mdp-simple-800.webp 800w,/assets/img/rl/mdp-simple-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mdp-simple.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Simple MDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p><strong>Policy 1 (\(\pi_1\)): Select <code class="language-plaintext highlighter-rouge">Left</code> Action Always</strong></p> <ul> <li>\(q^{\pi_1}(s_1, L) = 0\).</li> <li>\(q^{\pi_1}(s_1, R) = 0\).</li> <li>\(q^{\pi_1}(s_2, L) = 12\gamma^0 =12\).</li> <li>\(q^{\pi_1}(s_2, R) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(q^{\pi_1}(s_3, L) = 0\gamma^0 + 12\gamma^1 = 6\).</li> <li>\(q^{\pi_1}(s_3, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(q^{\pi_1}(s_4, L) = 0\gamma^0 + 0\gamma^1 + 12\gamma^2 = 3\).</li> <li>\(q^{\pi_1}(s_4, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 0\gamma^3 + 12\gamma^4 = 0.75\).</li> <li>\(q^{\pi_1}(s_5, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 12\gamma^3 = 1.5\).</li> <li>\(q^{\pi_1}(s_5, R) = 2\gamma^0 = 2\).</li> <li>\(q^{\pi_1}(s_6, L) = 0\).</li> <li>\(q^{\pi_1}(s_6, R) = 0\).</li> </ul> <p><strong>Policy 2 (\(\pi_2\)): Select <code class="language-plaintext highlighter-rouge">Right</code> Action Always</strong></p> <ul> <li>\(q^{\pi_2}(s_1, L) = 0\).</li> <li>\(q^{\pi_2}(s_1, R) = 0\).</li> <li>\(q^{\pi_2}(s_2, L) = 12\gamma^0 =12\).</li> <li>\(q^{\pi_2}(s_2, R) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(q^{\pi_2}(s_3, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 0\gamma^3 + 2\gamma^4 = 0.125\).</li> <li>\(q^{\pi_2}(s_3, R) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(q^{\pi_2}(s_4, L) = 0\gamma^0 + 0\gamma^1 + 0\gamma^2 + 2\gamma^3 = 0.25\).</li> <li>\(q^{\pi_2}(s_4, R) = 0\gamma^0 + 2\gamma^1 = 1\).</li> <li>\(q^{\pi_2}(s_5, L) = 0\gamma^0 + 0\gamma^1 + 2\gamma^2 = 0.5\).</li> <li>\(q^{\pi_2}(s_5, R) = 2\gamma^0 = 2\).</li> <li>\(q^{\pi_2}(s_6, L) = 0\).</li> <li>\(q^{\pi_2}(s_6, R) = 0\).</li> </ul> <h2 id="the-bellman-equation-for-vpi">The Bellman Equation for \(v^\pi\)</h2> <p>The <em>Bellman Equation for</em> \(v^\pi\) is a recursive expression for the state-value function. To derive the Bellman equation, we first isolate the immediate reward from the state-value function:</p> \[\begin{aligned} v^\pi(s) &amp;:= \textbf{E}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg\vert S_t=s, \pi\right] \\ &amp;= \textbf{E}\left[R_t + \sum_{k=1}^{\infty}\gamma^k R_{t+k} \bigg\vert S_t=s, \pi\right] \\ &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=1}^{\infty}\gamma^{k-1} R_{t+k} \bigg\vert S_t=s, \pi\right] \end{aligned}\] <p>We can perform a simple transformation by modifying the indexing of the sum to start at zero instead of one, which changes all uses of \(k\) within the sum to \(k+1\):</p> \[\begin{aligned} \textbf{E}\left[\gamma\sum_{k=1}^{\infty}\gamma^{k-1} R_{t+k} \bigg\vert S_t=s, \pi\right] = \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \end{aligned}\] <p>Recalling the law of total probability, \(\textbf{E}[X] = \textbf{E}[\textbf{E}[X\vert Y]]\), if we use the first action \(A_t=a\) and the first next state \(S_{t+1}=s^\prime\) as intermediate conditioning variables, we can rewrite the expected reward after the first action term as:</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ = &amp;\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) \times \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t=a, S_{t+1}=s^\prime, \pi\right] \end{aligned}\] <p>Furthermore, the Markov property tells us the future is independent of everything before \(t+1\). Hence, for the expected next-step reward, we can safely remove \(S_t\) and \(A_t\) from the conditioning:</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ = &amp;\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) \times \gamma\textbf{E}\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, \pi\right] \end{aligned}\] <p>Recall the definition of the state-value function, the last term is exactly \(v^\pi(s^\prime)\):</p> \[\begin{aligned} &amp;\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, \pi\right] \\ = &amp; \gamma\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) v^\pi(s^\prime) \end{aligned}\] <p>Similarly, since for any fixed \(s\) and \(a\) the transition probability to the next state \(s^\prime\) must be 1, we can multiply the immediate reward \(R_t\) by â€œoneâ€:</p> \[\begin{aligned} R_t = \sum_{a\in\mathcal{A}}\pi(s,a)R(s,a) = \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)R(s,a) \end{aligned}\] <p>Now, we put everything together:</p> \[\begin{aligned} v^\pi(s) &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, \pi\right] \\ &amp;= \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)R(s,a) + \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime) \gamma v^\pi(s^\prime) \end{aligned}\] <p>Now, we can combine the common terms to get the final simplified form of the state-value function:</p> \[v^\pi(s) = \boxed{\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\big(R(s,a) +\gamma v^\pi(s^\prime)\big)}\] <h3 id="pros-about-bellman-equation">Pros about Bellman Equation</h3> <div class="callout"> <p>We can view the Bellman equation as breaking the expected return that will occur into two parts:</p> <ol> <li>the reward that we will obtain during the next time step (<em>immediate reward</em>)</li> <li>the value of the next state that we end up in</li> </ol> \[v^\pi(s) = \textbf{E}\left[\underbrace{R(s,A_t)}_{\text{immediate reward}} + \gamma\underbrace{v^\pi(S_{t+1})}_{\text{value of next state}}\Bigg\vert S_t=s, \pi\right]\] <p>While the original definition of the value function must consider the entire sequence of states, the Bellman equation on the other hand, <strong>only needs to look forward one time step into the future</strong>.</p> <ul> <li>the recurrent nature of the Bellman equation makes it more computationally helpful</li> </ul> </div> <h2 id="the-bellman-equation-for-qpi">The Bellman Equation for \(q^\pi\)</h2> <p>While the Bellman equation for \(v^\pi\) is a recursive expression for \(v^\pi\), the Bellman equation for \(q^\pi\) is a recursive expression for the action-value function \(q^\pi\).</p> \[\begin{aligned} q^\pi(s,a) &amp;:= \mathbf{E}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, A_t = a, \pi\Bigg] \\ &amp;= R_t + \textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t = a, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}} \pi(s^\prime,a^\prime)\times\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_t=s, A_t = a, S_{t+1}=s^\prime, A_{t+1}=a^\prime, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime) \times\textbf{E}\left[\gamma\sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg\vert S_{t+1}=s^\prime, A_{t+1}=a^\prime, \pi\right] \\ &amp;= R_t + \sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)\gamma q^\pi(s^\prime, s^\prime) \\ &amp;= \boxed{R(s,a) + \gamma\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\sum_{a^\prime \in \mathcal{A}}\pi(s^\prime,a^\prime)q^\pi(s^\prime, a^\prime)} \end{aligned}\] <h2 id="optimal-value-functions">Optimal Value Functions</h2> <div class="callout"> <p><strong>Optimal Policy</strong> \(\pi^*\) An optimal policy, \(\pi^*\) is any policy that is at least as good as all other policies. In other words, \(\pi^*\) is an optimal policy if and only if</p> \[\forall \pi \in \Pi, \pi^* \geq \pi\] </div> <div class="callout"> <p>Notice that even when \(\pi^*\) is not unique, the optimal value functions \(v^*\) and \(q^*\) are uniqueâ€”all optimal policies share the same state-value function and action-value function.</p> </div> <div class="callout"> <details> <summary> Given the optimal state-value function, can you compute the optimal policy if you do not know the transition probabilities and reward function? </summary> <br/><strong>No</strong>. $$ \arg\max_{a\in\mathcal{A}}\sum_{s^\prime}p(s,a,s^\prime)[R(s,a) + \gamma v^\pi(s^\prime)] $$ is an optimal action in state s. Computing these actions requires knowledge of p and R </details> </div> <div class="callout"> <details> <summary> Given the optimal action-value function, can you compute the optimal policy if you do not know the transition probabilities and reward function? </summary> <br/><strong>Yes</strong>. $$ \arg\max_{a\in\mathcal{A}}q^*(s,a) $$ is an optimal action in state s </details> </div> <h3 id="bellman-optimality-equation-for-v">Bellman Optimality Equation for \(v^*\)</h3> <p>The <em>Bellman Optimality Equation for</em> \(v^*\) is a recursive expression for \(v^*\). Letâ€™s start with the Bellman equation:</p> \[v^*(s) = \sum_{a\in\mathcal{A}}\pi^*(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^*(s^\prime)]\] <p>Since the optimal policy \(\pi^*\) only picks the action that maximizes \(q^*(s,a)\), we do not need to consider all possible actions \(a\), but only those that cause the \(q^*(s,a)\) term to be maximized:</p> \[v^*(s) = \max_{a\in\mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^*(s^\prime)]\] <p>The equation above is the <em>Bellman optimality equation</em> <em>for</em> \(v^*\).</p> <div class="callout"> <p>A policy \(\pi\), <em>satisfies the Bellman optimality equation</em> if for all states \(s \in \mathcal{S}\):</p> \[v^*(s) = \max_{a\in\mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^*(s^\prime)]\] </div> <h3 id="bellman-optimality-equation-for-q">Bellman Optimality Equation for \(q^*\)</h3> <div class="callout"> <p>A policy \(\pi\), satisfies the Bellman optimality equation if for all actions \(a \in \mathcal{A}\):</p> \[q^*(s,a) = \sum_{s^\prime \in \mathcal{S}} p(s,a,s^\prime)\left[ R(s,a) + \gamma \max_{a^\prime\in\mathcal{A}}q^*(s^\prime, a^\prime)\right]\] </div> <h3 id="bellman-optimality-equation-and-the-optimal-policy">Bellman Optimality Equation and the Optimal Policy</h3> <div class="callout"> <p><em>If a policy <strong>\(\pi\)</strong> satisfies the Bellman optimality equation, then <strong>\(\pi\)</strong> is an optimal policy.</em></p> </div> <p><em>Proof:</em></p> <p>Assuming a policy \(\pi\) satisfies the Bellman optimality equation, we have for all states \(s\):</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)[R(s,a) + \gamma v^\pi(s^\prime)]\] <p>We can apply the Bellman optimality equation recursively into the expression and replace \(v^\pi(s^\prime)\):</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma v^\pi(s^{\prime\prime})\right)\right]\] <p>We could continue this process indefinitely until \(\pi\) is completely eliminated from the expression:</p> \[v^\pi(s) = \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right]\] <p>At each time \(t\), the action is chosen that maximizes the expected discounted sum of future rewards, given that future actions are also chosen to maximize the discounted sum of future rewards.</p> <p>Now, letâ€™s consider any new policy \(\pi^\prime\). What will be the relationship if we replace \(\max_{a \in \mathcal{A}}\) with \(\sum_{a \in \mathcal{A}}\pi^\prime(s,a)\)? We argue that the expression could not become bigger than the previous one. That is, for any policy \(\pi^\prime\):</p> \[\begin{aligned} v^\pi(s) &amp;= \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \\ &amp;\geq \sum_{a \in \mathcal{A}}\pi^\prime(s,a)\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\sum_{a^\prime \in \mathcal{A}}\pi^\prime(s^\prime,a^\prime)\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \end{aligned}\] <p>Given that the above holds for all policies \(\pi^\prime\), we have that for all states \(s \in \mathcal{S}\) and all policies \(\pi^\prime \in \Pi\):</p> \[\begin{aligned} v^\pi(s) &amp;= \max_{a \in \mathcal{A}}\sum_{s^\prime \in \mathcal{S}}p(s,a,s^\prime)\left[R(s,a) + \gamma \left(\max_{a^\prime \in \mathcal{A}}\sum_{s^{\prime\prime}}p(s^\prime, a^\prime, s^{\prime\prime})(R(s^\prime, a^\prime) + \gamma \ldots\right)\right] \\ &amp;\geq \mathbf{E}[G_t | S_t = s, \pi^\prime] \\ &amp;= v^{\pi^\prime}(s) \end{aligned}\] <p>Hence, for all states \(s \in \mathcal{S}\), and all policies \(\pi^\prime \in \Pi\), \(v^\pi(s) \geq v^{\pi^\prime}(s)\). In other words, for all policies \(\pi^\prime \in \Pi\), we have that \(\pi \geq \pi^\prime\), and hence \(\pi\) is an optimal policy.</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[A deep dive into value functions and the Bellman equation in Reinforcement Learning.]]></summary></entry><entry><title type="html">å¼ºåŒ–å­¦ä¹  - é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸å¼ºåŒ–å­¦ä¹ </title><link href="https://m0gician.github.io/blog/2025/mdp-and-rl-zh/" rel="alternate" type="text/html" title="å¼ºåŒ–å­¦ä¹  - é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸å¼ºåŒ–å­¦ä¹ "/><published>2025-07-15T04:00:00+00:00</published><updated>2025-07-15T04:00:00+00:00</updated><id>https://m0gician.github.io/blog/2025/mdp-and-rl-zh</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/mdp-and-rl-zh/"><![CDATA[<h2 id="ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ">ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ</h2> <div class="blockquote"> <p>å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé¢†åŸŸï¼Œå…¶çµæ„Ÿæ¥è‡ªè¡Œä¸ºä¸»ä¹‰å¿ƒç†å­¦ï¼Œç ”ç©¶æ™ºèƒ½ä½“å¦‚ä½•ä»ä¸ç¯å¢ƒçš„äº’åŠ¨ä¸­å­¦ä¹ ã€‚ <br/>â€”Sutton &amp; Barto (1998), Phil, <cite>ç»´åŸºç™¾ç§‘</cite></p> </div> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/rl-system-480.webp 480w,/assets/img/rl/rl-system-800.webp 800w,/assets/img/rl/rl-system-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/rl-system.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="å¼ºåŒ–å­¦ä¹ å›¾ç¤º" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p>ä¸€ä¸ªå…¸å‹çš„å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿç”±5ä¸ªéƒ¨åˆ†ç»„æˆï¼š<strong>æ™ºèƒ½ä½“</strong>ï¼ˆagentï¼‰åœ¨<strong>ç¯å¢ƒ</strong>ï¼ˆenvironmentï¼‰ä¸­çš„æ¯ä¸ª<strong>çŠ¶æ€</strong>ï¼ˆstateï¼‰ä¸‹æ‰§è¡Œä¸€ä¸ª<strong>åŠ¨ä½œ</strong>ï¼ˆactionï¼‰ï¼Œå¹¶åœ¨æ»¡è¶³æŸäº›æ ‡å‡†æ—¶è·å¾—<strong>å¥–åŠ±</strong>ï¼ˆrewardï¼‰ã€‚</p> <div class="callout"> <details><summary><strong>ç›‘ç£å­¦ä¹ é—®é¢˜å¯ä»¥è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜å—ï¼Ÿ</strong></summary> <strong>å¯ä»¥</strong>ã€‚æˆ‘ä»¬å¯ä»¥å°†ä¸€ä¸ªç›‘ç£å­¦ä¹ é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼ˆçŠ¶æ€ä½œä¸ºåˆ†ç±»å™¨çš„è¾“å…¥ï¼›åŠ¨ä½œä½œä¸ºæ ‡ç­¾ï¼›å¦‚æœæ ‡ç­¾æ­£ç¡®ï¼Œå¥–åŠ±ä¸º1ï¼Œå¦åˆ™ä¸º-1ï¼‰ã€‚</details> </div> <div class="callout"> <details><summary><strong>å¼ºåŒ–å­¦ä¹ æ˜¯ç›‘ç£å­¦ä¹ çš„æ›¿ä»£å“å—ï¼Ÿ</strong></summary> <p><strong>ä¸æ˜¯</strong>ã€‚ç›‘ç£å­¦ä¹ ä½¿ç”¨æŒ‡å¯¼æ€§åé¦ˆï¼ˆæ™ºèƒ½ä½“åº”è¯¥é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ï¼‰ã€‚ä»»ä½•åç¦»æ‰€æä¾›åé¦ˆçš„è¡Œä¸ºéƒ½ä¼šå—åˆ°æƒ©ç½šã€‚</p> <p>å¦ä¸€æ–¹é¢ï¼Œå¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸æ˜¯ä»¥å›ºå®šçš„æ•°æ®é›†å½¢å¼æä¾›çš„ï¼Œè€Œæ˜¯ä»¥ä»£ç æˆ–æ•´ä¸ªç¯å¢ƒçš„æè¿°å½¢å¼æä¾›çš„ã€‚å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±åº”è¯¥ä¼ è¾¾æ™ºèƒ½ä½“çš„è¡Œä¸ºæœ‰å¤šâ€œå¥½â€ï¼Œè€Œä¸æ˜¯æœ€å¥½çš„è¡Œä¸ºåº”è¯¥æ˜¯ä»€ä¹ˆã€‚æ™ºèƒ½ä½“çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ€»å¥–åŠ±ï¼Œè¿™å¯èƒ½éœ€è¦æ™ºèƒ½ä½“æ”¾å¼ƒçœ¼å‰çš„å¥–åŠ±ä»¥è·å¾—ä»¥åæ›´å¤§çš„å¥–åŠ±ã€‚</p> <p>å¦‚æœä½ æœ‰ä¸€ä¸ªåºåˆ—é—®é¢˜æˆ–ä¸€ä¸ªåªæœ‰è¯„ä¼°æ€§åé¦ˆå¯ç”¨çš„é—®é¢˜ï¼ˆæˆ–ä¸¤è€…å…¼æœ‰ï¼ï¼‰ï¼Œé‚£ä¹ˆä½ åº”è¯¥è€ƒè™‘ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ã€‚</p></details> </div> <h3 id="ç¤ºä¾‹ç½‘æ ¼ä¸–ç•Œ">ç¤ºä¾‹ï¼šç½‘æ ¼ä¸–ç•Œ</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/gridworld-480.webp 480w,/assets/img/rl/gridworld-800.webp 800w,/assets/img/rl/gridworld-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/gridworld.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="ç½‘æ ¼ä¸–ç•Œ" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p><strong>çŠ¶æ€</strong>: æœºå™¨äººçš„ä½ç½®ã€‚æœºå™¨äººæ²¡æœ‰æœå‘ã€‚</p> <p><strong>åŠ¨ä½œ</strong>: <code class="language-plaintext highlighter-rouge">å°è¯•å‘ä¸Š</code> (AU), <code class="language-plaintext highlighter-rouge">å°è¯•å‘ä¸‹</code> (AD), <code class="language-plaintext highlighter-rouge">å°è¯•å‘å·¦</code> (AL), <code class="language-plaintext highlighter-rouge">å°è¯•å‘å³</code> (AR)</p> <p><strong>ç¯å¢ƒåŠ¨æ€</strong>:</p> <p><strong>å¥–åŠ±</strong>:</p> <ul> <li>æ™ºèƒ½ä½“è¿›å…¥æœ‰æ°´çš„çŠ¶æ€ä¼šå¾—åˆ°-10çš„å¥–åŠ±ï¼Œè¿›å…¥ç›®æ ‡çŠ¶æ€ä¼šå¾—åˆ°+10çš„å¥–åŠ±ã€‚</li> <li>è¿›å…¥ä»»ä½•å…¶ä»–çŠ¶æ€çš„å¥–åŠ±ä¸ºé›¶ã€‚</li> <li>ä»»ä½•å¯¼è‡´æ™ºèƒ½ä½“åœç•™åœ¨çŠ¶æ€21çš„åŠ¨ä½œéƒ½å°†è¢«è§†ä¸ºå†æ¬¡è¿›å…¥æ°´åŸŸçŠ¶æ€ï¼Œå¹¶å¯¼è‡´é¢å¤–çš„-10å¥–åŠ±ã€‚</li> <li>å¥–åŠ±æŠ˜æ‰£å‚æ•° \(\gamma = 0.9\)ã€‚</li> </ul> <p><strong>çŠ¶æ€æ•°é‡</strong>: 24</p> <ul> <li>23ä¸ªæ­£å¸¸çŠ¶æ€ + 1ä¸ªç»ˆæ­¢å¸æ”¶çŠ¶æ€ (\(s_\infty\)) <ul> <li>ä¸€æ—¦è¿›å…¥\(s_\infty\)ï¼Œæ™ºèƒ½ä½“å°±æ°¸è¿œæ— æ³•ç¦»å¼€ï¼ˆ<strong>å›åˆ</strong>ç»“æŸï¼‰ã€‚</li> <li>\(s_\infty\) ä¸åº”è¢«è®¤ä¸ºæ˜¯â€œç›®æ ‡â€çŠ¶æ€ã€‚</li> </ul> </li> </ul> <hr/> <h2 id="ç”¨æ•°å­¦æ–¹å¼æè¿°æ™ºèƒ½ä½“å’Œç¯å¢ƒ">ç”¨æ•°å­¦æ–¹å¼æè¿°æ™ºèƒ½ä½“å’Œç¯å¢ƒ</h2> <h3 id="ç¯å¢ƒçš„æ•°å­¦å®šä¹‰">ç¯å¢ƒçš„æ•°å­¦å®šä¹‰</h3> <p>æˆ‘ä»¬å¯ä»¥ä½¿ç”¨<strong>é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹</strong>ï¼ˆMDPsï¼‰æ¥å½¢å¼åŒ–å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„ç¯å¢ƒã€‚å…¶ä¸­çš„ç‹¬ç‰¹æœ¯è¯­æ˜¯\(\mathcal{S}\)ï¼ˆæ‰€æœ‰å¯èƒ½çŠ¶æ€çš„é›†åˆï¼‰ï¼Œ\(\mathcal{A}\)ï¼ˆæ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„é›†åˆï¼‰ï¼Œ\(p\)ï¼ˆè½¬ç§»å‡½æ•°ï¼‰ï¼Œ\(d_R\)ï¼ˆå¥–åŠ±åˆ†å¸ƒï¼‰ï¼Œ\(R\)ï¼ˆå¥–åŠ±å‡½æ•°ï¼‰ï¼Œ\(d_0\)ï¼ˆåˆå§‹çŠ¶æ€åˆ†å¸ƒï¼‰å’Œ\(\gamma\)ï¼ˆå¥–åŠ±æŠ˜æ‰£å‚æ•°ï¼‰ã€‚ç¯å¢ƒçš„é€šç”¨å®šä¹‰æ˜¯</p> \[(\mathcal{S}, \mathcal{A}, p, R, \gamma)\] <h3 id="æ™ºèƒ½ä½“çš„æ•°å­¦å®šä¹‰">æ™ºèƒ½ä½“çš„æ•°å­¦å®šä¹‰</h3> <p>æˆ‘ä»¬å°†æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œçš„å†³ç­–è§„åˆ™å®šä¹‰ä¸º<strong>ç­–ç•¥</strong>ã€‚å½¢å¼ä¸Šï¼Œç­–ç•¥\(\pi\)æ˜¯ä¸€ä¸ªå‡½æ•°</p> \[\begin{aligned} &amp;\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0,1] \\ &amp;\pi(s,a) := \text{Pr}(A_t=a | S_t=s) \end{aligned}\] <div class="callout"> <p><strong>æ™ºèƒ½ä½“çš„ç›®æ ‡</strong></p> <p>æ™ºèƒ½ä½“çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥\(\pi^*\)ï¼Œä»¥æœ€å¤§åŒ–æ™ºèƒ½ä½“å°†è·å¾—çš„æ€»å¥–åŠ±çš„æœŸæœ›å€¼ã€‚</p> </div> <h3 id="ç¤ºä¾‹å±±åœ°è½¦">ç¤ºä¾‹ï¼šå±±åœ°è½¦</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mountain-car-480.webp 480w,/assets/img/rl/mountain-car-800.webp 800w,/assets/img/rl/mountain-car-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mountain-car.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="å±±åœ°è½¦" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <div class="caption"> å±±åœ°è½¦ç¯å¢ƒ </div> <ul> <li><strong>çŠ¶æ€</strong>: \(s=(x,v)\), å…¶ä¸­ \(x \in \mathbb{R}\) æ˜¯å°è½¦çš„ä½ç½®ï¼Œ\(v \in \mathbb{R}\) æ˜¯é€Ÿåº¦ã€‚</li> <li><strong>åŠ¨ä½œ</strong>: \(a \in \{\texttt{å€’è½¦}, \texttt{ç©ºæŒ¡}, \texttt{å‰è¿›}\}\). è¿™äº›åŠ¨ä½œè¢«æ˜ å°„ä¸ºæ•°å€¼ \(a \in \{-1, 0 ,1\}\)ã€‚</li> <li> <p><strong>åŠ¨æ€</strong>: åŠ¨æ€æ˜¯ç¡®å®šæ€§çš„â€”â€”åœ¨çŠ¶æ€\(s\)ä¸‹é‡‡å–åŠ¨ä½œ\(a\)æ€»æ˜¯äº§ç”Ÿç›¸åŒçš„çŠ¶æ€\(s^\prime\)ã€‚å› æ­¤ï¼Œ\(p(s,a,s^\prime) \in \{0, 1\}\)ã€‚åŠ¨æ€ç‰¹æ€§å¦‚ä¸‹ï¼š</p> \[\begin{aligned} v_{t+1} &amp;= v_t + 0.001 a_t - 0.0025 \cos(3x_t) \\ x_{t+1} &amp;= x_t + v_{t+1} \end{aligned}\] <p>åœ¨è®¡ç®—å‡ºä¸‹ä¸€ä¸ªçŠ¶æ€ \(s^\prime = [x_{t+1}, v_{t+1}]\) åï¼Œ</p> <ul> <li>\(x_{t+1}\) çš„å€¼è¢«é™åˆ¶åœ¨é—­åŒºé—´ \([-1.2, 0.5]\) å†…ã€‚</li> <li>\(v_{t+1}\) çš„å€¼è¢«é™åˆ¶åœ¨é—­åŒºé—´ \([-0.7, 0.7]\) å†…ã€‚</li> <li>å¦‚æœ \(x_{t+1}\) åˆ°è¾¾å·¦è¾¹ç•Œæˆ–å³è¾¹ç•Œï¼ˆ\(x_{t+1} = -1.2\) æˆ– \(x_{t+1} = 0.5\)ï¼‰ï¼Œé‚£ä¹ˆå°è½¦çš„é€Ÿåº¦å°†é‡ç½®ä¸ºé›¶ï¼ˆ\(v_{t+1} = 0\)ï¼‰ã€‚</li> </ul> </li> <li><strong>åˆå§‹çŠ¶æ€</strong>: \(S_0 = (X_0, 0)\), å…¶ä¸­ \(X_0\) æ˜¯ä»åŒºé—´ \([-0.6, -0.4]\) ä¸­å‡åŒ€éšæœºæŠ½å–çš„åˆå§‹ä½ç½®ã€‚</li> <li><strong>ç»ˆæ­¢çŠ¶æ€</strong>: å¦‚æœ \(x_t = 0.5\)ï¼Œåˆ™è¯¥çŠ¶æ€ä¸ºç»ˆæ­¢çŠ¶æ€ï¼ˆå®ƒæ€»æ˜¯è½¬ç§»åˆ° \(s_\infty\)ï¼‰ã€‚</li> <li><strong>å¥–åŠ±</strong>: \(R_t\) æ€»æ˜¯ä¸º -1ï¼Œé™¤éè½¬ç§»åˆ° \(s_\infty\)ï¼ˆä» \(s_\infty\) æˆ–ä»ç»ˆæ­¢çŠ¶æ€ï¼‰ï¼Œæ­¤æ—¶ \(R_t = 0\)ã€‚</li> <li><strong>æŠ˜æ‰£</strong>: \(\gamma = 1.0\)ã€‚</li> </ul> <hr/> <h3 id="é™„åŠ æœ¯è¯­ç¬¦å·å’Œå‡è®¾">é™„åŠ æœ¯è¯­ã€ç¬¦å·å’Œå‡è®¾</h3> <ul> <li> <p><strong>å†å²</strong>ï¼ˆhistoryï¼‰ï¼Œ\(H_t\)ï¼Œæ˜¯å›åˆä¸­ç›´åˆ°æ—¶é—´\(t\)æ‰€å‘ç”Ÿäº‹ä»¶çš„è®°å½•ï¼š</p> \[H_t := (S_0, A_0, R_0, S_1, A_1, R_1, \ldots, S_t, A_t, R_t)\] </li> <li><strong>è½¨è¿¹</strong>ï¼ˆtrajectoryï¼‰æ˜¯æ•´ä¸ªå›åˆçš„å†å²ï¼š\(H_\infty\)</li> <li>è½¨è¿¹çš„<strong>å›æŠ¥</strong>ï¼ˆreturnï¼‰æˆ–<strong>æŠ˜æ‰£å›æŠ¥</strong>ï¼ˆdiscounted returnï¼‰æ˜¯å¥–åŠ±çš„æŠ˜æ‰£æ€»å’Œ \(G := \sum_{t = 0}^{\infty} \gamma^t R_t\)</li> <li><strong>æœŸæœ›å›æŠ¥</strong>ï¼ˆexpected returnï¼‰æˆ–<strong>æœŸæœ›æŠ˜æ‰£å›æŠ¥</strong>ï¼ˆexpected discounted returnï¼‰å¯ä»¥å†™æˆ \(J(\pi) := \mathbf{E}[G\vert\pi]\)</li> <li>ä»æ—¶é—´\(t\)å¼€å§‹çš„<strong>å›æŠ¥</strong>æˆ–ä»æ—¶é—´\(t\)å¼€å§‹çš„<strong>æŠ˜æ‰£å›æŠ¥</strong>ï¼Œ\(G_t\)ï¼Œæ˜¯ä»æ—¶é—´\(t\)å¼€å§‹çš„å¥–åŠ±çš„æŠ˜æ‰£æ€»å’Œ</li> </ul> \[G_t := \sum_{k=1}^{\infty} \gamma^k R_{t+k}\] <ul> <li> <p>MDPçš„<strong>èŒƒå›´</strong>ï¼ˆhorizonï¼‰ï¼Œ\(L\)ï¼Œæ˜¯æ»¡è¶³ä»¥ä¸‹æ¡ä»¶çš„æœ€å°æ•´æ•°</p> \[\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\] <ul> <li>å¦‚æœå¯¹äºæ‰€æœ‰ç­–ç•¥ \(L &lt; \infty\)ï¼Œæˆ‘ä»¬ç§°è¯¥MDPä¸º<strong>æœ‰é™èŒƒå›´</strong>ï¼ˆfinite horizonï¼‰</li> <li>å¦‚æœ \(L = \infty\)ï¼Œåˆ™è¯¥é¢†åŸŸå¯èƒ½æ˜¯<strong>ä¸ç¡®å®šèŒƒå›´</strong>ï¼ˆindefinite horizonï¼‰ï¼ˆæ™ºèƒ½ä½“æ€»æ˜¯ä¼šè¿›å…¥\(s_\infty\)ï¼‰æˆ–<strong>æ— é™èŒƒå›´</strong>ï¼ˆinfinite horizonï¼‰ï¼ˆæ™ºèƒ½ä½“å¯èƒ½æ°¸è¿œä¸ä¼šè¿›å…¥\(s_\infty\)ï¼‰</li> </ul> </li> </ul> <hr/> <h3 id="é©¬å°”å¯å¤«æ€§è´¨">é©¬å°”å¯å¤«æ€§è´¨</h3> <div class="callout"> <p><strong>é©¬å°”å¯å¤«æ€§è´¨ (</strong>é©¬å°”å¯å¤«å‡è®¾<strong>)</strong></p> <p>ç®€è€Œè¨€ä¹‹ï¼š<strong><em>ç»™å®šç°åœ¨ï¼Œæœªæ¥ä¸è¿‡å»æ— å…³</em></strong>ã€‚</p> <p>å½¢å¼ä¸Šï¼Œç»™å®š\(S_t\)ï¼Œ\(S_{t+1}\) æ¡ä»¶ç‹¬ç«‹äº \(H_{t-1}\)ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºæ‰€æœ‰çš„ \(h, s, a, s^\prime, t\)ï¼š</p> <p>\(\text{Pr}(S_{t+1} = s^\prime | H_{t-1} = h, S_{t}=s, A_{t}=a) = \text{Pr}(S_{t+1}=s^\prime | S_{t}=s, A_{t}=a)\)</p> </div> <p>å¦‚æœä¸€ä¸ªæ¨¡å‹ï¼ˆç¯å¢ƒã€å¥–åŠ±â€¦ï¼‰æ»¡è¶³é©¬å°”å¯å¤«å‡è®¾ï¼Œæˆ‘ä»¬å°±è¯´å®ƒå…·æœ‰é©¬å°”å¯å¤«æ€§è´¨ï¼Œæˆ–è€…è¯´è¿™ä¸ªæ¨¡å‹æ˜¯ <strong><em>Markovian</em></strong>çš„ã€‚</p> <hr/> <h2 id="ä¸ºä»€ä¹ˆåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä½¿ç”¨mdp">ä¸ºä»€ä¹ˆåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä½¿ç”¨MDPï¼Ÿ</h2> <p>MDPä¸ä»…åŠŸèƒ½å¼ºå¤§ï¼Œè¶³ä»¥æ¨¡æ‹Ÿå­¦ä¹ æ™ºèƒ½ä½“ä¸å…¶ç¯å¢ƒä¹‹é—´çš„äº¤äº’ï¼Œå®ƒè¿˜å¸¦æ¥äº†ä¸€äº›å…³é”®çš„ä¿è¯ï¼Œä½¿æˆ‘ä»¬çš„â€œå¼ºåŒ–å­¦ä¹ â€èƒ½å¤ŸçœŸæ­£èµ·ä½œç”¨ã€‚</p> <p>ç°åœ¨ï¼Œè®©æˆ‘ä»¬è·³è¿‡æ¨å¯¼ï¼Œç›´æ¥çœ‹ç»“è®ºã€‚</p> <div class="callout"> <p><strong>æœ€ä¼˜ç­–ç•¥çš„å­˜åœ¨æ€§</strong></p> <p>å¯¹äºæ‰€æœ‰æ»¡è¶³ \(|\mathcal{S}| &lt; \infty\), \(|\mathcal{A}| &lt; \infty\), \(R_\text{max} &lt; \infty\) å’Œ \(\gamma &lt; 1\) çš„MDPï¼Œè‡³å°‘å­˜åœ¨ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥ \(\pi^*\)ã€‚</p> </div> <p>ç¨åå½“æˆ‘ä»¬ä»‹ç»<strong>è´å°”æ›¼æ–¹ç¨‹</strong>å’Œ<strong>è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹</strong>æ—¶ï¼Œæˆ‘ä»¬å°†è¿›ä¸€æ­¥ç¡®å®šï¼š</p> <ol> <li>å¦‚æœä¸€ä¸ªç­–ç•¥\(\pi\)åœ¨æ¯ä¸ªæ­¥éª¤ä¸­éƒ½è¾¾åˆ°äº†ä¸€ä¸ªçŠ¶æ€ï¼Œå…¶æœŸæœ›çš„æœªæ¥å¥–åŠ±æ— æ³•é€šè¿‡ä»»ä½•å…¶ä»–è¡ŒåŠ¨æˆ–å†³ç­–è¿›ä¸€æ­¥æé«˜ï¼ˆ<strong>è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹</strong>ï¼‰ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥ã€‚</li> <li>å¦‚æœåªæœ‰æœ‰é™æ•°é‡çš„å¯èƒ½çŠ¶æ€å’ŒåŠ¨ä½œï¼Œå¥–åŠ±æœ‰ç•Œï¼Œå¹¶ä¸”æœªæ¥å¥–åŠ±è¢«æŠ˜æ‰£ï¼ˆæŠ˜æ‰£å› å­\(\gamma &lt; 1\)ï¼‰ï¼Œé‚£ä¹ˆå­˜åœ¨ä¸€ä¸ªæ»¡è¶³è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹çš„ç­–ç•¥\(\pi\)ã€‚</li> </ol> <p>æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹å’Œè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹æ‰§è¡Œç­–ç•¥/ä»·å€¼è¿­ä»£ï¼ˆç¨åå°†ä»‹ç»ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸ä»…å¯ä»¥ä¸€æ¬¡åˆä¸€æ¬¡åœ°è¿­ä»£å¾—åˆ°æ›´å¥½çš„ç­–ç•¥ï¼Œè€Œä¸”åœ¨æŸäº›çº¦æŸä¸‹ï¼Œè¿˜å¯ä»¥è¯æ˜æœ€ç»ˆç­–ç•¥å°†æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[å¼ºåŒ–å­¦ä¹ æ¦‚å¿µå…¥é—¨ï¼ŒåŒ…æ‹¬é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚]]></summary></entry><entry><title type="html">Reinforcement Learning - MDP and RL</title><link href="https://m0gician.github.io/blog/2025/mdp-and-rl/" rel="alternate" type="text/html" title="Reinforcement Learning - MDP and RL"/><published>2025-07-14T20:00:00+00:00</published><updated>2025-07-14T20:00:00+00:00</updated><id>https://m0gician.github.io/blog/2025/mdp-and-rl</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/mdp-and-rl/"><![CDATA[<h2 id="what-is-reinforcement-learning">What is Reinforcement Learning?</h2> <div class="blockquote"> <p>"Reinforcement Learning is an area of machine learning, inspired by behaviorist psychology, concerned with how an agent can learn from interactions with an environment." <br/>Sutton &amp; Barto (1998), Phil, <cite>Wikipedia</cite></p> </div> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/rl-system-480.webp 480w,/assets/img/rl/rl-system-800.webp 800w,/assets/img/rl/rl-system-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/rl-system.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="RL System" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p>A typical Reinforcement Learning system consists of 5 components: an <strong>agent</strong> takes an <strong>action</strong> at each <strong>state</strong> in an <strong>environment</strong> and receives a <strong>reward</strong> if some criteria are met.</p> <div class="callout"> <details><summary><strong>Can a Supervised Learning problem be converted into a RL problem?</strong></summary> <strong>Yes</strong>. One might take a supervised learning problem and convert it into an RL problem (the state as the input to a classifier; the action as a label; and the reward as 1 if the label is correct and -1 otherwise).</details> </div> <div class="callout"> <details><summary><strong>Is RL an alternative to Supervised Learning?</strong></summary> <p><strong>No</strong>. Supervised learning uses instructive feedback (what action the agent should have taken). Anything deviates the provided feedback will be penalized.</p> <p>RL problems on the other hand arenâ€™t provided as fixed data sets but as code or descriptions of the entire environment. Rewards in RL should convey how â€œgoodâ€ an agentâ€™s actions are, not what the best actions would have been. The goal of the agent is to maximize the total reward and this might require the agent forgoing immediate reward to obtain larger reward later.</p> <p>If you have a sequential problem or a problem where only evaluative feedback is available (or both!), then you should consider use RL.</p></details> </div> <h3 id="example-gridworld">Example: Gridworld</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/gridworld-480.webp 480w,/assets/img/rl/gridworld-800.webp 800w,/assets/img/rl/gridworld-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/gridworld.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Gridworld" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <p><strong>State</strong>: Position of robot. The robot does not have a direction that it is facing.</p> <p><strong>Actions</strong>: <code class="language-plaintext highlighter-rouge">Attemp_Up</code> (AU), <code class="language-plaintext highlighter-rouge">Attemp_Down</code> (AD), <code class="language-plaintext highlighter-rouge">Attemp_Left</code> (AL), <code class="language-plaintext highlighter-rouge">Attemp_Right</code> (AR)</p> <p><strong>Environment Dynamics</strong>:</p> <p><strong>Rewards</strong>:</p> <ul> <li>The agent receives a reward of -10 for entering the state with the water and a record of +10 for entering the goal state.</li> <li>Entering any other state results in a reward of zero.</li> <li>Any actions that cause the agent stays in state 21 will count as â€œenteringâ€ the water state again and result in an additional reward of -10.</li> <li>Reward discount parameter \(\gamma = 0.9\).</li> </ul> <p><strong>Number of States</strong>: 24</p> <ul> <li>23 normal states + 1 terminal absorbing state (\(s_\infty\)) <ul> <li>Once in \(s_\infty\), the agent can never leave (<em>episode</em> ends).</li> <li>\(s_\infty\) should not be thought as â€œgoalâ€ state.</li> </ul> </li> </ul> <hr/> <h2 id="describe-the-agent-and-environment-mathematically">Describe the Agent and Environment Mathematically</h2> <h3 id="math-definition-for-environment">Math Definition for Environment</h3> <p>We can use <em>Markov Decision Processes</em> (MDPs) to formalize the environment of an RL problem. The unique terms are \(\mathcal{S}\) (the set of all possible states), \(\mathcal{A}\) (the set of all possible actions), \(p\) (transition function), \(d_R\) (reward distribution), \(R\) (reward function), \(d_0\) (initial state distribution), and \(\gamma\) (reward discount parameter). The common definition of the environment is</p> \[(\mathcal{S}, \mathcal{A}, p, R, \gamma)\] <h3 id="math-definition-for-agent">Math Definition for Agent</h3> <p>We define the decision rule that the agent selects actions as a <strong>policy</strong>. Formally, a policy \(\pi\) is a function</p> \[\begin{aligned} &amp;\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0,1] \\ &amp;\pi(s,a) := \text{Pr}(A_t=a | S_t=s) \end{aligned}\] <div class="callout"> <p><strong>Agentâ€™s Goal</strong></p> <p>The agentâ€™s goal is to find an optimal policy \(\pi^*\) that maximizes the expected total amount of reward that the agent will obtain.</p> </div> <h3 id="example-mountain-car">Example: Mountain Car</h3> <div class="justify-content-sm-center"> <center><div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl/mountain-car-480.webp 480w,/assets/img/rl/mountain-car-800.webp 800w,/assets/img/rl/mountain-car-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/rl/mountain-car.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Mountain Car" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></center> </div> <ul> <li><strong>State</strong>: \(s=(x,v)\), where \(x \in \mathbb{R}\) is the position of the car and \(v \in \mathbb{R}\) is the velocity.</li> <li><strong>Actions</strong>: \(a \in \{\texttt{reverse}, \texttt{neutral}, \texttt{forward}\}\). These actions are mapped to numerical values as \(a \in \{-1, 0 ,1\}\).</li> <li><strong>Dynamics</strong>: The dynamics are deterministicâ€”taking action \(a\) in state \(s\) always produces the same state, \(s^\prime\). Thus, \(p(s,a,s^\prime) \in \{0, 1\}\). The dynamics are characterized by:</li> </ul> \[\begin{aligned} v_{t+1} &amp;= v_t + 0.001 a_t - 0.0025 \cos(3x_t) \\ x_{t+1} &amp;= x_t + v_{t+1} \end{aligned}\] <p>After the next state, \(s^\prime = [x_{t+1}, v_{t+1}]\) has been computed,</p> <ul> <li>the value of \(x_{t+1}\) is clipped so that it stays in the closed interval \([-1.2, 0.5]\).</li> <li>the value of \(v_{t+1}\) is clipped so that it stays in the closed interval \([-0.7, 0.7]\).</li> <li> <p>if \(x_{t+1}\) reaches the left or right bound (\(x_{t+1} = -1.2\) or \(x_{t+1} = 0.5\)), then the carâ€™s velocity is reset to zero (\(v_{t+1} = 0\)).</p> </li> <li><strong>Initial State</strong>: \(S_0 = (X_0, 0)\), where \(X_0\) is an initial position drawn uniformly at random from the interval \([-0.6, -0.4]\).</li> <li><strong>Terminal States</strong>: If \(x_t = 0.5\), then the state is terminal (it always transitions to \(s_\infty\)).</li> <li><strong>Rewards</strong>: \(R_t = -1\) always, except when transitioning to \(s_\infty\) (from \(s_\infty\) or from a terminal state), in which case \(R_t = 0\).</li> <li><strong>Discount</strong>: \(\gamma = 1.0\).</li> </ul> <hr/> <h3 id="additional-terminology-notation-and-assumptions">Additional Terminology, Notation, and Assumptions</h3> <ul> <li>A <em>history</em>, \(H_t\), is a recording of what has happened up to time \(t\) in an episode:</li> </ul> \[H_t := (S_0, A_0, R_0, S_1, A_1, R_1, \ldots, S_t, A_t, R_t)\] <ul> <li>A <em>trajectory</em> is the history of an entire episode: \(H_\infty\)</li> <li>The <em>return</em> or <em>discounted return</em> of a trajectory is the discounted sum of rewards \(G := \sum_{t = 0}^{\infty} \gamma^t R_t\)</li> <li>The <em>expected return</em> or <em>expected discounted return</em> can be written as \(J(\pi) := \mathbf{E}[G\vert\pi]\)</li> <li>The <em>return from time</em> \(t\) or <em>discounted return from time</em> \(t\), \(G_t\), is the discounted sum of rewards starting from time \(t\)</li> </ul> \[G_t := \sum_{k=1}^{\infty} \gamma^k R_{t+k}\] <ul> <li>The <em>horizon</em>, \(L\), of an MDP is the smallest integer such that \(\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\) <ul> <li>if \(L &lt; \infty\) for all policies, then we say that the MDP is <em>finite horizon</em></li> <li>if \(L = \infty\) then the domain may be <em>indefinite horizon</em> (the agent will always enter \(s_\infty\)) or <em>infinite horizon</em> (the agent may never enter \(s_\infty\))</li> </ul> </li> </ul> <hr/> <h3 id="markov-property">Markov Property</h3> <div class="callout"> <p><strong>Markov Property (<em>Markov Assumption</em>)</strong></p> <p>In short: <strong><em>given the present, the future does not depend on the past</em></strong>.</p> <p>Formally, \(S_{t+1}\) is conditionally independent of \(H_{t-1}\) given \(S_t\). That is, for all \(h, s, a, s^\prime, t\):</p> <p>\(\text{Pr}(S_{t+1} = s^\prime | H_{t-1} = h, S_{t}=s, A_{t}=a) = \text{Pr}(S_{t+1}=s^\prime | S_{t}=s, A_{t}=a)\)</p> </div> <p>If a model (environment, reward â€¦) holds the Markov assumption, we say it has the Markov property, or say the model is <strong><em>Markovian</em></strong>.</p> <hr/> <h2 id="why-use-mdp-in-rl">Why use MDP in RL?</h2> <p>MDP is not only powerful enough to model the interaction between a learning agent and its environment, it also brings some critical guarantees that make our â€œreinforcement learningâ€ actually work.</p> <p>For now, letâ€™s skip the derivation and jump straight to the conclusions.</p> <div class="callout"> <p><strong>Existence of an Optimal Policy</strong></p> <p>For all MDPs where \(|\mathcal{S}| &lt; \infty\), \(|\mathcal{A}| &lt; \infty\), \(R_\text{max} &lt; \infty\), and \(\gamma &lt; 1\), there exists at least one optimal policy, \(\pi^*\).</p> </div> <p>Later when we introduce the <em>Bellman equation</em> and the <em>Bellman optimality equation</em>, we will further establish that:</p> <ol> <li>if a policy \(\pi\) achieves a state where its expected future rewards cannot be improved further by any other action or decision at each step (<em>Bellman optimality equation</em>), then it is an optimal policy.</li> <li>if there are only a finite number of possible states and actions, rewards are bounded, and future rewards are discounted (with a discount factor \(\gamma &lt; 1\)), then there exists a policy \(\pi\) that satisfies the Bellman optimality equation.</li> </ol> <p>Furthermore, we can perform policy/value iteration (will be covered later) using the Bellman and Bellman optimality equation. As a result, we can not only get better policies iteration after iteration, but also, under some constraints, prove that the final policy will converge to the optimal policy.</p>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[An introduction to Reinforcement Learning concepts, including Markov Decision Processes (MDPs).]]></summary></entry><entry><title type="html">å¼ºåŒ–å­¦ä¹  - æ•°å­¦åŸºç¡€</title><link href="https://m0gician.github.io/blog/2025/math-foundations-for-RL-zh/" rel="alternate" type="text/html" title="å¼ºåŒ–å­¦ä¹  - æ•°å­¦åŸºç¡€"/><published>2025-07-04T19:12:00+00:00</published><updated>2025-07-04T19:12:00+00:00</updated><id>https://m0gician.github.io/blog/2025/math-foundations-for-RL-zh</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/math-foundations-for-RL-zh/"><![CDATA[<h2 id="mdpé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹">MDPï¼ˆé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼‰</h2> <p>æˆ‘ä»¬é€šå¸¸æŠŠä¸€ä¸ª MDP å®šä¹‰ä¸ºä¸€ä¸ªå…ƒç»„ \((\mathcal{S}, \mathcal{A}, p, R, \gamma)\)ã€‚</p> <div class="callout"> <p><strong>çŠ¶æ€é›†åˆï¼ˆ\(\mathcal{S}\)ï¼‰ï¼š</strong> ç¯å¢ƒæ‰€æœ‰å¯èƒ½çŠ¶æ€çš„é›†åˆã€‚</p> <ul> <li>æ—¶åˆ» \(t\) çš„çŠ¶æ€ \(S_t\) æ€»æ˜¯å–å€¼äº \(\mathcal{S}\)ã€‚</li> </ul> </div> <div class="callout"> <p><strong>åŠ¨ä½œé›†åˆï¼ˆ\(\mathcal{A}\)ï¼‰ï¼š</strong> æ™ºèƒ½ä½“å¯ä»¥é‡‡å–çš„æ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„é›†åˆã€‚</p> <ul> <li>æ—¶åˆ» \(t\) çš„åŠ¨ä½œ \(A_t\) æ€»æ˜¯å–å€¼äº \(\mathcal{A}\)ã€‚</li> </ul> </div> <div class="callout"> <p><strong>è½¬ç§»å‡½æ•°ï¼ˆ\(p\)ï¼‰ï¼š</strong> æè¿°ç¯å¢ƒçŠ¶æ€å¦‚ä½•å˜åŒ–ã€‚</p> \[p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]\] <p>å¯¹äºæ‰€æœ‰ \(s \in \mathcal{S}\)ã€\(a \in \mathcal{A}\)ã€\(s' \in \mathcal{S}\) ä»¥åŠ \(t \in \mathbb{N}_{\ge 0}\)ï¼š</p> \[p(s,a,s') := \Pr(S_{t+1}=s' \mid S_t=s, A_t=a)\] <p>å½“ \(p(s,a,s') \in \{0,1\}\) å¯¹æ‰€æœ‰çš„ \(s,a,s'\) æˆç«‹æ—¶ï¼Œè½¬ç§»å‡½æ•°æ˜¯ç¡®å®šæ€§çš„ã€‚</p> </div> <div class="callout"> <p>\(d_R\) æè¿°å¥–åŠ±çš„ç”Ÿæˆæ–¹å¼ã€‚</p> \[R_t \sim d_R(S_t, A_t, S_{t+1})\] </div> <div class="callout"> <p><strong>å¥–åŠ±å‡½æ•°ï¼ˆ\(R\)ï¼‰ï¼š</strong> ç”±å¥–åŠ±åˆ†å¸ƒ \(d_R\) éšå¼å®šä¹‰çš„å‡½æ•°ï¼Œæè¿°å¥–åŠ±å¦‚ä½•ç”Ÿæˆã€‚</p> \[R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\] \[R(s,a) := \mathrm{E}[R_t \mid S_t = s, A_t = a]\] </div> <div class="callout"> <p><strong>åˆå§‹çŠ¶æ€åˆ†å¸ƒï¼ˆ\(d_0\)ï¼‰ï¼š</strong></p> \[d_0: \mathcal{S} \rightarrow [0,1]\] \[d_0(s) = \Pr(S_0 = s)\] </div> <div class="callout"> <p><strong>æŠ˜æ‰£å› å­ï¼ˆ\(\gamma\)ï¼‰ï¼š</strong> å–å€¼èŒƒå›´ \([0,1]\)ï¼Œç”¨äºæŠ˜æ‰£æœªæ¥å¥–åŠ±ã€‚</p> </div> <hr/> <h3 id="ç›®æ ‡">ç›®æ ‡</h3> <p>æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€æ¡æœ€ä¼˜ç­–ç•¥ \(\pi^*\)ï¼Œä½¿å¾—æœŸæœ›ç´¯è®¡æŠ˜æ‰£å¥–åŠ±æœ€å¤§åŒ–ã€‚</p> <ul> <li>\(G^i\) è¡¨ç¤ºç¬¬ \(i\) ä¸ªå›åˆçš„å›æŠ¥ï¼ˆreturnï¼‰ã€‚</li> <li>\(R_t^i\) è¡¨ç¤ºç¬¬ \(i\) ä¸ªå›åˆæ—¶åˆ» \(t\) çš„å¥–åŠ±ã€‚</li> </ul> <div class="callout"> <p><strong>ç›®æ ‡å‡½æ•°ï¼ˆ\(J\)ï¼‰ï¼š</strong></p> \[J : \Pi \rightarrow \mathbb{R}, \quad \text{å¯¹äºæ‰€æœ‰ }\pi \in \Pi\] \[\begin{aligned} J(\pi) &amp;:= \mathrm{E}\Bigg[\sum_{t=1}^{\infty} \gamma^t R_t \,\Big|\, \pi\Bigg] \\[0.2cm] \hat{J}(\pi) &amp;:= \frac{1}{N}\sum_{i=1}^{N} G^i = \frac{1}{N}\sum_{i=1}^{N}\sum_{t=0}^{\infty}\gamma^t R_t^i \end{aligned}\] </div> <div class="callout"> <p><strong>æœ€ä¼˜ç­–ç•¥ï¼ˆ\(\pi^*\)ï¼‰ï¼š</strong></p> \[\pi^* \in \arg\max_{\pi \in \Pi} J(\pi)\] </div> <details> <summary>å½“æœ€ä¼˜ç­–ç•¥å­˜åœ¨æ—¶å®ƒæ€»æ˜¯å”¯ä¸€çš„å—ï¼Ÿ</summary> ä¸ä¸€å®šï¼Œå¯èƒ½å­˜åœ¨å¤šæ¡åŒæ ·ä¼˜ç§€çš„æœ€ä¼˜ç­–ç•¥ã€‚ </details> <hr/> <h3 id="æ€§è´¨">æ€§è´¨</h3> <div class="callout"> <p><strong>æ—¶é™ï¼ˆHorizonï¼Œ\(L\)ï¼‰ï¼š</strong> æœ€å°çš„æ•´æ•° \(L\)ï¼Œä½¿å¾—å¯¹æ‰€æœ‰ \(t \ge L\)ï¼Œå¤„äºç»ˆæ­¢çŠ¶æ€ \(s_\infty\) çš„æ¦‚ç‡ä¸º 1ã€‚</p> \[\forall t \ge L,\; \Pr(S_t = s_\infty) = 1\] <ul> <li>è‹¥ \(L &lt; \infty\)ï¼ˆå¯¹æ‰€æœ‰ç­–ç•¥å‡æˆç«‹ï¼‰ï¼Œåˆ™ MDP ä¸º <strong>æœ‰é™æ—¶é™</strong>ï¼ˆå›åˆå¼ï¼‰ã€‚</li> <li>è‹¥ \(L = \infty\)ï¼Œåˆ™ MDP ä¸º <strong>æ— é™æ—¶é™</strong>ï¼ˆè¿ç»­å¼ï¼‰ã€‚</li> </ul> </div> <div class="callout"> <p><strong>é©¬å°”å¯å¤«æ€§ï¼ˆMarkov Propertyï¼‰ï¼š</strong> ä¸€ç§å…³äºçŠ¶æ€è¡¨ç¤ºçš„æ€§è´¨ï¼Œå‡è®¾åœ¨ç»™å®šå½“å‰çŠ¶æ€çš„æ¡ä»¶ä¸‹ï¼Œæœªæ¥ä¸è¿‡å»ç‹¬ç«‹ã€‚</p> <ul> <li>ç»™å®šå½“å‰çŠ¶æ€ \(S_t\)ï¼Œ\(S_{t+1}\) ä¸å†å² \(H_{t-1}\) æ¡ä»¶ç‹¬ç«‹ã€‚</li> </ul> </div> <hr/> <h2 id="ç­–ç•¥policy">ç­–ç•¥ï¼ˆPolicyï¼‰</h2> <div class="callout"> <p><strong>ç­–ç•¥</strong> æ˜¯ä¸€ç§å†³ç­–è§„åˆ™â€”â€”æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œçš„æ–¹å¼ã€‚</p> \[\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]\] \[\pi(s,a) := \Pr(A_t = a \mid S_t = s)\] </div> <hr/> <h2 id="ä»·å€¼å‡½æ•°value-functions">ä»·å€¼å‡½æ•°ï¼ˆValue Functionsï¼‰</h2> <div class="callout"> <p><strong>çŠ¶æ€ä»·å€¼å‡½æ•°ï¼ˆ\(v^\pi\)ï¼‰</strong></p> <p>çŠ¶æ€ä»·å€¼å‡½æ•° \(v^\pi : \mathcal{S} \rightarrow \mathbb{R}\) è¡¡é‡ä»çŠ¶æ€ \(s\) å‡ºå‘å¹¶éµå¾ªç­–ç•¥ \(\pi\) æ—¶çš„æœŸæœ›å›æŠ¥ã€‚</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\Bigg[\sum_{k=1}^{\infty}\gamma^k R_{t+k} \,\Big|\, S_t = s, \pi\Bigg] \\[0.2cm] &amp;:= \mathbf{E}[G_t \mid S_t = s, \pi] \end{aligned}\] </div> <div class="callout"> <p><strong>åŠ¨ä½œä»·å€¼å‡½æ•°ï¼ˆQ å‡½æ•°ï¼Œ\(q^\pi\)ï¼‰</strong></p> <p>åŠ¨ä½œä»·å€¼å‡½æ•° \(q^\pi : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) è¡¡é‡åœ¨çŠ¶æ€ \(s\) é‡‡å–åŠ¨ä½œ \(a\) åå†éµå¾ªç­–ç•¥ \(\pi\) æ‰€å¾—åˆ°çš„æœŸæœ›å›æŠ¥ã€‚</p> \[q^\pi(s,a) := \mathbf{E}[G_t \mid S_t = s, A_t = a, \pi]\] </div> <hr/> <h3 id="bellman-æ–¹ç¨‹">Bellman æ–¹ç¨‹</h3> <div class="callout"> <p><strong>çŠ¶æ€ä»·å€¼å‡½æ•°çš„ Bellman æ–¹ç¨‹ï¼ˆ\(v^\pi\)ï¼‰</strong></p> \[\begin{aligned} v^\pi(s) &amp;= \mathbf{E}\Big[R(s,A_t) + \gamma v^\pi(S_{t+1}) \,\Big|\, S_t = s, \pi\Big] \\[0.3cm] &amp;= \sum_{a \in \mathcal{A}} \pi(s,a) \sum_{s' \in \mathcal{S}} p(s,a,s')\big(R(s,a) + \gamma v^\pi(s')\big) \end{aligned}\] <ul> <li>Bellman æ–¹ç¨‹åªéœ€å‘å‰çœ‹ä¸€æ­¥ã€‚</li> <li>æœ€ä¼˜çŠ¶æ€ä»·å€¼å‡½æ•° \(v^*\) æ˜¯å”¯ä¸€çš„â€”â€”æ‰€æœ‰æœ€ä¼˜ç­–ç•¥å…±äº«åŒä¸€ \(v^*\)ã€‚</li> </ul> </div> <div class="callout"> <p><strong>åŠ¨ä½œä»·å€¼å‡½æ•°çš„ Bellman æ–¹ç¨‹ï¼ˆ\(q^\pi\)ï¼‰</strong></p> \[q^\pi(s,a) = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} p(s,a,s') \sum_{a' \in \mathcal{A}} \pi(s',a') q^\pi(s',a')\] <ul> <li>æœ€ä¼˜åŠ¨ä½œä»·å€¼å‡½æ•° \(q^*\) å¯¹æ‰€æœ‰æœ€ä¼˜ç­–ç•¥ä¹Ÿæ˜¯å”¯ä¸€çš„ã€‚</li> </ul> </div> <h3 id="bellman-æœ€ä¼˜æ–¹ç¨‹">Bellman æœ€ä¼˜æ–¹ç¨‹</h3> <ol> <li>è‹¥ä¸€æ¡ç­–ç•¥ \(\pi\) æ»¡è¶³ Bellman æœ€ä¼˜æ–¹ç¨‹ï¼Œåˆ™ \(\pi\) æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚</li> <li>è‹¥çŠ¶æ€ã€åŠ¨ä½œé›†åˆæœ‰é™ï¼Œå¥–åŠ±æœ‰ç•Œä¸” \(\gamma &lt; 1\)ï¼Œé‚£ä¹ˆå­˜åœ¨æ»¡è¶³ Bellman æœ€ä¼˜æ–¹ç¨‹çš„ç­–ç•¥ \(\pi\)ã€‚</li> </ol> <div class="callout"> <p><strong>\(v^*\) çš„ Bellman æœ€ä¼˜æ–¹ç¨‹</strong></p> <p>å¯¹æ‰€æœ‰çŠ¶æ€ \(s \in \mathcal{S}\)ï¼š</p> \[v^\pi(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} p(s,a,s') \big[R(s,a) + \gamma v^\pi(s')\big]\] </div> <div class="callout"> <p><strong>\(q^*\) çš„ Bellman æœ€ä¼˜æ–¹ç¨‹</strong></p> \[q^*(s,a) = \sum_{s' \in \mathcal{S}} p(s,a,s') \big[R(s,a) + \gamma \max_{a' \in \mathcal{A}} q^*(s',a')\big]\] </div> <hr/> <h2 id="ç­–ç•¥è¿­ä»£policy-iteration">ç­–ç•¥è¿­ä»£ï¼ˆPolicy Iterationï¼‰</h2> <p>ç­–ç•¥è¿­ä»£é€šè¿‡äº¤æ›¿æ‰§è¡Œä¸¤æ­¥â€”â€”ç­–ç•¥è¯„ä¼°ä¸ç­–ç•¥æ”¹è¿›â€”â€”æ¥å¯»æ‰¾æœ€ä¼˜ç­–ç•¥ã€‚</p> <ul> <li>é€šè¿‡åŠ¨æ€è§„åˆ’è¿›è¡Œçš„ç­–ç•¥è¯„ä¼°è™½ç„¶ä¿è¯æ”¶æ•›åˆ° \(v^\pi\)ï¼Œä½†å¹¶ä¸ä¿è¯åœ¨æœ‰é™è®¡ç®—å†…å°±èƒ½åˆ°è¾¾ã€‚</li> </ul> <div class="callout"> <p><strong>ç­–ç•¥æ”¹è¿›å®šç†</strong></p> <p>å¯¹äºä»»æ„ç­–ç•¥ \(\pi\)ï¼Œè‹¥å­˜åœ¨ç¡®å®šæ€§ç­–ç•¥ \(\pi'\) ä½¿å¾— \(\forall s \in \mathcal{S}\)ï¼š</p> \[q^\pi(s, \pi'(s)) \ge v^\pi(s)\] <p>åˆ™æœ‰ \(\pi' \ge \pi\)ã€‚</p> </div> <div class="callout"> <p><strong>éšæœºç­–ç•¥çš„ç­–ç•¥æ”¹è¿›å®šç†</strong></p> <p>å¯¹äºä»»æ„ç­–ç•¥ \(\pi\)ï¼Œè‹¥ \(\pi'\) æ»¡è¶³ï¼š</p> \[\sum_{a \in \mathcal{A}} \pi'(s,a) q^\pi(s,a) \ge v^\pi(s),\] <p>åˆ™ \(\forall s \in \mathcal{S}\)ï¼Œéƒ½æœ‰ \(\pi' \ge \pi\)ã€‚</p> </div> <hr/> <h2 id="ä»·å€¼è¿­ä»£value-iteration">ä»·å€¼è¿­ä»£ï¼ˆValue Iterationï¼‰</h2> <p>ä»·å€¼è¿­ä»£é€šè¿‡è¿­ä»£åº”ç”¨ Bellman æœ€ä¼˜æ›´æ–°æ¥å¯»æ‰¾æœ€ä¼˜çŠ¶æ€ä»·å€¼å‡½æ•°ã€‚</p> <div class="callout"> <p><strong>Banach ä¸åŠ¨ç‚¹å®šç†</strong></p> <p>è‹¥æ˜ å°„ \(f\) åœ¨éç©ºå®Œå¤‡èµ‹èŒƒå‘é‡ç©ºé—´ä¸Šæ˜¯æ”¶ç¼©æ˜ å°„ï¼Œåˆ™å­˜åœ¨å”¯ä¸€ä¸åŠ¨ç‚¹ \(x^*\)ï¼Œä¸”ä»¥ä»»æ„ \(x_0\) ä¸ºåˆå§‹ç‚¹ã€æŒ‰ç…§ \(x_{k+1} = f(x_k)\) ç”Ÿæˆçš„åºåˆ—æ”¶æ•›åˆ° \(x^*\)ã€‚</p> </div> <div class="callout"> <p><strong>Bellman ç®—å­æ˜¯æ”¶ç¼©æ˜ å°„</strong></p> <p>å½“ \(\gamma &lt; 1\) æ—¶ï¼Œåœ¨åº¦é‡ \(d(v,v') := \max_{s \in \mathcal{S}} |v(s)-v'(s)|\) ä¸‹ï¼ŒBellman ç®—å­åœ¨ \(\mathbb{R}^{|\mathcal{S}|}\) ä¸Šæ˜¯æ”¶ç¼©æ˜ å°„ã€‚</p> </div> <ul> <li>å¯¹äºæœ‰é™çŠ¶æ€åŠ¨ä½œé›†ã€æœ‰ç•Œå¥–åŠ±ä¸” \(\gamma &lt; 1\) çš„ MDPï¼Œä»·å€¼è¿­ä»£ <strong>æ”¶æ•›</strong> åˆ°å”¯ä¸€çš„å›ºå®šç‚¹ \(v^\infty\)ã€‚</li> <li>è¿™ç±» MDP <strong>è‡³å°‘å­˜åœ¨</strong> ä¸€æ¡æœ€ä¼˜ç­–ç•¥ã€‚</li> </ul> <hr/> <h2 id="å¤§æ•°å®šå¾‹law-of-large-numbers">å¤§æ•°å®šå¾‹ï¼ˆLaw of Large Numbersï¼‰</h2> <div class="callout"> <p><strong>è¾›é’¦å¼ºå¤§æ•°å®šå¾‹ï¼ˆKhintchineâ€™s Strong Law of Large Numbersï¼‰</strong></p> <p>è®¾ \(\{X_i\}_{i=1}^{\infty}\) ä¸º <strong>ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰éšæœºå˜é‡</strong>ã€‚åˆ™æ ·æœ¬å¹³å‡åºåˆ— \((\frac{1}{n} \sum_{i=1}^{n} X_i)_{n=1}^\infty\) <strong>å‡ ä¹å¿…ç„¶</strong> æ”¶æ•›åˆ°æœŸæœ› \(\mathbf{E}[X_1]\)ã€‚</p> <p>å³ \(\displaystyle \frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{a.s.} \mathbf{E}[X_1]\)</p> </div> <div class="callout"> <p><strong>Kolmogorov å¼ºå¤§æ•°å®šå¾‹</strong></p> <p>è®¾ \(\{X_i\}_{i=1}^{\infty}\) ä¸º <strong>ç‹¬ç«‹ï¼ˆä¸è¦æ±‚åŒåˆ†å¸ƒï¼‰éšæœºå˜é‡</strong>ã€‚è‹¥æ‰€æœ‰ \(X_i\) å…·æœ‰ <strong>ç›¸åŒå‡å€¼ä¸”æ–¹å·®æœ‰ç•Œ</strong>ï¼Œåˆ™æ ·æœ¬å¹³å‡åºåˆ— \((\frac{1}{n}\sum_{i=1}^{n} X_i)^\infty_{n=1}\) äº¦å‡ ä¹å¿…ç„¶æ”¶æ•›åˆ° \(\mathbf{E}[X_1]\)ã€‚</p> </div>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[è¶…å¤§ä¸€å¨æ•°å­¦å…¬å¼]]></summary></entry><entry><title type="html">Reinforcement Learning - Mathematical Foundations</title><link href="https://m0gician.github.io/blog/2025/math-foundations-for-RL/" rel="alternate" type="text/html" title="Reinforcement Learning - Mathematical Foundations"/><published>2025-07-04T19:12:00+00:00</published><updated>2025-07-04T19:12:00+00:00</updated><id>https://m0gician.github.io/blog/2025/math-foundations-for-RL</id><content type="html" xml:base="https://m0gician.github.io/blog/2025/math-foundations-for-RL/"><![CDATA[<h2 id="mdp-markov-decision-process">MDP (Markov Decision Process)</h2> <p>We usually define an MDP as a tuple \((\mathcal{S}, \mathcal{A}, p, R, \gamma)\).</p> <div class="callout"> <p><strong>State Set (\(\mathcal{S}\)):</strong> The set of all possible states of the environment.</p> <ul> <li>The state at time \(t\), \(S_t\), always takes values in \(\mathcal{S}\).</li> </ul> </div> <div class="callout"> <p><strong>Action Set (\(\mathcal{A}\)):</strong> The set of all possible actions the agent can take.</p> <ul> <li>The action at time \(t\), \(A_t\), always takes values in \(\mathcal{A}\).</li> </ul> </div> <div class="callout"> <p><strong>Transition Function (\(p\)):</strong> Describes how the state of the environment changes.</p> \[p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]\] <p>For all \(s \in \mathcal{S}\), \(a \in \mathcal{A}\), \(sâ€™ \in \mathcal{S}\), and \(t \in \mathbb{N}_{\geq 0}\):</p> \[p(s,a,s') := \text{Pr}(S_{t+1}=s' | S_t=s, A_t=a)\] <p>A transition function is deterministic if \(p(s,a,sâ€™) \in \{0,1\}\) for all s, a, and sâ€™</p> </div> <div class="callout"> <p>\(d_R\) describes how rewards are generated.</p> \[R_t \sim d_r(S_t, A_t, S_{t+1})\] </div> <div class="callout"> <p><strong>Reward Function (\(R\)):</strong> A function implicitly defined by the reward distribution \(d_R\), which describes how rewards are generated.</p> \[R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\] \[R(s,a) := \mathrm{R}[R_t|S_t = s, A_t = a]\] </div> <div class="callout"> <p><strong>Initial State Distribution (\(d_0\)):</strong></p> \[d_0: \mathcal{S} \rightarrow [0,1]\] \[d_0(s) = \text{Pr}(S_0=s)\] </div> <div class="callout"> <p><strong>Discount Factor (\(\gamma\)):</strong> A parameter in \([0,1]\) that discounts future rewards.</p> </div> <hr/> <h3 id="objective">Objective</h3> <p>The goal is to find an optimal policy \(\pi^*\) that maximizes the expected total amount of discounted reward.</p> <ul> <li>\(G^i\) denotes the return of the i-th episode.</li> <li>\(R^i_t\) denotes the reward at time \(t\) during episode \(i\).</li> </ul> <div class="callout"> <p><strong>Objective function (\(J\)):</strong></p> \[J : \Pi \rightarrow \mathbb{R}, \text{where for all } \pi \in \Pi\] \[\begin{aligned} &amp;J(\pi) := \mathrm{E}\left[\sum_{t=1}^{\infty} \gamma^tR_t \bigg| \pi\right] \\ &amp;\hat{J}(\pi) := \frac{1}{N}\sum_{i=1}^{N}G^i = \frac{1}{n}\sum_{i=1}^{N}\sum_{t=0}^{\infty}\gamma^t R_t^i \end{aligned}\] </div> <div class="callout"> <p><strong>Optimal Policy (\(\pi^*\)):</strong></p> \[\pi^* \in \underset{\pi \in \Pi}{\text{argmax}}\,J(\pi)\] </div> <details> <summary>Is the optimal policy always unique when it exists?</summary> No. There can exist multiple optimal policies that are equally good. </details> <hr/> <h3 id="properties">Properties</h3> <div class="callout"> <p><strong>Horizon (\(L\)):</strong> The smallest integer \(L\) such that for all \(t \geq L\), the probability of being in a terminal state \(s_\infty\) is 1.</p> \[\forall t \geq L, \text{Pr}(S_t = s_\infty) = 1\] <ul> <li>The MDP is <strong>finite horizon</strong> (episodic) if \(L &lt; \infty\) for all policies.</li> <li>The MDP is <strong>infinite horizon</strong> (continuous) when \(L = \infty\).</li> </ul> </div> <div class="callout"> <p><strong>Markov Property:</strong> A property of the state representation. It assumes that the future is independent of the past given the present.</p> <ul> <li>\(S_{t+1}\) is conditionally independent of the history \(H_{t-1}\) given the current state \(S_t\).</li> </ul> </div> <hr/> <h2 id="policy">Policy</h2> <div class="callout"> <p>A <strong>policy</strong> is a decision ruleâ€”a way that the agent can select actions.</p> \[\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]\] \[\pi(s,a) := \text{Pr}(A_t=a | S_t=s)\] </div> <hr/> <h2 id="value-functions">Value Functions</h2> <div class="callout"> <p><strong>State-Value Function (\(v^\pi\))</strong></p> <p>The state-value function \(v^\pi : \mathcal{S} \rightarrow \mathbb{R}\) measures the expected return starting from a state \(s\) and following policy \(\pi\).</p> \[\begin{aligned} v^\pi(s) &amp;:= \mathbf{E}\left[\sum_{k=1}^{\infty}\gamma^k R_{t+k} \bigg| S_t=s, \pi\right] \\ &amp;:= \mathbf{E}[G_t|S_t=s, \pi] \end{aligned}\] </div> <div class="callout"> <p><strong>Action-Value Function (Q-function, \(q^\pi\))</strong></p> <p>The action-value function \(q^\pi : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) measures the expected return starting from state \(s\), taking action \(a\), and then following policy \(\pi\).</p> \[q^\pi(s,a) := \mathbf{E}[G_t | S_t=s, A_t=a, \pi]\] </div> <hr/> <h3 id="bellman-equations">Bellman Equations</h3> <div class="callout"> <p><strong>Bellman Equation for the State-Value Function (\(v^\pi\))</strong></p> \[\begin{aligned} v^\pi(s) &amp;= \mathbf{E}\left[\underbrace{R(s,A_t)}_{\text{immediate reward}} + \gamma \underbrace{v^\pi(S_{t+1}) }_{\text{value of next state}} \bigg| S_t = s, \pi\right] \\[0.3cm] &amp;= \sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s' \in \mathcal{S}}p(s,a,s')(R(s,a) + \gamma v^\pi(s')) \end{aligned}\] <ul> <li>The Bellman equation only needs to look forward one time step into the future.</li> <li>The optimal state-value function, \(v^*\), is uniqueâ€”all optimal policies share the same state-value function.</li> </ul> </div> <div class="callout"> <p><strong>Bellman Equation for the Action-Value Function (\(q^\pi\))</strong></p> \[q^\pi(s,a) = R(s,a) + \gamma\sum_{s' \in \mathcal{S}}p(s,a,s')\sum_{a' \in \mathcal{A}}\pi(s',a')q^\pi(s',a')\] <ul> <li>The optimal action-value function, \(q^*\), is also unique among all optimal policies.</li> </ul> </div> <h3 id="bellman-optimality-equations">Bellman Optimality Equations</h3> <ol> <li>If a policy \(\pi\) satisfies the Bellman optimality equation, then \(\pi\) is an optimal policy.</li> <li>If the state and action sets are finite, rewards are bounded, and \(\gamma &lt; 1\), then there exists a policy \(\pi\) that satisfies the Bellman optimality equation.</li> </ol> <div class="callout"> <p><strong>Bellman Optimality Equation for \(v^*\)</strong></p> <p>A policy \(\pi\) satisfies the Bellman optimality equation if for all states \(s\in\mathcal{S}\):</p> \[v^\pi(s) = \max_{a\in\mathcal{A}}\sum_{s'\in\mathcal{S}}p(s,a,s')[R(s,a)+\gamma v^\pi(s')]\] </div> <div class="callout"> <p><strong>Bellman Optimality Equation for \(q^*\)</strong></p> \[q^*(s,a) = \sum_{s'\in\mathcal{S}}p(s,a,s')\left[R(s,a) + \gamma\max_{a'\in\mathcal{A}}q^*(s',a')\right]\] </div> <hr/> <h2 id="policy-iteration">Policy Iteration</h2> <p>Policy iteration is an algorithm that finds an optimal policy by alternating between two steps: policy evaluation and policy improvement.</p> <ul> <li>Even though policy evaluation using dynamic programming is guaranteed to converge to \(v^\pi\), it is not guaranteed to reach \(v^\pi\) in a finite amount of computation.</li> </ul> <div class="callout"> <p><strong>Policy Improvement Theorem</strong></p> <p>For any policy \(\pi\), if \(\piâ€™\) is a deterministic policy such that \(\forall s \in \mathcal{S}\):</p> \[q^\pi(s, \pi'(s)) \geq v^\pi(s)\] <p>then \(\piâ€™ \geq \pi\).</p> </div> <div class="callout"> <p><strong>Policy Improvement Theorem for Stochastic Policies</strong></p> <p>For any policy \(\pi\), if \(\piâ€™\) satisfies:</p> \[\sum_{a\in\mathcal{A}}\pi'(s,a) q^\pi(s,a) \geq v^\pi(s),\] <p>for all \(s \in \mathcal{S}\), then \(\pi' \geq \pi\).</p> </div> <hr/> <h2 id="value-iteration">Value Iteration</h2> <p>Value iteration is an algorithm that finds the optimal state-value function by iteratively applying the Bellman optimality update.</p> <div class="callout"> <p><strong>Banach Fixed-Point Theorem</strong></p> <p>If \(f\) is a contraction mapping on a non-empty complete normed vector space, then \(f\) has a unique fixed point, \(x^*\), and the sequence defined by \(x_{k+1} = f(x_k)\), with \(x_0\) chosen arbitrarily, converges to \(x^*\).</p> </div> <div class="callout"> <p><strong>Bellman Operator is a Contraction Mapping</strong></p> <p>The Bellman operator is a contraction mapping on \(\mathbb{R}^{\vert\mathcal{S}\vert}\) with distance metric \(d(v,vâ€™) := \max_{s\in\mathcal{S}}\vert v(s)-vâ€™(s) \vert\) if \(\gamma &lt; 1\).</p> </div> <ul> <li>Value iteration <strong>converges</strong> to a unique fixed point \(v^\infty\) for all MDPs with finite state and action sets, bounded rewards, and \(\gamma &lt; 1\).</li> <li>All MDPs with finite state and action sets, bounded rewards, and \(\gamma &lt; 1\) <strong>have at least one optimal policy</strong>.</li> </ul> <hr/> <h2 id="law-of-large-numbers">Law of Large Numbers</h2> <div class="callout"> <p><strong>Khintchineâ€™s Strong Law of Large Numbers</strong></p> <p>Let \(\{X_i\}_{i=1}^{\infty}\) be <strong>independent and identically distributed (i.i.d.) random variables</strong>. Then the sequence of sample averages \((\frac{1}{n} \sum_{i=1}^{n} X_i)_{n=1}^\infty\) converges <strong>almost surely</strong> to the expected value \(\mathbf{E}[X_1]\).</p> <p>i.e., \(\displaystyle \frac{1}{n}\sum_{i=1}^{\infty} X_i \overset{a.s.}{\rightarrow}\mathbf{E}[X_1]\)</p> </div> <div class="callout"> <p><strong>Kolmogorovâ€™s Strong Law of Large Numbers</strong></p> <p>Let \(\{X_i\}^\infty_{i=1}\) be <strong>independent (not necessarily identically distributed) random variables</strong>. If all \(X_i\) have the <strong>same mean and bounded variance</strong>, then the sequence of sample averages \((\frac{1}{n}\sum_{i=1}^n X_i)^\infty_{n=1}\) converges almost surely to \(\mathbf{E}[X_1]\).</p> </div>]]></content><author><name></name></author><category term="reinforcement-learning"/><category term="RL"/><category term="math"/><summary type="html"><![CDATA[A very, very, very large chunk of math]]></summary></entry></feed>