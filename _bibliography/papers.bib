---
---

@inproceedings{wang-etal-2024-raccoon,
  title     = {Raccoon: Prompt Extraction Benchmark of {LLM}-Integrated Applications},
  author    = {Wang, Junlin  and
               Yang, Tianyi  and
               Xie, Roy  and
               Dhingra, Bhuwan},
  editor    = {Ku, Lun-Wei  and
               Martins, Andre  and
               Srikumar, Vivek},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},
  month     = aug,
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.findings-acl.791},
  doi       = {10.18653/v1/2024.findings-acl.791},
  pages     = {13349--13365},
  abstract  = {With the proliferation of LLM-integrated applications such as GPT-s, millions are deployed, offering valuable services through proprietary instruction prompts. These systems, however, are prone to prompt extraction attacks through meticulously designed queries. To help mitigate this problem, we introduce the Raccoon benchmark which comprehensively evaluates a model{'}s susceptibility to prompt extraction attacks. Our novel evaluation method assesses models under both defenseless and defended scenarios, employing a dual approach to evaluate the effectiveness of existing defenses and the resilience of the models. The benchmark encompasses 14 categories of prompt extraction attacks, with additional compounded attacks that closely mimic the strategies of potential attackers, alongside a diverse collection of defense templates. This array is, to our knowledge, the most extensive compilation of prompt theft attacks and defense mechanisms to date. Our findings highlight universal susceptibility to prompt theft in the absence of defenses, with OpenAI models demonstrating notable resilience when protected. This paper aims to establish a more systematic benchmark for assessing LLM robustness against prompt extraction attacks, offering insights into their causes and potential countermeasures.},
  selected  = {true},
  preview   = {raccoon.png}
}

@inproceedings{10.1145/3627703.3650074,
  author    = {Yang, Qizheng and Yang, Tianyi and Xiang, Mingcan and Zhang, Lijun and Wang, Haoliang and Serafini, Marco and Guan, Hui},
  title     = {GMorph: Accelerating Multi-DNN Inference via Model Fusion},
  year      = {2024},
  isbn      = {9798400704376},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3627703.3650074},
  doi       = {10.1145/3627703.3650074},
  abstract  = {AI-powered applications often involve multiple deep neural network (DNN)-based prediction tasks to support application-level functionalities. However, executing multi-DNNs can be challenging due to the high resource demands and computation costs that increase linearly with the number of DNNs. Multi-task learning (MTL) addresses this problem by designing a multi-task model that shares parameters across tasks based on a single backbone DNN. This paper explores an alternative approach called model fusion: rather than training a single multi-task model from scratch as MTL does, model fusion fuses multiple task-specific DNNs that are pre-trained separately and can have heterogeneous architectures into a single multi-task model. We materialize model fusion in a software framework called GMorph to accelerate multi-DNN inference while maintaining task accuracy. GMorph features three main technical contributions: graph mutations to fuse multi-DNNs into resource-efficient multi-task models, search-space sampling algorithms, and predictive filtering to reduce the high search costs. Our experiments show that GMorph can outperform MTL baselines and reduce the inference latency of multi-DNNs by 1.1-3\texttimes{} while meeting the target task accuracy.},
  booktitle = {Proceedings of the Nineteenth European Conference on Computer Systems},
  pages     = {505â€“523},
  numpages  = {19},
  keywords  = {deep learning systems, machine learning inference, multi-task inference},
  location  = {Athens, Greece},
  series    = {EuroSys '24},
  selected  = {true},
  preview   = {gmorph.png}
}

@inproceedings{hwang-etal-2022-event,
  title     = {Event-Event Relation Extraction using Probabilistic Box Embedding},
  author    = {Hwang, EunJeong  and
               Lee, Jay-Yoon  and
               Yang, Tianyi  and
               Patel, Dhruvesh  and
               Zhang, Dongxu  and
               McCallum, Andrew},
  editor    = {Muresan, Smaranda  and
               Nakov, Preslav  and
               Villavicencio, Aline},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = may,
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.acl-short.26},
  doi       = {10.18653/v1/2022.acl-short.26},
  pages     = {235--244},
  abstract  = {To understand a story with multiple events, it is important to capture the proper relations across these events. However, existing event relation extraction (ERE) framework regards it as a multi-class classification task and do not guarantee any coherence between different relation types, such as anti-symmetry. If a phone line {``}died{''} after {``}storm{''}, then it is obvious that the {``}storm{''} happened before the {``}died{''}. Current framework of event relation extraction do not guarantee this coherence and thus enforces it via constraint loss function (Wang et al., 2020). In this work, we propose to modify the underlying ERE model to guarantee coherence by representing each event as a box representation (BERE) without applying explicit constraints. From our experiments, BERE also shows stronger conjunctive constraint satisfaction while performing on par or better in F1 compared to previous models with constraint injection.},
  selected  = {true},
  preview   = {ce2ere.png}
}
